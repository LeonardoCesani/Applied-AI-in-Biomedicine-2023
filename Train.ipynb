{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lcKtEN5APNij"
   },
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MUOlTg7NPHa-"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import json\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Model\n",
    "from keras import Sequential\n",
    "from keras.layers import Input, Add, Dense, Activation, ZeroPadding1D, Flatten, GlobalAveragePooling1D, Concatenate\n",
    "from keras.layers import RepeatVector, Conv1D, MaxPooling1D, GlobalMaxPooling1D, Dropout,concatenate, BatchNormalization, Reshape\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras.metrics import Precision, Recall, FalseNegatives\n",
    "from keras.losses import KLDivergence, BinaryCrossentropy, CategoricalCrossentropy\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "from tqdm.auto import tqdm  # For progress bars\n",
    "\n",
    "import keras.backend as K\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "K.set_image_data_format('channels_last')\n",
    "\n",
    "colors_palette = [\"#024059\", \"#0388A6\", \"#8C031C\"] # Starting colors\n",
    "sns.set_palette(sns.color_palette(colors_palette)) # Set colors for Seaborn\n",
    "palette = sns.light_palette(colors_palette[0], as_cmap=True) # Palette"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YdO4r1NlRyer"
   },
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Avf6VVdORzr_"
   },
   "outputs": [],
   "source": [
    "X_128 = np.load('Dataset/X_128_2.npy')\n",
    "y_128 = np.load('Dataset/y_128_2.npy')\n",
    "maps_idx_128 = np.load('Dataset/maps_idx_128_2.npy')\n",
    "features_128 = np.load('Dataset/features_128_2.npy')\n",
    "\n",
    "X_250 = np.load('Dataset/X_250_under_2.npy')\n",
    "y_250 = np.load('Dataset/y_250_under_2.npy')\n",
    "maps_idx_250 = np.load('Dataset/maps_idx_250_under_2.npy')\n",
    "features_250 = np.load('Dataset/features_250_under_2.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1706608437210,
     "user": {
      "displayName": "leonardo cesani",
      "userId": "06996620850007426608"
     },
     "user_tz": -60
    },
    "id": "XvgePYbLR5Df",
    "outputId": "1571ca1a-9c28-4c2c-fb1d-85656cac8309"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((117072, 128, 2), (117072, 1), (117072, 2), (117072, 8))"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "X = np.concatenate([X_128, X_250], axis=0)\n",
    "y = np.concatenate([y_128, y_250], axis=0)\n",
    "features = np.concatenate([features_128, features_250], axis=0)\n",
    "# Fix map to signal\n",
    "maps_idx_250[:,0] = maps_idx_250[:,0] + np.max(maps_idx_128[:,0]) + 1\n",
    "maps_idx = np.concatenate([maps_idx_128, maps_idx_250], axis=0)\n",
    "\n",
    "X.shape, y.shape, maps_idx.shape, features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bG-a_xj9Sc5g"
   },
   "outputs": [],
   "source": [
    "del X_128, X_250, y_128, y_250, maps_idx_128, maps_idx_250, features_128, features_250"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_c3FzHcEg3Mw"
   },
   "source": [
    "### Remove anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 49230,
     "status": "ok",
     "timestamp": 1706608486436,
     "user": {
      "displayName": "leonardo cesani",
      "userId": "06996620850007426608"
     },
     "user_tz": -60
    },
    "id": "NjYGyeiFg6NV",
    "outputId": "15426fe8-db54-43d5-d9e1-e8cc98ca4bf2"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of anomalies detected: 7023\n"
     ]
    }
   ],
   "source": [
    "# Train Isolation Forest model\n",
    "model = IsolationForest(n_estimators=150, contamination=0.04, bootstrap=True, verbose = False)  # Contamination = expected proportion of anomalies\n",
    "model.fit(X[:, :, 0])\n",
    "\n",
    "# Predict the anomaly scores\n",
    "anomaly_scores = model.decision_function(X[:, :, 0])\n",
    "\n",
    "# Anomaly scores will be more negative for anomalies\n",
    "threshold = np.percentile(anomaly_scores, 6)  # Consider bottom 6% as anomalies\n",
    "\n",
    "# Identify anomalies using the threshold\n",
    "anomalies_indices = np.where(anomaly_scores < threshold)[0]\n",
    "\n",
    "# Number of detected anomalies\n",
    "print(f\"Number of anomalies detected: {len(anomalies_indices)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 446,
     "status": "ok",
     "timestamp": 1706608486876,
     "user": {
      "displayName": "leonardo cesani",
      "userId": "06996620850007426608"
     },
     "user_tz": -60
    },
    "id": "jZLnwuFZhnDM",
    "outputId": "9bd81a90-ae95-4c9c-f4a0-3511fa93e5ad"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((112869, 128, 2), (112869, 1), (112869, 2), (112869, 8))"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "# Delete only Normal signals with anomalies\n",
    "anomalies_N = []\n",
    "for i in anomalies_indices:\n",
    "  if y[i]=='N':\n",
    "    anomalies_N.append(i)\n",
    "\n",
    "X = np.delete(X, anomalies_N, axis=0)\n",
    "maps_idx = np.delete(maps_idx, anomalies_N, axis=0)\n",
    "features = np.delete(features, anomalies_N, axis=0)\n",
    "y = np.delete(y, anomalies_N, axis=0)\n",
    "\n",
    "X.shape, y.shape, maps_idx.shape, features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "929xR2rvPmrK"
   },
   "source": [
    "# Base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6clhDM4OPmCs"
   },
   "outputs": [],
   "source": [
    "class ResNetModel:\n",
    "\n",
    "    seed = 42\n",
    "\n",
    "    def __init__(self, input_shape, num_classes, num_features, N, filters, verbose=False, labels=['0', '1']):\n",
    "        self.model = self.__get_model(input_shape, num_classes, num_features, N, filters)\n",
    "        self.X_train = self.X_test = self.y_ohe_train = self.y_ohe_test = self.features_train = self.features_test = None\n",
    "        self.verbose = verbose\n",
    "        self.labels = labels\n",
    "\n",
    "    def __get_model(self, input_shape, num_classes, num_features, N, filters):\n",
    "        # Input\n",
    "        signal_inputs = Input(shape=input_shape, name = 'SignalInput')\n",
    "        conv_layer = self.__ResBs_Conv(signal_inputs, filters)\n",
    "\n",
    "\n",
    "        # Class information branch\n",
    "        features_input = Input(shape=(num_features,), name = 'FeaturesInput')\n",
    "        repeat_class = RepeatVector(input_shape[0]//2, name='RepeatLayer')(features_input)\n",
    "        class_combined = Concatenate(axis=-1, name = 'MergingLayer')([conv_layer, repeat_class])\n",
    "        conv_layer = self.__ResBs_Conv(class_combined, filters)\n",
    "\n",
    "        # First Stage Attention\n",
    "        attention_1 = self.__CBAM_block(conv_layer, filters)\n",
    "\n",
    "        conv_layer = BatchNormalization()(attention_1)\n",
    "\n",
    "        # Decoder\n",
    "        M = int((N - 2)/2)\n",
    "        for i in range(M):\n",
    "            filters = filters * 2\n",
    "            # define N-th ResBs block\n",
    "            conv_layer = self.__ResBs_Conv(conv_layer, filters)\n",
    "            conv_layer = self.__ResBs_Identity(conv_layer, filters)\n",
    "\n",
    "        # Second Stage Attention\n",
    "        attention_2 = self.__CBAM_block(conv_layer, filters)\n",
    "\n",
    "        #flatten = Flatten()(attention_2)\n",
    "        gap = GlobalAveragePooling1D()(attention_2)\n",
    "\n",
    "        dense = Dense(filters*2, activation='relu', name='dense1')(gap)\n",
    "\n",
    "        dense = Dense(filters/2, activation='relu', name='dense2')(dense)\n",
    "\n",
    "        # Classification head\n",
    "        classification_output = Dense(num_classes, activation='softmax', name='classification_output')(dense)\n",
    "\n",
    "        model = Model(inputs=[signal_inputs, features_input], outputs=classification_output)\n",
    "\n",
    "        return model\n",
    "\n",
    "    def __ResBs_Identity(self, block_input, num_filters):\n",
    "\n",
    "        # 1. First Convolutional Layer\n",
    "        conv1 = Conv1D(filters=num_filters, kernel_size=7, padding= 'same')(block_input)\n",
    "        norm1 = BatchNormalization()(conv1)\n",
    "        relu1 = Activation('relu')(norm1)\n",
    "        dropout = Dropout(0.2)(relu1)\n",
    "\n",
    "        # 2. Second Convolutional Layer\n",
    "        conv2 = Conv1D(num_filters, kernel_size=7, padding= 'same')(dropout) #per avere concordanza\n",
    "        norm2 = BatchNormalization()(conv2)\n",
    "\n",
    "        # 3. Summing Layer (adding a residual connection)\n",
    "        sum = Add()([block_input, norm2])\n",
    "\n",
    "        # 4. Activation Layer\n",
    "        relu2 = Activation('relu')(sum)\n",
    "\n",
    "        return relu2\n",
    "\n",
    "\n",
    "    def __ResBs_Conv(self, block_input, num_filters):\n",
    "\n",
    "        # 0. Filter Block input and BatchNormalization\n",
    "        block_shortcut = Conv1D(num_filters, kernel_size=7, strides=2,  padding='same')(block_input)\n",
    "        block_shortcut = BatchNormalization()(block_shortcut)\n",
    "\n",
    "        # 1. First Convolutional Layer\n",
    "        conv1 = Conv1D(filters=num_filters, kernel_size=7, strides=2, padding='same')(block_input)\n",
    "        norm1 = BatchNormalization()(conv1)\n",
    "        relu1 = Activation('relu')(norm1)\n",
    "        dropout = Dropout(0.2)(relu1)\n",
    "\n",
    "        # 2. Second Convolutional Layer\n",
    "        conv2 = Conv1D(num_filters, kernel_size=7, padding='same')(dropout) # per avere concordanza\n",
    "        norm2 = BatchNormalization()(conv2)\n",
    "\n",
    "        # 3. Summing Layer (adding a residual connection)\n",
    "        sum = Add()([block_shortcut, norm2])\n",
    "\n",
    "        # 4. Activation Layer\n",
    "        relu2 = Activation('relu')(sum)\n",
    "\n",
    "        return relu2\n",
    "\n",
    "\n",
    "    def __CBAM_block(self, in_block, ch, ratio=16):\n",
    "        # 1. Channel attention block\n",
    "        avg_pool = tf.reduce_mean(in_block, axis=1, keepdims=True)\n",
    "        max_pool = tf.reduce_max(in_block, axis=1, keepdims=True)\n",
    "\n",
    "        dense1 = Conv1D(filters=ch // ratio, kernel_size=1, activation='relu')\n",
    "        avg_reduced = dense1(avg_pool)\n",
    "        max_reduced = dense1(max_pool)\n",
    "\n",
    "        dense2 = Conv1D(filters=ch, kernel_size=1, activation='sigmoid')\n",
    "        avg_attention = dense2(avg_reduced)\n",
    "        max_attention = dense2(max_reduced)\n",
    "\n",
    "        x = tf.add(avg_attention, max_attention)\n",
    "        x = tf.nn.sigmoid(x)\n",
    "        x = tf.multiply(in_block, x)\n",
    "\n",
    "        # 2. Spatial attention block\n",
    "        y_mean = tf.reduce_mean(x, axis=-1, keepdims=True)\n",
    "        y_max = tf.reduce_max(x, axis=-1, keepdims=True)\n",
    "\n",
    "        y = tf.concat([y_mean, y_max], axis=-1)\n",
    "        y = Conv1D(filters=1, kernel_size=5, padding='same', activation='sigmoid')(y)\n",
    "        y = tf.multiply(x, y)\n",
    "\n",
    "        return y\n",
    "\n",
    "    def __compute_class_weights(self):\n",
    "        y_integers = np.argmax(self.y_ohe_train, axis=1)\n",
    "        class_weights = compute_class_weight(class_weight='balanced', y=y_integers, classes=np.unique(y_integers))\n",
    "        d_class_weights = dict(enumerate(class_weights))\n",
    "        return d_class_weights\n",
    "\n",
    "    def get_summary_model(self):\n",
    "        self.model.summary()\n",
    "\n",
    "    def compile_train_model(self, metrics, loss, callbacks, lr, batch_size, epochs, validation_split, use_sample_weight=False):\n",
    "\n",
    "        if self.X_train is None:\n",
    "            raise ValueError(\"Data has not been preprocessed!\")\n",
    "\n",
    "        self.model.compile(\n",
    "            optimizer = keras.optimizers.Adam(lr),\n",
    "            loss = loss,\n",
    "            metrics = metrics\n",
    "        )\n",
    "\n",
    "        self.model.fit(\n",
    "            x = [self.X_train, self.features_train],\n",
    "            y = self.y_ohe_train,\n",
    "            batch_size = batch_size,\n",
    "            epochs = epochs,\n",
    "            validation_split = validation_split,\n",
    "            callbacks = callbacks,\n",
    "            class_weight=self.__compute_class_weights() if use_sample_weight else None\n",
    "        )\n",
    "\n",
    "    def test_model(self):\n",
    "\n",
    "        if self.X_test is None:\n",
    "            raise ValueError(\"There is no data to test!\")\n",
    "\n",
    "        predicted_probabilities = self.model.predict([self.X_test, self.features_test])\n",
    "        predicted_labels = np.argmax(predicted_probabilities, axis=-1)\n",
    "        true_labels = np.argmax(self.y_ohe_test, axis=-1)\n",
    "\n",
    "        confusion_mat = confusion_matrix(true_labels, predicted_labels)\n",
    "        precision = precision_score(true_labels, predicted_labels)\n",
    "        recall = recall_score(true_labels, predicted_labels)\n",
    "        f1 = f1_score(true_labels, predicted_labels)\n",
    "\n",
    "        if self.verbose:\n",
    "            colors_palette = [\"#024059\", \"#0388A6\", \"#8C031C\"]\n",
    "            # Build Seaborn palette with cmap=True\n",
    "            palette = sns.light_palette(colors_palette[0], as_cmap=True)\n",
    "            plt.figure(figsize=(6, 5))\n",
    "            sns.heatmap(confusion_mat, xticklabels=self.labels, yticklabels=self.labels, cmap = palette, annot=True, fmt='g')\n",
    "            plt.xlabel('Predicted labels')\n",
    "            plt.ylabel('True labels')\n",
    "            plt.title('Confusion Matrix')\n",
    "            plt.show()\n",
    "            print('Precision {:.5f} Recall {:.5f} F1 {:.5f}'.format(precision, recall, f1))\n",
    "\n",
    "        return confusion_mat, precision, recall, f1\n",
    "\n",
    "    def get_predictions_model(self, X, features):\n",
    "        predicted_probabilities = self.model.predict([X, features])\n",
    "        predicted_labels = np.argmax(predicted_probabilities, axis=-1)\n",
    "        return predicted_labels\n",
    "\n",
    "    def save_model(self, name, path=\"./Models\"):\n",
    "        final_path = os.path.join(path, name+'.keras')\n",
    "        if self.verbose:\n",
    "            print(\"Saving model at\", final_path)\n",
    "        self.model.save(final_path)\n",
    "\n",
    "    def preprocess_data(self, X, features, y):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "caLbubaVT5CU"
   },
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "grKAoBBaT4ip"
   },
   "outputs": [],
   "source": [
    "def get_metrics():\n",
    "    precision =  Precision(name='precision')\n",
    "    recall =  Recall(name='recall')\n",
    "    fn = FalseNegatives(name='fn')\n",
    "    return [precision, recall, fn]\n",
    "\n",
    "def get_callbacks(patience_es=4, patience_lr=3, factor_lr=.1, monitor='val_recall'):\n",
    "    return [\n",
    "        EarlyStopping(\n",
    "            mode = 'max',\n",
    "            monitor=monitor,\n",
    "            patience=patience_es,\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            mode = 'max',\n",
    "            monitor=monitor,\n",
    "            factor=factor_lr,\n",
    "            patience=patience_lr,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "\n",
    "def get_score_model(predicted_labels, true_labels):\n",
    "    confusion_mat = confusion_matrix(true_labels, predicted_labels)\n",
    "    precision = precision_score(true_labels, predicted_labels, average=None)\n",
    "    recall = recall_score(true_labels, predicted_labels, average=None)\n",
    "    f1 = f1_score(true_labels, predicted_labels, average=None)\n",
    "    return confusion_mat, precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gyEKLr4iWqDv"
   },
   "source": [
    "# Model 1: Normal vs Abnormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-rTz503SmdPR"
   },
   "outputs": [],
   "source": [
    "class NormalAbnormalModel(ResNetModel):\n",
    "    def __init__(self, input_shape, num_classes, num_features, N, filters, verbose, labels):\n",
    "        super().__init__(input_shape, num_classes, num_features, N, filters, verbose, labels)\n",
    "\n",
    "    def preprocess_data(self, X, features, y, test_size = None):\n",
    "\n",
    "        map_nor_abn = {'N': [1, 0], 'S': [0, 1], 'V': [0, 1]}\n",
    "        y_ohe = np.array([map_nor_abn[label[0]] for label in y], dtype=np.float32)\n",
    "\n",
    "        if test_size is None:\n",
    "            self.X_train = X\n",
    "            self.y_ohe_train = y_ohe\n",
    "            self.features_train = features\n",
    "        else:\n",
    "            self.X_train, self.X_test, self.y_ohe_train, self.y_ohe_test, self.features_train, self.features_test = train_test_split(\n",
    "                X, y_ohe, features,\n",
    "                test_size=test_size,\n",
    "                stratify=y,\n",
    "                random_state=self.seed,\n",
    "                shuffle=True\n",
    "            )\n",
    "\n",
    "        if self.verbose:\n",
    "            print('X_train.shape {} y_ohe_train.shape {} features_test.shape {}'.format(self.X_train.shape,  self.y_ohe_train.shape, self.features_train.shape))\n",
    "            if test_size is not None:\n",
    "                print('X_test.shape {} y_ohe_test.shape {} features_test.shape {}'.format(self.X_test.shape, self.y_ohe_test.shape, self.features_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jqogpOmBIuKV",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1706609102060,
     "user_tz": -60,
     "elapsed": 579549,
     "user": {
      "displayName": "leonardo cesani",
      "userId": "06996620850007426608"
     }
    },
    "outputId": "3abdda3b-35ca-4495-b305-66777d059ce6"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "X_train.shape (101582, 128, 2) y_ohe_train.shape (101582, 2) features_test.shape (101582, 8)\n",
      "X_test.shape (11287, 128, 2) y_ohe_test.shape (11287, 2) features_test.shape (11287, 8)\n",
      "Epoch 1/20\n",
      "358/358 [==============================] - 61s 100ms/step - loss: 0.0802 - precision: 0.9719 - recall: 0.9719 - fn: 2569.0000 - val_loss: 0.1106 - val_precision: 0.9636 - val_recall: 0.9636 - val_fn: 370.0000 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "358/358 [==============================] - 34s 95ms/step - loss: 0.0715 - precision: 0.9748 - recall: 0.9748 - fn: 2300.0000 - val_loss: 0.0714 - val_precision: 0.9768 - val_recall: 0.9768 - val_fn: 236.0000 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "358/358 [==============================] - 34s 95ms/step - loss: 0.0685 - precision: 0.9764 - recall: 0.9764 - fn: 2160.0000 - val_loss: 0.0783 - val_precision: 0.9721 - val_recall: 0.9721 - val_fn: 283.0000 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "358/358 [==============================] - 34s 95ms/step - loss: 0.0650 - precision: 0.9775 - recall: 0.9775 - fn: 2059.0000 - val_loss: 0.0680 - val_precision: 0.9780 - val_recall: 0.9780 - val_fn: 223.0000 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "358/358 [==============================] - 34s 96ms/step - loss: 0.0635 - precision: 0.9780 - recall: 0.9780 - fn: 2014.0000 - val_loss: 0.0647 - val_precision: 0.9784 - val_recall: 0.9784 - val_fn: 219.0000 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "358/358 [==============================] - 34s 96ms/step - loss: 0.0617 - precision: 0.9789 - recall: 0.9789 - fn: 1932.0000 - val_loss: 0.0642 - val_precision: 0.9757 - val_recall: 0.9757 - val_fn: 247.0000 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "358/358 [==============================] - 34s 94ms/step - loss: 0.0605 - precision: 0.9788 - recall: 0.9788 - fn: 1935.0000 - val_loss: 0.0684 - val_precision: 0.9784 - val_recall: 0.9784 - val_fn: 219.0000 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "357/358 [============================>.] - ETA: 0s - loss: 0.0597 - precision: 0.9792 - recall: 0.9792 - fn: 1903.0000\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "358/358 [==============================] - 34s 95ms/step - loss: 0.0596 - precision: 0.9792 - recall: 0.9792 - fn: 1903.0000 - val_loss: 0.0647 - val_precision: 0.9781 - val_recall: 0.9781 - val_fn: 222.0000 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "358/358 [==============================] - 34s 96ms/step - loss: 0.0551 - precision: 0.9803 - recall: 0.9803 - fn: 1805.0000 - val_loss: 0.0556 - val_precision: 0.9794 - val_recall: 0.9794 - val_fn: 209.0000 - lr: 1.0000e-04\n",
      "Epoch 10/20\n",
      "358/358 [==============================] - 34s 94ms/step - loss: 0.0536 - precision: 0.9807 - recall: 0.9807 - fn: 1764.0000 - val_loss: 0.0553 - val_precision: 0.9790 - val_recall: 0.9790 - val_fn: 213.0000 - lr: 1.0000e-04\n",
      "Epoch 11/20\n",
      "358/358 [==============================] - 34s 95ms/step - loss: 0.0529 - precision: 0.9810 - recall: 0.9810 - fn: 1734.0000 - val_loss: 0.0561 - val_precision: 0.9796 - val_recall: 0.9796 - val_fn: 207.0000 - lr: 1.0000e-04\n",
      "Epoch 12/20\n",
      "358/358 [==============================] - 34s 95ms/step - loss: 0.0521 - precision: 0.9812 - recall: 0.9812 - fn: 1722.0000 - val_loss: 0.0551 - val_precision: 0.9791 - val_recall: 0.9791 - val_fn: 212.0000 - lr: 1.0000e-04\n",
      "Epoch 13/20\n",
      "358/358 [==============================] - 34s 96ms/step - loss: 0.0519 - precision: 0.9814 - recall: 0.9814 - fn: 1700.0000 - val_loss: 0.0560 - val_precision: 0.9787 - val_recall: 0.9787 - val_fn: 216.0000 - lr: 1.0000e-04\n",
      "Epoch 14/20\n",
      "357/358 [============================>.] - ETA: 0s - loss: 0.0513 - precision: 0.9814 - recall: 0.9814 - fn: 1696.0000\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "358/358 [==============================] - 34s 95ms/step - loss: 0.0513 - precision: 0.9814 - recall: 0.9814 - fn: 1697.0000 - val_loss: 0.0559 - val_precision: 0.9791 - val_recall: 0.9791 - val_fn: 212.0000 - lr: 1.0000e-04\n",
      "Epoch 15/20\n",
      "358/358 [==============================] - 34s 95ms/step - loss: 0.0504 - precision: 0.9818 - recall: 0.9818 - fn: 1665.0000 - val_loss: 0.0544 - val_precision: 0.9790 - val_recall: 0.9790 - val_fn: 213.0000 - lr: 1.0000e-05\n",
      "353/353 [==============================] - 4s 8ms/step\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 600x500 with 2 Axes>"
      ],
      "image/png": "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\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Precision 0.90486 Recall 0.88247 F1 0.89353\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(array([[10227,    90],\n",
       "        [  114,   856]]),\n",
       " 0.904862579281184,\n",
       " 0.8824742268041237,\n",
       " 0.8935281837160751)"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "nor_abn_model = NormalAbnormalModel(\n",
    "        input_shape=X.shape[1:],\n",
    "        num_classes=2,\n",
    "        num_features=8,\n",
    "        N=6,\n",
    "        filters=128,\n",
    "        verbose=True,\n",
    "        labels=[\"Normal\", \"Abnormal\"]\n",
    "    )\n",
    "nor_abn_model.preprocess_data(X, features, y, test_size=.1)\n",
    "nor_abn_model.compile_train_model(\n",
    "    metrics = get_metrics(),\n",
    "    loss = KLDivergence(reduction=\"sum_over_batch_size\", name=\"kl_divergence\"),\n",
    "    callbacks = get_callbacks(),\n",
    "    lr=1e-3,\n",
    "    batch_size=256,\n",
    "    epochs=20,\n",
    "    validation_split=.1\n",
    ")\n",
    "nor_abn_model.test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w5Das2gJgWKm"
   },
   "source": [
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1375262,
     "status": "ok",
     "timestamp": 1706392168016,
     "user": {
      "displayName": "A Z",
      "userId": "08254881691717370367"
     },
     "user_tz": -60
    },
    "id": "dGPAwX7ugUpE",
    "outputId": "82b9841a-8bc7-4963-8c38-439e5c51f1c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Epoch 1/20\n",
      "358/358 [==============================] - 43s 52ms/step - loss: 0.0872 - precision: 0.9671 - recall: 0.9671 - fn: 3012.0000 - val_loss: 0.0111 - val_precision: 0.9997 - val_recall: 0.9997 - val_fn: 3.0000 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "358/358 [==============================] - 16s 45ms/step - loss: 0.0743 - precision: 0.9738 - recall: 0.9738 - fn: 2401.0000 - val_loss: 0.0054 - val_precision: 0.9994 - val_recall: 0.9994 - val_fn: 6.0000 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "358/358 [==============================] - 18s 49ms/step - loss: 0.0708 - precision: 0.9750 - recall: 0.9750 - fn: 2288.0000 - val_loss: 0.0070 - val_precision: 0.9963 - val_recall: 0.9963 - val_fn: 38.0000 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "357/358 [============================>.] - ETA: 0s - loss: 0.0685 - precision: 0.9759 - recall: 0.9759 - fn: 2201.0000\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "358/358 [==============================] - 17s 46ms/step - loss: 0.0685 - precision: 0.9759 - recall: 0.9759 - fn: 2203.0000 - val_loss: 0.0062 - val_precision: 0.9966 - val_recall: 0.9966 - val_fn: 35.0000 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "358/358 [==============================] - 16s 45ms/step - loss: 0.0634 - precision: 0.9771 - recall: 0.9771 - fn: 2095.0000 - val_loss: 0.0083 - val_precision: 0.9961 - val_recall: 0.9961 - val_fn: 40.0000 - lr: 1.0000e-04\n",
      "353/353 [==============================] - 3s 7ms/step\n",
      "Fold 2\n",
      "Epoch 1/20\n",
      "358/358 [==============================] - 31s 47ms/step - loss: 0.0874 - precision: 0.9690 - recall: 0.9690 - fn: 2836.0000 - val_loss: 0.0092 - val_precision: 0.9996 - val_recall: 0.9996 - val_fn: 4.0000 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "358/358 [==============================] - 16s 46ms/step - loss: 0.0751 - precision: 0.9733 - recall: 0.9733 - fn: 2446.0000 - val_loss: 0.0066 - val_precision: 0.9995 - val_recall: 0.9995 - val_fn: 5.0000 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "358/358 [==============================] - 16s 45ms/step - loss: 0.0720 - precision: 0.9742 - recall: 0.9742 - fn: 2359.0000 - val_loss: 0.0041 - val_precision: 0.9996 - val_recall: 0.9996 - val_fn: 4.0000 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "358/358 [==============================] - ETA: 0s - loss: 0.0694 - precision: 0.9753 - recall: 0.9753 - fn: 2264.0000\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "358/358 [==============================] - 16s 44ms/step - loss: 0.0694 - precision: 0.9753 - recall: 0.9753 - fn: 2264.0000 - val_loss: 0.0054 - val_precision: 0.9976 - val_recall: 0.9976 - val_fn: 24.0000 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "358/358 [==============================] - 16s 46ms/step - loss: 0.0647 - precision: 0.9765 - recall: 0.9765 - fn: 2153.0000 - val_loss: 0.0049 - val_precision: 0.9982 - val_recall: 0.9982 - val_fn: 18.0000 - lr: 1.0000e-04\n",
      "353/353 [==============================] - 3s 6ms/step\n",
      "Fold 3\n",
      "Epoch 1/20\n",
      "358/358 [==============================] - 32s 48ms/step - loss: 0.0902 - precision: 0.9672 - recall: 0.9672 - fn: 3000.0000 - val_loss: 0.0124 - val_precision: 0.9980 - val_recall: 0.9980 - val_fn: 20.0000 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "358/358 [==============================] - 16s 44ms/step - loss: 0.0785 - precision: 0.9719 - recall: 0.9719 - fn: 2568.0000 - val_loss: 0.0066 - val_precision: 0.9975 - val_recall: 0.9975 - val_fn: 25.0000 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "358/358 [==============================] - 18s 50ms/step - loss: 0.0742 - precision: 0.9742 - recall: 0.9742 - fn: 2364.0000 - val_loss: 0.0042 - val_precision: 0.9997 - val_recall: 0.9997 - val_fn: 3.0000 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "358/358 [==============================] - 16s 46ms/step - loss: 0.0712 - precision: 0.9749 - recall: 0.9749 - fn: 2297.0000 - val_loss: 0.0062 - val_precision: 0.9997 - val_recall: 0.9997 - val_fn: 3.0000 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "358/358 [==============================] - 16s 45ms/step - loss: 0.0709 - precision: 0.9751 - recall: 0.9751 - fn: 2274.0000 - val_loss: 0.0067 - val_precision: 0.9972 - val_recall: 0.9972 - val_fn: 28.0000 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "358/358 [==============================] - ETA: 0s - loss: 0.0691 - precision: 0.9755 - recall: 0.9755 - fn: 2237.0000\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "358/358 [==============================] - 16s 44ms/step - loss: 0.0691 - precision: 0.9755 - recall: 0.9755 - fn: 2237.0000 - val_loss: 0.0041 - val_precision: 0.9992 - val_recall: 0.9992 - val_fn: 8.0000 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "358/358 [==============================] - 16s 45ms/step - loss: 0.0651 - precision: 0.9767 - recall: 0.9767 - fn: 2129.0000 - val_loss: 0.0047 - val_precision: 0.9992 - val_recall: 0.9992 - val_fn: 8.0000 - lr: 1.0000e-04\n",
      "353/353 [==============================] - 3s 6ms/step\n",
      "Fold 4\n",
      "Epoch 1/20\n",
      "358/358 [==============================] - 32s 48ms/step - loss: 0.0905 - precision: 0.9662 - recall: 0.9662 - fn: 3093.0000 - val_loss: 0.0102 - val_precision: 0.9997 - val_recall: 0.9997 - val_fn: 3.0000 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "358/358 [==============================] - 16s 46ms/step - loss: 0.0771 - precision: 0.9731 - recall: 0.9731 - fn: 2462.0000 - val_loss: 0.0054 - val_precision: 0.9997 - val_recall: 0.9997 - val_fn: 3.0000 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "358/358 [==============================] - 16s 44ms/step - loss: 0.0728 - precision: 0.9744 - recall: 0.9744 - fn: 2343.0000 - val_loss: 0.0103 - val_precision: 0.9965 - val_recall: 0.9965 - val_fn: 36.0000 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "357/358 [============================>.] - ETA: 0s - loss: 0.0714 - precision: 0.9745 - recall: 0.9745 - fn: 2330.0000\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "358/358 [==============================] - 16s 44ms/step - loss: 0.0714 - precision: 0.9745 - recall: 0.9745 - fn: 2332.0000 - val_loss: 0.0064 - val_precision: 0.9984 - val_recall: 0.9984 - val_fn: 16.0000 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "358/358 [==============================] - 16s 44ms/step - loss: 0.0655 - precision: 0.9762 - recall: 0.9762 - fn: 2179.0000 - val_loss: 0.0054 - val_precision: 0.9976 - val_recall: 0.9976 - val_fn: 24.0000 - lr: 1.0000e-04\n",
      "353/353 [==============================] - 3s 8ms/step\n",
      "Fold 5\n",
      "Epoch 1/20\n",
      "358/358 [==============================] - 31s 47ms/step - loss: 0.0900 - precision: 0.9670 - recall: 0.9670 - fn: 3021.0000 - val_loss: 0.0042 - val_precision: 0.9997 - val_recall: 0.9997 - val_fn: 3.0000 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "358/358 [==============================] - 16s 44ms/step - loss: 0.0743 - precision: 0.9735 - recall: 0.9735 - fn: 2426.0000 - val_loss: 0.0086 - val_precision: 0.9957 - val_recall: 0.9957 - val_fn: 44.0000 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "358/358 [==============================] - 16s 45ms/step - loss: 0.0703 - precision: 0.9750 - recall: 0.9750 - fn: 2291.0000 - val_loss: 0.0032 - val_precision: 0.9998 - val_recall: 0.9998 - val_fn: 2.0000 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "357/358 [============================>.] - ETA: 0s - loss: 0.0685 - precision: 0.9756 - recall: 0.9756 - fn: 2233.0000\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "358/358 [==============================] - 16s 44ms/step - loss: 0.0685 - precision: 0.9756 - recall: 0.9756 - fn: 2233.0000 - val_loss: 0.0037 - val_precision: 0.9988 - val_recall: 0.9988 - val_fn: 12.0000 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "358/358 [==============================] - 16s 45ms/step - loss: 0.0632 - precision: 0.9771 - recall: 0.9771 - fn: 2094.0000 - val_loss: 0.0055 - val_precision: 0.9979 - val_recall: 0.9979 - val_fn: 21.0000 - lr: 1.0000e-04\n",
      "Epoch 6/20\n",
      "358/358 [==============================] - 16s 44ms/step - loss: 0.0619 - precision: 0.9779 - recall: 0.9779 - fn: 2019.0000 - val_loss: 0.0070 - val_precision: 0.9967 - val_recall: 0.9967 - val_fn: 34.0000 - lr: 1.0000e-04\n",
      "Epoch 7/20\n",
      "358/358 [==============================] - ETA: 0s - loss: 0.0605 - precision: 0.9779 - recall: 0.9779 - fn: 2025.0000\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "358/358 [==============================] - 16s 45ms/step - loss: 0.0605 - precision: 0.9779 - recall: 0.9779 - fn: 2025.0000 - val_loss: 0.0070 - val_precision: 0.9968 - val_recall: 0.9968 - val_fn: 33.0000 - lr: 1.0000e-04\n",
      "353/353 [==============================] - 3s 6ms/step\n",
      "Fold 6\n",
      "Epoch 1/20\n",
      "358/358 [==============================] - 32s 47ms/step - loss: 0.0819 - precision: 0.9694 - recall: 0.9694 - fn: 2802.0000 - val_loss: 0.0101 - val_precision: 0.9994 - val_recall: 0.9994 - val_fn: 6.0000 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "358/358 [==============================] - 16s 44ms/step - loss: 0.0718 - precision: 0.9747 - recall: 0.9747 - fn: 2313.0000 - val_loss: 0.0079 - val_precision: 0.9995 - val_recall: 0.9995 - val_fn: 5.0000 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "358/358 [==============================] - 16s 45ms/step - loss: 0.0698 - precision: 0.9756 - recall: 0.9756 - fn: 2233.0000 - val_loss: 0.0036 - val_precision: 0.9994 - val_recall: 0.9994 - val_fn: 6.0000 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "358/358 [==============================] - 16s 45ms/step - loss: 0.0677 - precision: 0.9756 - recall: 0.9756 - fn: 2236.0000 - val_loss: 0.0020 - val_precision: 0.9997 - val_recall: 0.9997 - val_fn: 3.0000 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "358/358 [==============================] - 17s 46ms/step - loss: 0.0656 - precision: 0.9766 - recall: 0.9766 - fn: 2142.0000 - val_loss: 0.0126 - val_precision: 0.9971 - val_recall: 0.9971 - val_fn: 29.0000 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "358/358 [==============================] - 16s 44ms/step - loss: 0.0647 - precision: 0.9772 - recall: 0.9772 - fn: 2082.0000 - val_loss: 0.0128 - val_precision: 0.9944 - val_recall: 0.9944 - val_fn: 57.0000 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "357/358 [============================>.] - ETA: 0s - loss: 0.0637 - precision: 0.9767 - recall: 0.9767 - fn: 2127.0000\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "358/358 [==============================] - 16s 44ms/step - loss: 0.0637 - precision: 0.9767 - recall: 0.9767 - fn: 2128.0000 - val_loss: 0.0052 - val_precision: 0.9982 - val_recall: 0.9982 - val_fn: 18.0000 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "358/358 [==============================] - 16s 46ms/step - loss: 0.0589 - precision: 0.9787 - recall: 0.9787 - fn: 1951.0000 - val_loss: 0.0107 - val_precision: 0.9965 - val_recall: 0.9965 - val_fn: 36.0000 - lr: 1.0000e-04\n",
      "353/353 [==============================] - 4s 6ms/step\n",
      "Fold 7\n",
      "Epoch 1/20\n",
      "358/358 [==============================] - 31s 47ms/step - loss: 0.0911 - precision: 0.9668 - recall: 0.9668 - fn: 3040.0000 - val_loss: 0.0051 - val_precision: 0.9989 - val_recall: 0.9989 - val_fn: 11.0000 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "358/358 [==============================] - 17s 47ms/step - loss: 0.0773 - precision: 0.9728 - recall: 0.9728 - fn: 2492.0000 - val_loss: 0.0044 - val_precision: 0.9995 - val_recall: 0.9995 - val_fn: 5.0000 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "358/358 [==============================] - 16s 45ms/step - loss: 0.0746 - precision: 0.9734 - recall: 0.9734 - fn: 2435.0000 - val_loss: 0.0067 - val_precision: 0.9993 - val_recall: 0.9993 - val_fn: 7.0000 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "358/358 [==============================] - 16s 46ms/step - loss: 0.0711 - precision: 0.9751 - recall: 0.9751 - fn: 2280.0000 - val_loss: 0.0128 - val_precision: 0.9959 - val_recall: 0.9959 - val_fn: 42.0000 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "357/358 [============================>.] - ETA: 0s - loss: 0.0694 - precision: 0.9755 - recall: 0.9755 - fn: 2243.0000\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "358/358 [==============================] - 16s 45ms/step - loss: 0.0693 - precision: 0.9755 - recall: 0.9755 - fn: 2244.0000 - val_loss: 0.0084 - val_precision: 0.9972 - val_recall: 0.9972 - val_fn: 28.0000 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "358/358 [==============================] - 16s 44ms/step - loss: 0.0649 - precision: 0.9769 - recall: 0.9769 - fn: 2109.0000 - val_loss: 0.0054 - val_precision: 0.9989 - val_recall: 0.9989 - val_fn: 11.0000 - lr: 1.0000e-04\n",
      "353/353 [==============================] - 3s 6ms/step\n",
      "Fold 8\n",
      "Epoch 1/20\n",
      "358/358 [==============================] - 32s 50ms/step - loss: 0.0875 - precision: 0.9671 - recall: 0.9671 - fn: 3014.0000 - val_loss: 0.0216 - val_precision: 0.9987 - val_recall: 0.9987 - val_fn: 13.0000 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "358/358 [==============================] - 16s 44ms/step - loss: 0.0734 - precision: 0.9741 - recall: 0.9741 - fn: 2369.0000 - val_loss: 0.0077 - val_precision: 0.9993 - val_recall: 0.9993 - val_fn: 7.0000 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "358/358 [==============================] - 16s 45ms/step - loss: 0.0711 - precision: 0.9746 - recall: 0.9746 - fn: 2327.0000 - val_loss: 0.0079 - val_precision: 0.9966 - val_recall: 0.9966 - val_fn: 35.0000 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "358/358 [==============================] - 16s 45ms/step - loss: 0.0681 - precision: 0.9757 - recall: 0.9757 - fn: 2226.0000 - val_loss: 0.0096 - val_precision: 0.9958 - val_recall: 0.9958 - val_fn: 43.0000 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "357/358 [============================>.] - ETA: 0s - loss: 0.0674 - precision: 0.9762 - recall: 0.9762 - fn: 2179.0000\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "358/358 [==============================] - 16s 44ms/step - loss: 0.0673 - precision: 0.9761 - recall: 0.9761 - fn: 2182.0000 - val_loss: 0.0100 - val_precision: 0.9967 - val_recall: 0.9967 - val_fn: 34.0000 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "358/358 [==============================] - 16s 45ms/step - loss: 0.0624 - precision: 0.9777 - recall: 0.9777 - fn: 2037.0000 - val_loss: 0.0062 - val_precision: 0.9985 - val_recall: 0.9985 - val_fn: 15.0000 - lr: 1.0000e-04\n",
      "353/353 [==============================] - 3s 7ms/step\n",
      "Fold 9\n",
      "Epoch 1/20\n",
      "358/358 [==============================] - 32s 47ms/step - loss: 0.0884 - precision: 0.9679 - recall: 0.9679 - fn: 2934.0000 - val_loss: 0.0084 - val_precision: 0.9993 - val_recall: 0.9993 - val_fn: 7.0000 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "358/358 [==============================] - 16s 45ms/step - loss: 0.0759 - precision: 0.9731 - recall: 0.9731 - fn: 2459.0000 - val_loss: 0.0030 - val_precision: 0.9998 - val_recall: 0.9998 - val_fn: 2.0000 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "358/358 [==============================] - 16s 45ms/step - loss: 0.0725 - precision: 0.9741 - recall: 0.9741 - fn: 2372.0000 - val_loss: 0.0067 - val_precision: 0.9998 - val_recall: 0.9998 - val_fn: 2.0000 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "358/358 [==============================] - 16s 45ms/step - loss: 0.0698 - precision: 0.9751 - recall: 0.9751 - fn: 2280.0000 - val_loss: 0.0074 - val_precision: 0.9981 - val_recall: 0.9981 - val_fn: 19.0000 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "357/358 [============================>.] - ETA: 0s - loss: 0.0680 - precision: 0.9761 - recall: 0.9761 - fn: 2184.0000\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "358/358 [==============================] - 16s 45ms/step - loss: 0.0679 - precision: 0.9761 - recall: 0.9761 - fn: 2185.0000 - val_loss: 0.0074 - val_precision: 0.9969 - val_recall: 0.9969 - val_fn: 32.0000 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "358/358 [==============================] - 17s 46ms/step - loss: 0.0629 - precision: 0.9776 - recall: 0.9776 - fn: 2051.0000 - val_loss: 0.0086 - val_precision: 0.9963 - val_recall: 0.9963 - val_fn: 38.0000 - lr: 1.0000e-04\n",
      "353/353 [==============================] - 3s 6ms/step\n",
      "Fold 10\n",
      "Epoch 1/20\n",
      "358/358 [==============================] - 31s 48ms/step - loss: 0.0943 - precision: 0.9664 - recall: 0.9664 - fn: 3075.0000 - val_loss: 0.0087 - val_precision: 0.9995 - val_recall: 0.9995 - val_fn: 5.0000 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "358/358 [==============================] - 17s 49ms/step - loss: 0.0802 - precision: 0.9717 - recall: 0.9717 - fn: 2585.0000 - val_loss: 0.0050 - val_precision: 0.9997 - val_recall: 0.9997 - val_fn: 3.0000 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "358/358 [==============================] - 16s 44ms/step - loss: 0.0777 - precision: 0.9731 - recall: 0.9731 - fn: 2457.0000 - val_loss: 0.0082 - val_precision: 0.9963 - val_recall: 0.9963 - val_fn: 38.0000 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "358/358 [==============================] - 16s 45ms/step - loss: 0.0739 - precision: 0.9745 - recall: 0.9745 - fn: 2336.0000 - val_loss: 0.0053 - val_precision: 0.9996 - val_recall: 0.9996 - val_fn: 4.0000 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "358/358 [==============================] - ETA: 0s - loss: 0.0716 - precision: 0.9752 - recall: 0.9752 - fn: 2272.0000\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "358/358 [==============================] - 16s 44ms/step - loss: 0.0716 - precision: 0.9752 - recall: 0.9752 - fn: 2272.0000 - val_loss: 0.0051 - val_precision: 0.9997 - val_recall: 0.9997 - val_fn: 3.0000 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "358/358 [==============================] - 16s 46ms/step - loss: 0.0671 - precision: 0.9761 - recall: 0.9761 - fn: 2184.0000 - val_loss: 0.0039 - val_precision: 0.9992 - val_recall: 0.9992 - val_fn: 8.0000 - lr: 1.0000e-04\n",
      "353/353 [==============================] - 3s 6ms/step\n"
     ]
    }
   ],
   "source": [
    "metrics = {'folds':[]}\n",
    "i = 1\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "for train_ids, test_ids in kf.split(np.arange(len(X))):\n",
    "\n",
    "    print(\"Fold\", i)\n",
    "    i = i+1\n",
    "\n",
    "    X_train = X[train_ids]\n",
    "    X_test = X[test_ids]\n",
    "    features_train = features[train_ids]\n",
    "    features_test = features[test_ids]\n",
    "    y_train = y[train_ids]\n",
    "    y_test = y[test_ids]\n",
    "\n",
    "    train_uniques, train_counts = np.unique(y_train, return_counts=True)\n",
    "    test_uniques, test_counts = np.unique(y_test, return_counts=True)\n",
    "    fold_metrics = {'train_unique':train_uniques.tolist(), 'train_dist':train_counts.tolist(), 'test_unique':test_uniques.tolist(), 'test_dist':test_counts.tolist()}\n",
    "\n",
    "    nor_abn_model = NormalAbnormalModel(\n",
    "        input_shape=X_train.shape[1:],\n",
    "        num_classes=2,\n",
    "        num_features=8,\n",
    "        N=6,\n",
    "        filters=128,\n",
    "        verbose=False,\n",
    "        labels=[\"Normal\", \"Abnormal\"]\n",
    "    )\n",
    "    nor_abn_model.preprocess_data(X_train, features_train, y_train)\n",
    "    nor_abn_model.compile_train_model(\n",
    "        metrics = get_metrics(),\n",
    "        loss = KLDivergence(reduction=\"sum_over_batch_size\", name=\"kl_divergence\"),\n",
    "        callbacks = get_callbacks(),\n",
    "        lr=1e-3,\n",
    "        batch_size=256,\n",
    "        epochs=20,\n",
    "        validation_split=.1\n",
    "    )\n",
    "\n",
    "    # Evaluate models on unseen data\n",
    "    pred_nor_abn = nor_abn_model.get_predictions_model(X_test, features_test)\n",
    "    map_labels = {'N': 0, 'V': 1, 'S': 1}\n",
    "    y_real = np.array([map_labels[label[0]] for label in y_test], dtype=np.float32)\n",
    "\n",
    "    confusion_mat, precision, recall, f1 = get_score_model(pred_nor_abn, y_real)\n",
    "    fold_metrics['conf_matrix'] = confusion_mat.tolist()\n",
    "    fold_metrics['precision'] = precision.tolist()\n",
    "    fold_metrics['recall'] = recall.tolist()\n",
    "    fold_metrics['f1'] = f1.tolist()\n",
    "    metrics['folds'].append(fold_metrics)\n",
    "\n",
    "\n",
    "with open('./Metrics/metrics_cv_nor_abn_model.json', 'w') as jsonfile:\n",
    "    json.dump(metrics, jsonfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6LN5uqufXGJu"
   },
   "source": [
    "# Model 2: Ventricular vs Sopraventricular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fLcxq-_HXAp7"
   },
   "outputs": [],
   "source": [
    "class VentricularSopraventricularModel(ResNetModel):\n",
    "    def __init__(self, input_shape, num_classes, num_features, N, filters, verbose, labels):\n",
    "        super().__init__(input_shape, num_classes, num_features, N, filters, verbose, labels)\n",
    "\n",
    "    def preprocess_data(self, X, features, y, test_size = None):\n",
    "\n",
    "        # Remove normal beats\n",
    "        normal_indices = np.where(y == 'N')[0]\n",
    "        X = np.delete(X, normal_indices, axis=0)\n",
    "        features = np.delete(features, normal_indices, axis=0)\n",
    "        y = np.delete(y, normal_indices, axis=0)\n",
    "\n",
    "        map_v_s = {'V': [1, 0], 'S': [0, 1]}\n",
    "        y_ohe = np.array([map_v_s[label[0]] for label in y], dtype=np.float32)\n",
    "\n",
    "        if test_size is None:\n",
    "            self.X_train = X\n",
    "            self.y_ohe_train = y_ohe\n",
    "            self.features_train = features\n",
    "        else:\n",
    "            self.X_train, self.X_test, self.y_ohe_train, self.y_ohe_test, self.features_train, self.features_test = train_test_split(\n",
    "                X, y_ohe, features,\n",
    "                test_size=test_size,\n",
    "                stratify=y,\n",
    "                random_state=self.seed,\n",
    "                shuffle=True\n",
    "            )\n",
    "\n",
    "        if self.verbose:\n",
    "            print('X_train.shape {} y_ohe_train.shape {} features_test.shape {}'.format(self.X_train.shape,  self.y_ohe_train.shape, self.features_train.shape))\n",
    "            if test_size is not None:\n",
    "                print('X_test.shape {} y_ohe_test.shape {} features_test.shape {}'.format(self.X_test.shape, self.y_ohe_test.shape, self.features_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 139017,
     "status": "ok",
     "timestamp": 1706451362889,
     "user": {
      "displayName": "A Z",
      "userId": "08254881691717370367"
     },
     "user_tz": -60
    },
    "id": "Krror-GMO_4R",
    "outputId": "ffbce061-20fa-4833-bc31-8ba3410b55dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape (8727, 128, 2) y_ohe_train.shape (8727, 2) features_test.shape (8727, 8)\n",
      "X_test.shape (970, 128, 2) y_ohe_test.shape (970, 2) features_test.shape (970, 8)\n",
      "Epoch 1/70\n",
      "62/62 [==============================] - 14s 38ms/step - loss: 0.5655 - precision: 0.6808 - recall: 0.6808 - fn: 2507.0000 - val_loss: 1.3112 - val_precision: 0.4353 - val_recall: 0.4353 - val_fn: 493.0000 - lr: 0.0010\n",
      "Epoch 2/70\n",
      "62/62 [==============================] - 2s 27ms/step - loss: 0.5254 - precision: 0.7170 - recall: 0.7170 - fn: 2223.0000 - val_loss: 0.9360 - val_precision: 0.4387 - val_recall: 0.4387 - val_fn: 490.0000 - lr: 0.0010\n",
      "Epoch 3/70\n",
      "62/62 [==============================] - 2s 27ms/step - loss: 0.5037 - precision: 0.7306 - recall: 0.7306 - fn: 2116.0000 - val_loss: 0.7683 - val_precision: 0.6334 - val_recall: 0.6334 - val_fn: 320.0000 - lr: 0.0010\n",
      "Epoch 4/70\n",
      "62/62 [==============================] - 2s 35ms/step - loss: 0.4984 - precision: 0.7367 - recall: 0.7367 - fn: 2068.0000 - val_loss: 0.9636 - val_precision: 0.4364 - val_recall: 0.4364 - val_fn: 492.0000 - lr: 0.0010\n",
      "Epoch 5/70\n",
      "62/62 [==============================] - 2s 35ms/step - loss: 0.4812 - precision: 0.7506 - recall: 0.7506 - fn: 1959.0000 - val_loss: 0.6551 - val_precision: 0.6403 - val_recall: 0.6403 - val_fn: 314.0000 - lr: 0.0010\n",
      "Epoch 6/70\n",
      "62/62 [==============================] - 2s 28ms/step - loss: 0.4799 - precision: 0.7506 - recall: 0.7506 - fn: 1959.0000 - val_loss: 0.6053 - val_precision: 0.6850 - val_recall: 0.6850 - val_fn: 275.0000 - lr: 0.0010\n",
      "Epoch 7/70\n",
      "62/62 [==============================] - 2s 28ms/step - loss: 0.4677 - precision: 0.7641 - recall: 0.7641 - fn: 1853.0000 - val_loss: 0.7153 - val_precision: 0.6575 - val_recall: 0.6575 - val_fn: 299.0000 - lr: 0.0010\n",
      "Epoch 8/70\n",
      "62/62 [==============================] - 2s 34ms/step - loss: 0.4609 - precision: 0.7711 - recall: 0.7711 - fn: 1798.0000 - val_loss: 0.6605 - val_precision: 0.6953 - val_recall: 0.6953 - val_fn: 266.0000 - lr: 0.0010\n",
      "Epoch 9/70\n",
      "62/62 [==============================] - 2s 38ms/step - loss: 0.4510 - precision: 0.7753 - recall: 0.7753 - fn: 1765.0000 - val_loss: 0.6116 - val_precision: 0.7045 - val_recall: 0.7045 - val_fn: 258.0000 - lr: 0.0010\n",
      "Epoch 10/70\n",
      "62/62 [==============================] - 2s 31ms/step - loss: 0.4380 - precision: 0.7827 - recall: 0.7827 - fn: 1707.0000 - val_loss: 0.5246 - val_precision: 0.7274 - val_recall: 0.7274 - val_fn: 238.0000 - lr: 0.0010\n",
      "Epoch 11/70\n",
      "62/62 [==============================] - 2s 36ms/step - loss: 0.4411 - precision: 0.7805 - recall: 0.7805 - fn: 1724.0000 - val_loss: 0.5366 - val_precision: 0.7274 - val_recall: 0.7274 - val_fn: 238.0000 - lr: 0.0010\n",
      "Epoch 12/70\n",
      "62/62 [==============================] - 2s 26ms/step - loss: 0.4385 - precision: 0.7783 - recall: 0.7783 - fn: 1741.0000 - val_loss: 0.5681 - val_precision: 0.7056 - val_recall: 0.7056 - val_fn: 257.0000 - lr: 0.0010\n",
      "Epoch 13/70\n",
      "62/62 [==============================] - 2s 26ms/step - loss: 0.4197 - precision: 0.7898 - recall: 0.7898 - fn: 1651.0000 - val_loss: 0.5290 - val_precision: 0.7549 - val_recall: 0.7549 - val_fn: 214.0000 - lr: 0.0010\n",
      "Epoch 14/70\n",
      "62/62 [==============================] - 2s 26ms/step - loss: 0.4218 - precision: 0.7935 - recall: 0.7935 - fn: 1622.0000 - val_loss: 0.4834 - val_precision: 0.7434 - val_recall: 0.7434 - val_fn: 224.0000 - lr: 0.0010\n",
      "Epoch 15/70\n",
      "62/62 [==============================] - 2s 25ms/step - loss: 0.4035 - precision: 0.8026 - recall: 0.8026 - fn: 1550.0000 - val_loss: 0.5121 - val_precision: 0.7388 - val_recall: 0.7388 - val_fn: 228.0000 - lr: 0.0010\n",
      "Epoch 16/70\n",
      "62/62 [==============================] - 2s 26ms/step - loss: 0.4030 - precision: 0.8015 - recall: 0.8015 - fn: 1559.0000 - val_loss: 0.5000 - val_precision: 0.7560 - val_recall: 0.7560 - val_fn: 213.0000 - lr: 0.0010\n",
      "Epoch 17/70\n",
      "62/62 [==============================] - 2s 26ms/step - loss: 0.3948 - precision: 0.8111 - recall: 0.8111 - fn: 1484.0000 - val_loss: 0.4825 - val_precision: 0.7583 - val_recall: 0.7583 - val_fn: 211.0000 - lr: 0.0010\n",
      "Epoch 18/70\n",
      "62/62 [==============================] - 2s 34ms/step - loss: 0.3861 - precision: 0.8161 - recall: 0.8161 - fn: 1444.0000 - val_loss: 0.5278 - val_precision: 0.7572 - val_recall: 0.7572 - val_fn: 212.0000 - lr: 0.0010\n",
      "Epoch 19/70\n",
      "62/62 [==============================] - 2s 30ms/step - loss: 0.3811 - precision: 0.8158 - recall: 0.8158 - fn: 1447.0000 - val_loss: 0.5481 - val_precision: 0.7308 - val_recall: 0.7308 - val_fn: 235.0000 - lr: 0.0010\n",
      "Epoch 20/70\n",
      "62/62 [==============================] - 2s 26ms/step - loss: 0.3715 - precision: 0.8279 - recall: 0.8279 - fn: 1352.0000 - val_loss: 0.6045 - val_precision: 0.7262 - val_recall: 0.7262 - val_fn: 239.0000 - lr: 0.0010\n",
      "Epoch 21/70\n",
      "62/62 [==============================] - 2s 26ms/step - loss: 0.3756 - precision: 0.8238 - recall: 0.8238 - fn: 1384.0000 - val_loss: 0.4746 - val_precision: 0.7801 - val_recall: 0.7801 - val_fn: 192.0000 - lr: 0.0010\n",
      "Epoch 22/70\n",
      "62/62 [==============================] - 2s 26ms/step - loss: 0.3651 - precision: 0.8266 - recall: 0.8266 - fn: 1362.0000 - val_loss: 0.4171 - val_precision: 0.8030 - val_recall: 0.8030 - val_fn: 172.0000 - lr: 0.0010\n",
      "Epoch 23/70\n",
      "62/62 [==============================] - 2s 26ms/step - loss: 0.3538 - precision: 0.8321 - recall: 0.8321 - fn: 1319.0000 - val_loss: 0.4726 - val_precision: 0.7709 - val_recall: 0.7709 - val_fn: 200.0000 - lr: 0.0010\n",
      "Epoch 24/70\n",
      "62/62 [==============================] - 2s 26ms/step - loss: 0.3567 - precision: 0.8364 - recall: 0.8364 - fn: 1285.0000 - val_loss: 0.4129 - val_precision: 0.8110 - val_recall: 0.8110 - val_fn: 165.0000 - lr: 0.0010\n",
      "Epoch 25/70\n",
      "62/62 [==============================] - 2s 31ms/step - loss: 0.3422 - precision: 0.8426 - recall: 0.8426 - fn: 1236.0000 - val_loss: 0.5631 - val_precision: 0.7136 - val_recall: 0.7136 - val_fn: 250.0000 - lr: 0.0010\n",
      "Epoch 26/70\n",
      "62/62 [==============================] - 2s 34ms/step - loss: 0.3408 - precision: 0.8408 - recall: 0.8408 - fn: 1250.0000 - val_loss: 0.4962 - val_precision: 0.7583 - val_recall: 0.7583 - val_fn: 211.0000 - lr: 0.0010\n",
      "Epoch 27/70\n",
      "62/62 [==============================] - 2s 24ms/step - loss: 0.3367 - precision: 0.8378 - recall: 0.8378 - fn: 1274.0000 - val_loss: 0.4234 - val_precision: 0.7812 - val_recall: 0.7812 - val_fn: 191.0000 - lr: 0.0010\n",
      "Epoch 28/70\n",
      "62/62 [==============================] - 2s 25ms/step - loss: 0.3334 - precision: 0.8464 - recall: 0.8464 - fn: 1206.0000 - val_loss: 0.4120 - val_precision: 0.7995 - val_recall: 0.7995 - val_fn: 175.0000 - lr: 0.0010\n",
      "Epoch 29/70\n",
      "62/62 [==============================] - 2s 25ms/step - loss: 0.3259 - precision: 0.8519 - recall: 0.8519 - fn: 1163.0000 - val_loss: 0.5091 - val_precision: 0.7652 - val_recall: 0.7652 - val_fn: 205.0000 - lr: 0.0010\n",
      "Epoch 30/70\n",
      "62/62 [==============================] - 2s 26ms/step - loss: 0.3229 - precision: 0.8487 - recall: 0.8487 - fn: 1188.0000 - val_loss: 0.4662 - val_precision: 0.8041 - val_recall: 0.8041 - val_fn: 171.0000 - lr: 0.0010\n",
      "Epoch 31/70\n",
      "62/62 [==============================] - 2s 26ms/step - loss: 0.3191 - precision: 0.8547 - recall: 0.8547 - fn: 1141.0000 - val_loss: 0.4525 - val_precision: 0.7927 - val_recall: 0.7927 - val_fn: 181.0000 - lr: 0.0010\n",
      "Epoch 32/70\n",
      "62/62 [==============================] - 2s 27ms/step - loss: 0.3080 - precision: 0.8605 - recall: 0.8605 - fn: 1096.0000 - val_loss: 0.4422 - val_precision: 0.8213 - val_recall: 0.8213 - val_fn: 156.0000 - lr: 0.0010\n",
      "Epoch 33/70\n",
      "62/62 [==============================] - 2s 36ms/step - loss: 0.3116 - precision: 0.8603 - recall: 0.8603 - fn: 1097.0000 - val_loss: 0.3727 - val_precision: 0.8385 - val_recall: 0.8385 - val_fn: 141.0000 - lr: 0.0010\n",
      "Epoch 34/70\n",
      "62/62 [==============================] - 2s 29ms/step - loss: 0.2984 - precision: 0.8644 - recall: 0.8644 - fn: 1065.0000 - val_loss: 0.3997 - val_precision: 0.8316 - val_recall: 0.8316 - val_fn: 147.0000 - lr: 0.0010\n",
      "Epoch 35/70\n",
      "62/62 [==============================] - 2s 26ms/step - loss: 0.2963 - precision: 0.8658 - recall: 0.8658 - fn: 1054.0000 - val_loss: 0.4581 - val_precision: 0.7938 - val_recall: 0.7938 - val_fn: 180.0000 - lr: 0.0010\n",
      "Epoch 36/70\n",
      "62/62 [==============================] - 2s 25ms/step - loss: 0.3002 - precision: 0.8627 - recall: 0.8627 - fn: 1078.0000 - val_loss: 0.4192 - val_precision: 0.8076 - val_recall: 0.8076 - val_fn: 168.0000 - lr: 0.0010\n",
      "Epoch 37/70\n",
      "62/62 [==============================] - 2s 28ms/step - loss: 0.2956 - precision: 0.8691 - recall: 0.8691 - fn: 1028.0000 - val_loss: 0.4479 - val_precision: 0.7961 - val_recall: 0.7961 - val_fn: 178.0000 - lr: 0.0010\n",
      "Epoch 38/70\n",
      "62/62 [==============================] - 2s 28ms/step - loss: 0.2923 - precision: 0.8676 - recall: 0.8676 - fn: 1040.0000 - val_loss: 0.4714 - val_precision: 0.7766 - val_recall: 0.7766 - val_fn: 195.0000 - lr: 0.0010\n",
      "Epoch 39/70\n",
      "62/62 [==============================] - 2s 28ms/step - loss: 0.2859 - precision: 0.8683 - recall: 0.8683 - fn: 1034.0000 - val_loss: 0.4530 - val_precision: 0.8030 - val_recall: 0.8030 - val_fn: 172.0000 - lr: 0.0010\n",
      "Epoch 40/70\n",
      "62/62 [==============================] - 2s 36ms/step - loss: 0.2797 - precision: 0.8748 - recall: 0.8748 - fn: 983.0000 - val_loss: 0.4751 - val_precision: 0.7847 - val_recall: 0.7847 - val_fn: 188.0000 - lr: 0.0010\n",
      "Epoch 41/70\n",
      "62/62 [==============================] - 2s 33ms/step - loss: 0.2681 - precision: 0.8818 - recall: 0.8818 - fn: 928.0000 - val_loss: 0.5617 - val_precision: 0.7675 - val_recall: 0.7675 - val_fn: 203.0000 - lr: 0.0010\n",
      "Epoch 42/70\n",
      "62/62 [==============================] - 2s 28ms/step - loss: 0.2679 - precision: 0.8775 - recall: 0.8775 - fn: 962.0000 - val_loss: 0.4227 - val_precision: 0.8190 - val_recall: 0.8190 - val_fn: 158.0000 - lr: 0.0010\n",
      "Epoch 43/70\n",
      "62/62 [==============================] - 2s 29ms/step - loss: 0.2706 - precision: 0.8841 - recall: 0.8841 - fn: 910.0000 - val_loss: 0.4570 - val_precision: 0.8144 - val_recall: 0.8144 - val_fn: 162.0000 - lr: 0.0010\n",
      "Epoch 44/70\n",
      "62/62 [==============================] - 2s 28ms/step - loss: 0.2521 - precision: 0.8906 - recall: 0.8906 - fn: 859.0000 - val_loss: 0.4675 - val_precision: 0.8270 - val_recall: 0.8270 - val_fn: 151.0000 - lr: 0.0010\n",
      "Epoch 45/70\n",
      "62/62 [==============================] - 2s 26ms/step - loss: 0.2550 - precision: 0.8908 - recall: 0.8908 - fn: 858.0000 - val_loss: 0.3976 - val_precision: 0.8270 - val_recall: 0.8270 - val_fn: 151.0000 - lr: 0.0010\n",
      "Epoch 46/70\n",
      "62/62 [==============================] - 2s 27ms/step - loss: 0.2470 - precision: 0.8936 - recall: 0.8936 - fn: 836.0000 - val_loss: 0.4890 - val_precision: 0.8018 - val_recall: 0.8018 - val_fn: 173.0000 - lr: 0.0010\n",
      "Epoch 47/70\n",
      "62/62 [==============================] - 2s 33ms/step - loss: 0.2533 - precision: 0.8878 - recall: 0.8878 - fn: 881.0000 - val_loss: 0.5050 - val_precision: 0.7835 - val_recall: 0.7835 - val_fn: 189.0000 - lr: 0.0010\n",
      "Epoch 48/70\n",
      "62/62 [==============================] - 2s 33ms/step - loss: 0.2336 - precision: 0.8970 - recall: 0.8970 - fn: 809.0000 - val_loss: 0.4994 - val_precision: 0.8018 - val_recall: 0.8018 - val_fn: 173.0000 - lr: 0.0010\n",
      "Epoch 49/70\n",
      "62/62 [==============================] - 2s 26ms/step - loss: 0.2346 - precision: 0.9009 - recall: 0.9009 - fn: 778.0000 - val_loss: 0.4383 - val_precision: 0.8282 - val_recall: 0.8282 - val_fn: 150.0000 - lr: 0.0010\n",
      "Epoch 50/70\n",
      "62/62 [==============================] - 2s 26ms/step - loss: 0.2329 - precision: 0.8981 - recall: 0.8981 - fn: 800.0000 - val_loss: 0.3990 - val_precision: 0.8316 - val_recall: 0.8316 - val_fn: 147.0000 - lr: 0.0010\n",
      "Epoch 51/70\n",
      "62/62 [==============================] - 2s 26ms/step - loss: 0.2219 - precision: 0.9049 - recall: 0.9049 - fn: 747.0000 - val_loss: 0.4199 - val_precision: 0.8121 - val_recall: 0.8121 - val_fn: 164.0000 - lr: 0.0010\n",
      "Epoch 52/70\n",
      "62/62 [==============================] - 2s 26ms/step - loss: 0.2265 - precision: 0.9029 - recall: 0.9029 - fn: 763.0000 - val_loss: 0.5041 - val_precision: 0.7824 - val_recall: 0.7824 - val_fn: 190.0000 - lr: 0.0010\n",
      "Epoch 53/70\n",
      "62/62 [==============================] - 2s 26ms/step - loss: 0.2256 - precision: 0.9036 - recall: 0.9036 - fn: 757.0000 - val_loss: 0.4659 - val_precision: 0.8121 - val_recall: 0.8121 - val_fn: 164.0000 - lr: 0.0010\n",
      "Epoch 54/70\n",
      "62/62 [==============================] - 2s 30ms/step - loss: 0.2108 - precision: 0.9087 - recall: 0.9087 - fn: 717.0000 - val_loss: 0.4418 - val_precision: 0.8316 - val_recall: 0.8316 - val_fn: 147.0000 - lr: 0.0010\n",
      "Epoch 55/70\n",
      "62/62 [==============================] - 2s 37ms/step - loss: 0.2178 - precision: 0.9049 - recall: 0.9049 - fn: 747.0000 - val_loss: 0.6906 - val_precision: 0.7709 - val_recall: 0.7709 - val_fn: 200.0000 - lr: 0.0010\n",
      "Epoch 56/70\n",
      "62/62 [==============================] - 2s 25ms/step - loss: 0.2081 - precision: 0.9113 - recall: 0.9113 - fn: 697.0000 - val_loss: 0.4932 - val_precision: 0.7973 - val_recall: 0.7973 - val_fn: 177.0000 - lr: 0.0010\n",
      "Epoch 57/70\n",
      "62/62 [==============================] - 2s 26ms/step - loss: 0.2054 - precision: 0.9128 - recall: 0.9128 - fn: 685.0000 - val_loss: 0.5008 - val_precision: 0.8247 - val_recall: 0.8247 - val_fn: 153.0000 - lr: 0.0010\n",
      "Epoch 58/70\n",
      "60/62 [============================>.] - ETA: 0s - loss: 0.1995 - precision: 0.9146 - recall: 0.9146 - fn: 656.0000\n",
      "Epoch 58: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "62/62 [==============================] - 2s 26ms/step - loss: 0.1996 - precision: 0.9148 - recall: 0.9148 - fn: 669.0000 - val_loss: 0.4864 - val_precision: 0.8259 - val_recall: 0.8259 - val_fn: 152.0000 - lr: 0.0010\n",
      "Epoch 59/70\n",
      "62/62 [==============================] - 2s 26ms/step - loss: 0.1682 - precision: 0.9321 - recall: 0.9321 - fn: 533.0000 - val_loss: 0.4210 - val_precision: 0.8477 - val_recall: 0.8477 - val_fn: 133.0000 - lr: 1.0000e-04\n",
      "Epoch 60/70\n",
      "62/62 [==============================] - 2s 26ms/step - loss: 0.1481 - precision: 0.9404 - recall: 0.9404 - fn: 468.0000 - val_loss: 0.4195 - val_precision: 0.8477 - val_recall: 0.8477 - val_fn: 133.0000 - lr: 1.0000e-04\n",
      "Epoch 61/70\n",
      "62/62 [==============================] - 2s 25ms/step - loss: 0.1393 - precision: 0.9437 - recall: 0.9437 - fn: 442.0000 - val_loss: 0.4392 - val_precision: 0.8419 - val_recall: 0.8419 - val_fn: 138.0000 - lr: 1.0000e-04\n",
      "Epoch 62/70\n",
      "62/62 [==============================] - 2s 35ms/step - loss: 0.1405 - precision: 0.9419 - recall: 0.9419 - fn: 456.0000 - val_loss: 0.4480 - val_precision: 0.8396 - val_recall: 0.8396 - val_fn: 140.0000 - lr: 1.0000e-04\n",
      "Epoch 63/70\n",
      "62/62 [==============================] - 2s 31ms/step - loss: 0.1377 - precision: 0.9417 - recall: 0.9417 - fn: 458.0000 - val_loss: 0.4543 - val_precision: 0.8408 - val_recall: 0.8408 - val_fn: 139.0000 - lr: 1.0000e-04\n",
      "Epoch 64/70\n",
      "62/62 [==============================] - 2s 25ms/step - loss: 0.1375 - precision: 0.9412 - recall: 0.9412 - fn: 462.0000 - val_loss: 0.4600 - val_precision: 0.8431 - val_recall: 0.8431 - val_fn: 137.0000 - lr: 1.0000e-04\n",
      "Epoch 65/70\n",
      "62/62 [==============================] - 2s 25ms/step - loss: 0.1329 - precision: 0.9460 - recall: 0.9460 - fn: 424.0000 - val_loss: 0.4606 - val_precision: 0.8488 - val_recall: 0.8488 - val_fn: 132.0000 - lr: 1.0000e-04\n",
      "Epoch 66/70\n",
      "62/62 [==============================] - 2s 25ms/step - loss: 0.1312 - precision: 0.9463 - recall: 0.9463 - fn: 422.0000 - val_loss: 0.4737 - val_precision: 0.8488 - val_recall: 0.8488 - val_fn: 132.0000 - lr: 1.0000e-04\n",
      "Epoch 67/70\n",
      "62/62 [==============================] - 2s 26ms/step - loss: 0.1282 - precision: 0.9451 - recall: 0.9451 - fn: 431.0000 - val_loss: 0.4799 - val_precision: 0.8522 - val_recall: 0.8522 - val_fn: 129.0000 - lr: 1.0000e-04\n",
      "Epoch 68/70\n",
      "62/62 [==============================] - 2s 26ms/step - loss: 0.1272 - precision: 0.9486 - recall: 0.9486 - fn: 404.0000 - val_loss: 0.4888 - val_precision: 0.8431 - val_recall: 0.8431 - val_fn: 137.0000 - lr: 1.0000e-04\n",
      "Epoch 69/70\n",
      "62/62 [==============================] - 2s 30ms/step - loss: 0.1207 - precision: 0.9498 - recall: 0.9498 - fn: 394.0000 - val_loss: 0.5098 - val_precision: 0.8396 - val_recall: 0.8396 - val_fn: 140.0000 - lr: 1.0000e-04\n",
      "Epoch 70/70\n",
      "62/62 [==============================] - 2s 37ms/step - loss: 0.1244 - precision: 0.9496 - recall: 0.9496 - fn: 396.0000 - val_loss: 0.5097 - val_precision: 0.8454 - val_recall: 0.8454 - val_fn: 135.0000 - lr: 1.0000e-04\n",
      "31/31 [==============================] - 1s 6ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgEAAAHWCAYAAADuNVprAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABhLklEQVR4nO3deVhUZfsH8O+wDfsmuwqImEqAaykuoKGgYmqSpqKimZbhipry5r6EUeZSpra4pbxuqSW54QJakjuKGwqiuICQiijGNnN+f/hj3iZQZ3TgCOf76TrXxTznOc+5h0DuebYjEwRBABEREUmOntgBEBERkTiYBBAREUkUkwAiIiKJYhJAREQkUUwCiIiIJIpJABERkUQxCSAiIpIoJgFEREQSxSSAiIhIopgEEGnoypUrCAoKgpWVFWQyGbZv367T9q9duwaZTIbVq1frtN3qrEOHDujQoYPYYRDVWEwCqFpJT0/Hhx9+CA8PDxgbG8PS0hJt27bF4sWL8ffff1fqvcPDw5GSkoJ58+bhp59+QsuWLSv1flVpyJAhkMlksLS0rPD7eOXKFchkMshkMnz55Zdat3/79m3MnDkTycnJOoiWiHTFQOwAiDT122+/oU+fPpDL5Rg8eDC8vb1RXFyM33//HZMmTcL58+fx3XffVcq9//77byQlJeHTTz/FqFGjKuUebm5u+Pvvv2FoaFgp7T+PgYEBHj9+jB07dqBv375q59avXw9jY2MUFha+UNu3b9/GrFmz4O7ujqZNm2p83d69e1/ofkSkGSYBVC1kZGSgX79+cHNzw4EDB+Ds7Kw6FxERgbS0NPz222+Vdv/c3FwAgLW1daXdQyaTwdjYuNLafx65XI62bdviv//9b7kkIDY2FiEhIfj555+rJJbHjx/D1NQURkZGVXI/IqnicABVCzExMXj06BF+/PFHtQSgjKenJ8aOHat6XVpaijlz5qB+/fqQy+Vwd3fHf/7zHxQVFald5+7uju7du+P333/Hm2++CWNjY3h4eGDt2rWqOjNnzoSbmxsAYNKkSZDJZHB3dwfwpBu97Ot/mjlzJmQymVpZfHw82rVrB2tra5ibm6Nhw4b4z3/+ozr/tDkBBw4cQPv27WFmZgZra2v07NkTFy9erPB+aWlpGDJkCKytrWFlZYWhQ4fi8ePHT//G/suAAQOwa9cu5OXlqcqOHz+OK1euYMCAAeXq37t3DxMnToSPjw/Mzc1haWmJrl274syZM6o6CQkJeOONNwAAQ4cOVQ0rlL3PDh06wNvbGydPnoS/vz9MTU1V35d/zwkIDw+HsbFxufcfHBwMGxsb3L59W+P3SkRMAqia2LFjBzw8PNCmTRuN6n/wwQeYPn06mjdvjoULFyIgIADR0dHo169fubppaWl499130blzZyxYsAA2NjYYMmQIzp8/DwDo3bs3Fi5cCADo378/fvrpJyxatEir+M+fP4/u3bujqKgIs2fPxoIFC9CjRw/88ccfz7xu3759CA4ORk5ODmbOnInIyEgcOXIEbdu2xbVr18rV79u3Lx4+fIjo6Gj07dsXq1evxqxZszSOs3fv3pDJZNi6dauqLDY2Fo0aNULz5s3L1b969Sq2b9+O7t2746uvvsKkSZOQkpKCgIAA1R/kxo0bY/bs2QCAESNG4KeffsJPP/0Ef39/VTt3795F165d0bRpUyxatAgdO3asML7FixfD3t4e4eHhUCgUAIAVK1Zg7969+Prrr+Hi4qLxeyUiAALRK+7BgwcCAKFnz54a1U9OThYACB988IFa+cSJEwUAwoEDB1Rlbm5uAgDh0KFDqrKcnBxBLpcLEyZMUJVlZGQIAIQvvvhCrc3w8HDBzc2tXAwzZswQ/vnrtXDhQgGAkJub+9S4y+6xatUqVVnTpk0FBwcH4e7du6qyM2fOCHp6esLgwYPL3e/9999Xa/Odd94RatWq9dR7/vN9mJmZCYIgCO+++64QGBgoCIIgKBQKwcnJSZg1a1aF34PCwkJBoVCUex9yuVyYPXu2quz48ePl3luZgIAAAYCwfPnyCs8FBASole3Zs0cAIMydO1e4evWqYG5uLvTq1eu575GIymNPAL3y8vPzAQAWFhYa1d+5cycAIDIyUq18woQJAFBu7oCXlxfat2+vem1vb4+GDRvi6tWrLxzzv5XNJfjll1+gVCo1uiYrKwvJyckYMmQIbG1tVeW+vr7o3Lmz6n3+00cffaT2un379rh7967qe6iJAQMGICEhAdnZ2Thw4ACys7MrHAoAnswj0NN78s+IQqHA3bt3VUMdp06d0viecrkcQ4cO1ahuUFAQPvzwQ8yePRu9e/eGsbExVqxYofG9iOh/mATQK8/S0hIA8PDhQ43qX79+HXp6evD09FQrd3JygrW1Na5fv65W7urqWq4NGxsb3L9//wUjLu+9995D27Zt8cEHH8DR0RH9+vXDpk2bnpkQlMXZsGHDcucaN26Mv/76CwUFBWrl/34vNjY2AKDVe+nWrRssLCywceNGrF+/Hm+88Ua572UZpVKJhQsXokGDBpDL5bCzs4O9vT3Onj2LBw8eaHzP2rVrazUJ8Msvv4StrS2Sk5OxZMkSODg4aHwtEf0PkwB65VlaWsLFxQXnzp3T6rp/T8x7Gn19/QrLBUF44XuUjVeXMTExwaFDh7Bv3z4MGjQIZ8+exXvvvYfOnTuXq/syXua9lJHL5ejduzfWrFmDbdu2PbUXAAA+++wzREZGwt/fH+vWrcOePXsQHx+P119/XeMeD+DJ90cbp0+fRk5ODgAgJSVFq2uJ6H+YBFC10L17d6SnpyMpKem5dd3c3KBUKnHlyhW18jt37iAvL081018XbGxs1GbSl/l3bwMA6OnpITAwEF999RUuXLiAefPm4cCBAzh48GCFbZfFmZqaWu7cpUuXYGdnBzMzs5d7A08xYMAAnD59Gg8fPqxwMmWZLVu2oGPHjvjxxx/Rr18/BAUFoVOnTuW+J5omZJooKCjA0KFD4eXlhREjRiAmJgbHjx/XWftEUsIkgKqFTz75BGZmZvjggw9w586dcufT09OxePFiAE+6swGUm8H/1VdfAQBCQkJ0Flf9+vXx4MEDnD17VlWWlZWFbdu2qdW7d+9euWvLNs3597LFMs7OzmjatCnWrFmj9kf13Llz2Lt3r+p9VoaOHTtizpw5+Oabb+Dk5PTUevr6+uV6GTZv3oxbt26plZUlKxUlTNqaPHkyMjMzsWbNGnz11Vdwd3dHeHj4U7+PRPR03CyIqoX69esjNjYW7733Hho3bqy2Y+CRI0ewefNmDBkyBADQpEkThIeH47vvvkNeXh4CAgJw7NgxrFmzBr169Xrq8rMX0a9fP0yePBnvvPMOxowZg8ePH2PZsmV47bXX1CbGzZ49G4cOHUJISAjc3NyQk5ODb7/9FnXq1EG7du2e2v4XX3yBrl27ws/PD8OGDcPff/+Nr7/+GlZWVpg5c6bO3se/6enpYerUqc+t1717d8yePRtDhw5FmzZtkJKSgvXr18PDw0OtXv369WFtbY3ly5fDwsICZmZmaNWqFerVq6dVXAcOHMC3336LGTNmqJYsrlq1Ch06dMC0adMQExOjVXtEkify6gQirVy+fFkYPny44O7uLhgZGQkWFhZC27Ztha+//looLCxU1SspKRFmzZol1KtXTzA0NBTq1q0rREVFqdURhCdLBENCQsrd599L0562RFAQBGHv3r2Ct7e3YGRkJDRs2FBYt25duSWC+/fvF3r27Cm4uLgIRkZGgouLi9C/f3/h8uXL5e7x72V0+/btE9q2bSuYmJgIlpaWwttvvy1cuHBBrU7Z/f69BHHVqlUCACEjI+Op31NBUF8i+DRPWyI4YcIEwdnZWTAxMRHatm0rJCUlVbi075dffhG8vLwEAwMDtfcZEBAgvP766xXe85/t5OfnC25ubkLz5s2FkpIStXrjx48X9PT0hKSkpGe+ByJSJxMELWYMERERUY3BOQFEREQSxSSAiIhIopgEEBERSRSTACIiIoliEkBERCRRTAKIiIgkikkAERGRRNXIHQMLZoeJHQJRpbOamyB2CESVrrT41vMrvYSSv3T3yHBDO4/nV3rF1MgkgIiISCNK3T3FszricAAREZFEsSeAiIikS1CKHYGo2BNARETSpVTq7nhB8+fPh0wmw7hx41RlHTp0gEwmUzs++ugjtesyMzMREhICU1NTODg4YNKkSSgtLdXq3uwJICIiEsnx48exYsUK+Pr6ljs3fPhwzJ49W/Xa1NRU9bVCoUBISAicnJxw5MgRZGVlYfDgwTA0NMRnn32m8f3ZE0BERJIlCEqdHdp69OgRwsLC8P3338PGxqbceVNTUzg5OakOS0tL1bm9e/fiwoULWLduHZo2bYquXbtizpw5WLp0KYqLizWOgUkAERFJlw6HA4qKipCfn692FBUVPfXWERERCAkJQadOnSo8v379etjZ2cHb2xtRUVF4/Pix6lxSUhJ8fHzg6OioKgsODkZ+fj7Onz+v8dtnEkBERKQD0dHRsLKyUjuio6MrrLthwwacOnXqqecHDBiAdevW4eDBg4iKisJPP/2EgQMHqs5nZ2erJQAAVK+zs7M1jplzAoiISLp0uDogKioKkZGRamVyubxcvRs3bmDs2LGIj4+HsbFxhW2NGDFC9bWPjw+cnZ0RGBiI9PR01K9fX2cxMwkgIiLp0uFmQXK5vMI/+v928uRJ5OTkoHnz5qoyhUKBQ4cO4ZtvvkFRURH09fXVrmnVqhUAIC0tDfXr14eTkxOOHTumVufOnTsAACcnJ41j5nAAERFRFQoMDERKSgqSk5NVR8uWLREWFobk5ORyCQAAJCcnAwCcnZ0BAH5+fkhJSUFOTo6qTnx8PCwtLeHl5aVxLOwJICIi6RJhsyALCwt4e3urlZmZmaFWrVrw9vZGeno6YmNj0a1bN9SqVQtnz57F+PHj4e/vr1pKGBQUBC8vLwwaNAgxMTHIzs7G1KlTERERoVFvRBkmAUREJF0vsclPZTEyMsK+ffuwaNEiFBQUoG7duggNDcXUqVNVdfT19REXF4eRI0fCz88PZmZmCA8PV9tXQBMyQRAEXb8BsfEpgiQFfIogSUFlP0Ww+Oqx51fSkJHHmzprq6qwJ4CIiCTrRTb5qUmYBBARkXS9gsMBVYmrA4iIiCSKPQFERCRdHA4gIiKSKB1uFlQdcTiAiIhIotgTQERE0sXhACIiIoni6gAiIiKSIvYEEBGRdHE4gIiISKI4HEBERERSxJ4AIiKSLEGQ9j4BTAKIiEi6JD4ngMMBREREEsWeACIiki6JTwxkEkBERNLF4QAiIiKSIvYEEBGRdEn8KYJMAoiISLo4HEBERERSxJ4AIiKSLq4OICIikigOBxAREZEUsSeAiIiki8MBREREEiXxJIDDAURERBLFngAiIpIsPkqYiIhIqjgcQERERFLEngAiIpIuie8TwCSAiIiki8MBREREJEXsCSAiIunicAAREZFEcTiAiIiIpIg9AUREJF0cDiAiIpIoDgcQERGRWObPnw+ZTIZx48apygoLCxEREYFatWrB3NwcoaGhuHPnjtp1mZmZCAkJgampKRwcHDBp0iSUlpZqdW8mAUREJF1Kpe6OF3D8+HGsWLECvr6+auXjx4/Hjh07sHnzZiQmJuL27dvo3bu36rxCoUBISAiKi4tx5MgRrFmzBqtXr8b06dO1uj+TACIiki5BqbtDS48ePUJYWBi+//572NjYqMofPHiAH3/8EV999RXeeusttGjRAqtWrcKRI0fw559/AgD27t2LCxcuYN26dWjatCm6du2KOXPmYOnSpSguLtY4BiYBREREOlBUVIT8/Hy1o6io6Kn1IyIiEBISgk6dOqmVnzx5EiUlJWrljRo1gqurK5KSkgAASUlJ8PHxgaOjo6pOcHAw8vPzcf78eY1jZhJARETSpcPhgOjoaFhZWakd0dHRFd52w4YNOHXqVIXns7OzYWRkBGtra7VyR0dHZGdnq+r8MwEoO192TlNcHUBERNKlwyWCUVFRiIyMVCuTy+Xl6t24cQNjx45FfHw8jI2NdXb/F8GeACIiIh2Qy+WwtLRUOypKAk6ePImcnBw0b94cBgYGMDAwQGJiIpYsWQIDAwM4OjqiuLgYeXl5atfduXMHTk5OAAAnJ6dyqwXKXpfV0YSoSUBpaSnWrl1b7o0QERFVCRFWBwQGBiIlJQXJycmqo2XLlggLC1N9bWhoiP3796uuSU1NRWZmJvz8/AAAfn5+SElJQU5OjqpOfHw8LC0t4eXlpXEsog4HGBgY4KOPPsLFixfFDIOIiKRKhB0DLSws4O3trVZmZmaGWrVqqcqHDRuGyMhI2NrawtLSEqNHj4afnx9at24NAAgKCoKXlxcGDRqEmJgYZGdnY+rUqYiIiKiw9+FpRJ8T8OabbyI5ORlubm5ih0JERPRKWLhwIfT09BAaGoqioiIEBwfj22+/VZ3X19dHXFwcRo4cCT8/P5iZmSE8PByzZ8/W6j4yQRAEXQevjU2bNiEqKgrjx49HixYtYGZmpnb+3xsoaKJgdpiuwiN6ZVnNTRA7BKJKV1p8q1Lb/3vLXJ21ZfLuVJ21VVVE7wno168fAGDMmDGqMplMBkEQIJPJoFAoxAqNiIhqOok/O0D0JCAjI0PsEIiIiCRJ9CSAcwGIiEg04o6Ii070JKDMhQsXkJmZWW7P4x49eogUERER1XgcDhDX1atX8c477yAlJUU1FwB4Mi8AAOcEEBERVRLRdwwcO3Ys6tWrh5ycHJiamuL8+fM4dOgQWrZsiYSEBLHDIyKimkzkRwmLTfSegKSkJBw4cAB2dnbQ09ODnp4e2rVrh+joaIwZMwanT58WO0QiIqqpRNgs6FUiek+AQqGAhYUFAMDOzg63b98G8GTCYGpqqpihERER1Wii9wR4e3vjzJkzqFevHlq1aoWYmBgYGRnhu+++g4eHh9jhERFRTVZNu/F1RfQkYOrUqSgoKAAAzJ49G927d0f79u1Rq1YtbNy4UeToiIioRuMSQXEFBwervvb09MSlS5dw79492NjYqFYIEBERke6JngRUxNbWVuwQiIhICjgcUPV69+6tcd2tW7dWYiRERCRpTAKqnpWVlRi3JSIion8QJQlYtWqVGLclIiJSJ/F9Al7JOQFERERVQVBydYCo6tWr98xVAFevXq3CaIiIiKRD9CRg3Lhxaq9LSkpw+vRp7N69G5MmTRInKCIikgZODBTX2LFjKyxfunQpTpw4UcXREBGRpEh8ToDozw54mq5du+Lnn38WOwwiIqIaS/SegKfZsmULNw0iIqLKxYmB4mrWrJnaxEBBEJCdnY3c3Fx8++23IkZGREQ1HucEiKtXr15qr/X09GBvb48OHTqgUaNG4gRFREQkAaInATNmzBA7BCIikir2BIhr586d0NfXV3uaIADs2bMHSqUSXbt2FSkyIiKq8ST+KGHRVwdMmTIFCoWiXLkgCJgyZYoIEREREUmD6D0BV65cgZeXV7nyRo0aIS0tTYSIiIhIMjgcIC4rKytcvXoV7u7uauVpaWkwMzMTJyiCQYtAGLbsBJm1PQBAmXsTJYe2QZF2BgBgPPhT6LurJ28lJ/ajeOdK9Xaa+MOwdVfIajkBRX+j9MIxFO9aXSXvgehFpF3+E+7udcuVf7tsNcaM/RRyuRxfxEzHe317Qi43wt74BIwa/R/k5PwlQrT00rhEUFw9e/bEuHHjsG3bNtSvXx/AkwRgwoQJ6NGjh8jRSZfw8B6K92+A8l42ABkMmrSH/L1I/P3dfyDk3gIAlJw8gJKELf+7pqRYrQ2D1l1h2Lobivf9F8pbaYChHHr/n1QQvapat+kGfX191Wvv1xthz+4N+PnnOADAgi9nolvXQPTr/yEePMjHksXzsGXTD/Dv0EukiIlenOhJQExMDLp06YJGjRqhTp06AICbN2+iffv2+PLLL0WOTroUl0+rvS45uBmGLTtBv7YnSv8/CUBJEYSCBxU3YGwKo459ULhhAZQZ5//Xbs6NygqZSCf++uue2utPJo1CWloGEg8lwdLSAu8P7YeBg0fhYMIfAIBhw8fjfMohtHqzOY4eOyVGyPQyJL5tsOhJgJWVFY4cOYL4+HicOXMGJiYm8PX1hb+/v9ihURmZDPperQBDORQ3/zdPw8CnLQx820F4lIfSy6dRcmgbUPqkN0DfwweQyaBnYQP5yBhAbgLljcsojl8PIf/e0+5E9EoxNDRE2IDeWLT4OwBAi+a+MDIywv79h1V1UlPTcf36TbRu3YJJQHXE4QDxyWQyBAUFISgoSOtri4qKUFRUpFZWWqqA3ED/KVeQpmQOdWHy/kzAwBAoLkTRpoUQ/nrSC1B67giEB39B+TAPeg51YdSpP/RqOaNo8yIAgJ6NAyDTg2G7nijesxZC4d8w6tgHxgOj8PfyKYCy/IoQoldNz55dYG1tiTVrNwEAHJ3sUVRUhAcP8tXq5eTkwsmJQ11U/YiSBCxZsgQjRoyAsbExlixZ8sy6Y8aMeeb56OhozJo1S60sqoM3Pu3o+9JxSp3w1238veI/kBmbQL9xK8h7foS/18yF8NctlJ46qKqnyLmBokd5MBn8KYptHCDczwFkMsj0DVC0ey0UV1MAAIVbv4Fp5LfQr+cFRXqKWG+LSGPvD+mH3XsOIivrjtihUCURuDqg6i1cuBBhYWEwNjbGwoULn1pPJpM9NwmIiopCZGSkWlnplyN0EqfkKRUQ7t+BAECZdQ36Lh4wbBWM4t9Wlq96Kx0AoGfrCMX9HAgP856Ul80fAIDHD4HHDyGztKuC4IlejqtrbQQGtse7fT9Qld3JzoVcLoeVlaVab4CDgz2ys3PFCJNeFocDql5GRkaFX78IuVwOuVyuVlbAoYDKIZMB+oYVntJzcgMAKP//j7/ixuUnl9g5Q3j4/3MAjM0AUwsID7iUil59Q8LfQ07OX9i5c7+q7OSpsyguLsZbb7XDtm07AQCvvVYfbm518OefJ8UKleiFiTonoKSkBI0aNUJcXBwaN24sZij0L4ZvvQdF2pknf7DlJjDwbgM998YoXv85ZDYOMPBuA0VaMoTHj6Dn6AqjoIFQXL8I4f9n/wv3slF66QTkwYNQFPcjhKK/YRT4HoS/bkNx7YLI747o2WQyGcIHv4ef1m1W29E0P/8hVq7agC9jZuD+vTzk5z/E4kVzkZR0gpMCqyuuDhCPoaEhCgsLxQyBnkJmZgl5r48gM7cGih5DeecGCtd/DuXVc5BZ2kLfwxuGrboARnIID+6h9NJxlBzartZG0fblMAoeCOP+kwBBCcX1SyiM/ZyTAumV1ymwPdzc6mDV6o3lzk2YOBNKpRKbNn4HuVyu2iyIqimJDwfIBEHcpyd89tlnuHz5Mn744QcYGOgmJymYHaaTdoheZVZzE8QOgajSlRbfen6ll6DLvxdm09frrK2qIvoSwePHj2P//v3Yu3cvfHx8ym0VvHXrVpEiIyKiGk/iqwNEf4qgtbU1QkNDERwcDBcXF1hZWakdRERElUYp6O7QwrJly+Dr6wtLS0tYWlrCz88Pu3btUp3v0KEDZDKZ2vHRRx+ptZGZmYmQkBCYmprCwcEBkyZNQmlpqVZxiN4TsGrVKrFDICIiqlJ16tTB/Pnz0aBBAwiCgDVr1qBnz544ffo0Xn/9dQDA8OHDMXv2bNU1pqamqq8VCgVCQkLg5OSEI0eOICsrC4MHD4ahoSE+++wzjeMQvSfgrbfeQl5eXrny/Px8vPXWW1UfEBERSYeg1N2hhbfffhvdunVDgwYN8Nprr2HevHkwNzfHn3/+qapjamoKJycn1WFpaak6t3fvXly4cAHr1q1D06ZN0bVrV8yZMwdLly5FcXFxRbeskOhJQEJCQoUBFxYW4vDhwxVcQUREpCM6HA4oKipCfn6+2vHvbe0rolAosGHDBhQUFMDPz09Vvn79etjZ2cHb2xtRUVF4/Pix6lxSUhJ8fHzg6OioKgsODkZ+fj7Onz8PTYk2HHD27FnV1xcuXEB2drbqtUKhwO7du1G7dm0xQiMiItJaRdvYz5gxAzNnzqywfkpKCvz8/FBYWAhzc3Ns27YNXl5eAIABAwbAzc0NLi4uOHv2LCZPnozU1FTVZPns7Gy1BACA6vU//54+j2hJQNOmTVWTHSrq9jcxMcHXX38tQmRERCQVunx2QEXb2P97R9t/atiwIZKTk/HgwQNs2bIF4eHhSExMhJeXF0aM+N/29z4+PnB2dkZgYCDS09NRv359ncUsWhKQkZEBQRDg4eGBY8eOwd7+f0/gMjIygoODA/T1uf0vERFVDxVtY/8sRkZG8PT0BAC0aNECx48fx+LFi7FixYpydVu1agUASEtLQ/369eHk5IRjx46p1blz58mDrpycnDSOQbQkwM3t//eal/gaTSIiEtErtGOgUql86hyC5ORkAICzszMAwM/PD/PmzUNOTg4cHBwAAPHx8bC0tFQNKWhC9CWCAHDlyhUcPHgQOTk55ZKC6dOnixQVERHVeCIlAVFRUejatStcXV3x8OFDxMbGIiEhAXv27EF6ejpiY2PRrVs31KpVC2fPnsX48ePh7+8PX19fAEBQUBC8vLwwaNAgxMTEIDs7G1OnTkVERIRWvRGiJwHff/89Ro4cCTs7Ozg5OUEmk6nOyWQyJgFERFTj5OTkYPDgwcjKyoKVlRV8fX2xZ88edO7cGTdu3MC+ffuwaNEiFBQUoG7duggNDcXUqVNV1+vr6yMuLg4jR46En58fzMzMEB4erravgCZEf3aAm5sbPv74Y0yePFlnbfLZASQFfHYASUFlPzvg0cSeOmvL/MtfdNZWVRG9J+D+/fvo06eP2GEQEZEUvUJzAsQg+mZBffr0wd69e8UOg4iISHJE7wnw9PTEtGnT8Oeff8LHxweGhoZq58eMGSNSZEREVNMJEu8JED0J+O6772Bubo7ExEQkJiaqnZPJZEwCiIio8jAJEFdGRobYIRAREUmS6HMCyhQXFyM1NVXrZyETERG9MKVSd0c1JHoS8PjxYwwbNgympqZ4/fXXkZmZCQAYPXo05s+fL3J0RERUo+nwKYLVkehJQFRUFM6cOYOEhAQYGxuryjt16oSNGzeKGBkREVHNJvqcgO3bt2Pjxo1o3bq12m6Br7/+OtLT00WMjIiIarxq+gleV0RPAnJzc1UPP/ingoICtaSAiIhI10TeNFd0og8HtGzZEr/99pvqddkf/h9++AF+fn5ihUVERFTjidYTcO7cOXh7eyM6OhpdunTBhQsXUFJSgsWLF+PChQs4cuRIuX0DiIiIdEriwwGi9QT4+vqiVatWuHDhAv744w+UlpbC19cXe/fuhYODA5KSktCiRQuxwiMiIimQ+OoA0XoCEhMTsWrVKkyYMAFKpRKhoaH48ssv4e/vL1ZIREREkiJaT0D79u2xcuVKZGVl4euvv8a1a9fQoUMHvPbaa/j888+RnZ0tVmhERCQRglLQ2VEdiT4x0MzMDEOHDkViYiIuX76MPn36YOnSpXB1dUWPHj3EDo+IiGoyiQ8HiJ4E/JOnpyf+85//YOrUqbCwsFBbNUBERES6Jfo+AWUOHTqElStX4ueff4aenh769u2LYcOGiR0WERHVZNVzy3+dETUJuH37NlavXo3Vq1cjLS0Nbdq0wZIlS9C3b1+YmZmJGRoREUlAdR3L1xXRkoCuXbti3759sLOzw+DBg/H++++jYcOGYoVDREQkOaIlAYaGhtiyZQu6d+8OfX19scIgIiIpY0+AOH799Vexbk1ERPSExOcEvFKrA4iIiKjqvDKrA4iIiKoaJwYSERFJFYcDiIiISIrYE0BERJLF4QAiIiKp4nAAERERSRF7AoiISLIEifcEMAkgIiLpkngSwOEAIiIiiWJPABERSRaHA4iIiKRK4kkAhwOIiIgkij0BREQkWRwOICIikiipJwEcDiAiIpIo9gQQEZFksSeAiIhIqgSZ7g4tLFu2DL6+vrC0tISlpSX8/Pywa9cu1fnCwkJERESgVq1aMDc3R2hoKO7cuaPWRmZmJkJCQmBqagoHBwdMmjQJpaWlWsXBJICIiKiK1alTB/Pnz8fJkydx4sQJvPXWW+jZsyfOnz8PABg/fjx27NiBzZs3IzExEbdv30bv3r1V1ysUCoSEhKC4uBhHjhzBmjVrsHr1akyfPl2rOGSCINS45ygWzA4TOwSiSmc1N0HsEIgqXWnxrUptP9u/g87acjqU8FLX29ra4osvvsC7774Le3t7xMbG4t133wUAXLp0CY0bN0ZSUhJat26NXbt2oXv37rh9+zYcHR0BAMuXL8fkyZORm5sLIyMjje6pk56AvLw8XTRDRERUpQSlTGdHUVER8vPz1Y6ioqLnxqBQKLBhwwYUFBTAz88PJ0+eRElJCTp16qSq06hRI7i6uiIpKQkAkJSUBB8fH1UCAADBwcHIz89X9SZoQusk4PPPP8fGjRtVr/v27YtatWqhdu3aOHPmjLbNERER1QjR0dGwsrJSO6Kjo59aPyUlBebm5pDL5fjoo4+wbds2eHl5ITs7G0ZGRrC2tlar7+joiOzsbABAdna2WgJQdr7snKa0TgKWL1+OunXrAgDi4+MRHx+PXbt2oWvXrpg0aZK2zREREYlGUOruiIqKwoMHD9SOqKiop967YcOGSE5OxtGjRzFy5EiEh4fjwoULVfjuX2CJYHZ2tioJiIuLQ9++fREUFAR3d3e0atVK5wESERFVFkHLWf3PIpfLIZfLNa5vZGQET09PAECLFi1w/PhxLF68GO+99x6Ki4uRl5en1htw584dODk5AQCcnJxw7NgxtfbKVg+U1dGE1j0BNjY2uHHjBgBg9+7dqjELQRCgUCi0bY6IiIgAKJVKFBUVoUWLFjA0NMT+/ftV51JTU5GZmQk/Pz8AgJ+fH1JSUpCTk6OqEx8fD0tLS3h5eWl8T617Anr37o0BAwagQYMGuHv3Lrp27QoAOH36tCqjISIiqg7E2iwoKioKXbt2haurKx4+fIjY2FgkJCRgz549sLKywrBhwxAZGQlbW1tYWlpi9OjR8PPzQ+vWrQEAQUFB8PLywqBBgxATE4Ps7GxMnToVERERWvVGaJ0ELFy4EO7u7rhx4wZiYmJgbm4OAMjKysLHH3+sbXNERESiEZS6Gw7QRk5ODgYPHoysrCxYWVnB19cXe/bsQefOnQE8+Vurp6eH0NBQFBUVITg4GN9++63qen19fcTFxWHkyJHw8/ODmZkZwsPDMXv2bK3i4D4BRNUU9wkgKajsfQJuvBGos7bqHt///EqvGI16An799VeNG+zRo8cLB0NERFSVat7HYO1olAT06tVLo8ZkMhknBxIRUbUh1nDAq0KjJECplPhjloiIiGqgl3qUcGFhIYyNjXUVCxERUZWSek+A1vsEKBQKzJkzB7Vr14a5uTmuXr0KAJg2bRp+/PFHnQdIRERUWQRBd0d1pHUSMG/ePKxevRoxMTFqTyny9vbGDz/8oNPgiIiIqPJonQSsXbsW3333HcLCwqCvr68qb9KkCS5duqTT4IiIiCqTLp8iWB1pPSfg1q1bFe4MqFQqUVJSopOgiIiIqoIunx1QHWndE+Dl5YXDhw+XK9+yZQuaNWumk6CIiIio8mndEzB9+nSEh4fj1q1bUCqV2Lp1K1JTU7F27VrExcVVRoxERESVQqxnB7wqtO4J6NmzJ3bs2IF9+/bBzMwM06dPx8WLF7Fjxw7VnsdERETVgVKQ6eyojl5on4D27dsjPj5e17EQERFRFXrhzYJOnDiBixcvAngyT6BFixY6C4qIiKgqSH1ioNZJwM2bN9G/f3/88ccfsLa2BgDk5eWhTZs22LBhA+rUqaPrGImIiCpFdV3apytazwn44IMPUFJSgosXL+LevXu4d+8eLl68CKVSiQ8++KAyYiQiIqJKoHVPQGJiIo4cOYKGDRuqyho2bIivv/4a7du312lwRERElam6bverK1onAXXr1q1wUyCFQgEXFxedBEVERFQVOBygpS+++AKjR4/GiRMnVGUnTpzA2LFj8eWXX+o0OCIiIqo8GvUE2NjYQCb7X7ZUUFCAVq1awcDgyeWlpaUwMDDA+++/j169elVKoERERLpWXdf364pGScCiRYsqOQwiIqKqxyWCGggPD6/sOIiIiKiKvfBmQQBQWFiI4uJitTJLS8uXCoiIiKiqcHWAlgoKCjB58mRs2rQJd+/eLXdeoVDoJDAiIqLKJvU5AVqvDvjkk09w4MABLFu2DHK5HD/88ANmzZoFFxcXrF27tjJiJCIiokqgdU/Ajh07sHbtWnTo0AFDhw5F+/bt4enpCTc3N6xfvx5hYWGVEScREZHOSX1ioNY9Affu3YOHhweAJ+P/9+7dAwC0a9cOhw4d0m10RERElUgQdHdUR1onAR4eHsjIyAAANGrUCJs2bQLwpIeg7IFCRERE9OrTejhg6NChOHPmDAICAjBlyhS8/fbb+Oabb1BSUoKvvvqqMmIkIiKqFFKfGCgThJfrxLh+/TpOnjwJT09P+Pr66iqul2Jj7il2CESVLufaXrFDIKp0hnYeldr+8drv6KytN25t01lbVeWl9gkAADc3N7i5uekiFiIiIqpCGiUBS5Ys0bjBMWPGvHAwREREVUnqwwEaJQELFy7UqDGZTMYkgIiIqo1qOqlfZzRKAspWAxAREVHN8dJzAoiIiKorDgcQERFJFHcMJCIiIkliTwAREUmWUuwARMYkgIiIJEsAhwO0dvjwYQwcOBB+fn64desWAOCnn37C77//rtPgiIiIaqLo6Gi88cYbsLCwgIODA3r16oXU1FS1Oh06dIBMJlM7PvroI7U6mZmZCAkJgampKRwcHDBp0iSUlpZqHIfWScDPP/+M4OBgmJiY4PTp0ygqKgIAPHjwAJ999pm2zREREYlGKeju0EZiYiIiIiLw559/Ij4+HiUlJQgKCkJBQYFaveHDhyMrK0t1xMTEqM4pFAqEhISguLgYR44cwZo1a7B69WpMnz5d4zi0fnZAs2bNMH78eAwePBgWFhY4c+YMPDw8cPr0aXTt2hXZ2dnaNFcp+OwAkgI+O4CkoLKfHXDAsa/O2nrrzqYXvjY3NxcODg5ITEyEv78/gCc9AU2bNsWiRYsqvGbXrl3o3r07bt++DUdHRwDA8uXLMXnyZOTm5sLIyOi599W6JyA1NVUV4D9ZWVkhLy9P2+aIiIhqhKKiIuTn56sdZb3lz/PgwQMAgK2trVr5+vXrYWdnB29vb0RFReHx48eqc0lJSfDx8VElAAAQHByM/Px8nD9/XqP7ap0EODk5IS0trVz577//Dg+Pys3YiIiIdEmATGdHdHQ0rKys1I7o6OjnxqBUKjFu3Di0bdsW3t7eqvIBAwZg3bp1OHjwIKKiovDTTz9h4MCBqvPZ2dlqCQAA1WtNe+W1Xh0wfPhwjB07FitXroRMJsPt27eRlJSEiRMnYtq0ado2R0REJBpdLhGMiopCZGSkWplcLn/udRERETh37ly5yfUjRoxQfe3j4wNnZ2cEBgYiPT0d9evX10nMWicBU6ZMgVKpRGBgIB4/fgx/f3/I5XJMnDgRo0eP1klQRERE1Y1cLtfoj/4/jRo1CnFxcTh06BDq1KnzzLqtWrUCAKSlpaF+/fpwcnLCsWPH1OrcuXMHwJNee01oPRwgk8nw6aef4t69ezh37hz+/PNP5ObmYs6cOdo2RUREJCpdDgdodV9BwKhRo7Bt2zYcOHAA9erVe+41ycnJAABnZ2cAgJ+fH1JSUpCTk6OqEx8fD0tLS3h5eWkUxwtvFmRkZKTxTYiIiF5FYu0YGBERgdjYWPzyyy+wsLBQjeFbWVnBxMQE6enpiI2NRbdu3VCrVi2cPXsW48ePh7+/P3x9fQEAQUFB8PLywqBBgxATE4Ps7GxMnToVERERGvdIaL1EsGPHjpDJnp7xHDhwQJvmKgWXCJIUcIkgSUFlLxHc7dhPZ211ubNB47pP+zu6atUqDBkyBDdu3MDAgQNx7tw5FBQUoG7dunjnnXcwdepUWFpaqupfv34dI0eOREJCAszMzBAeHo758+fDwECzz/ha9wQ0bdpU7XVJSQmSk5Nx7tw5hIeHa9scERGRaMTqCXje5++6desiMTHxue24ublh586dLxyH1knAwoULKyyfOXMmHj169MKBEBERVTU+O0BHBg4ciJUrV+qqOSIiIqpkOnuKYFJSEoyNjXXVHBERUaVTSrsjQPskoHfv3mqvBUFAVlYWTpw4wc2CiIioWlFKfDhA6yTAyspK7bWenh4aNmyI2bNnIygoSGeBERERUeXSKglQKBQYOnQofHx8YGNjU1kxERERVQktnwBc42g1MVBfXx9BQUF8WiAREdUISh0e1ZHWqwO8vb1x9erVyoiFiIiIqpDWScDcuXMxceJExMXFISsrq9yzk4mIiKoLpUyms6M60nhOwOzZszFhwgR069YNANCjRw+1bQ8FQYBMJoNCodB9lERERJVA6nMCNE4CZs2ahY8++ggHDx6szHiIiIioimicBJTtcxwQEFBpwRAREVWl6jqhT1e0WiL4rKcHEhERVTfcMVALr7322nMTgXv37r1UQERERFQ1tEoCZs2aVW7HQCIiouqK2wZroV+/fnBwcKisWIiIiKqU1FcHaLxPAOcDEBER1Sxarw4gIiKqKTgxUENKpdQXUhARUU0j9b9sWm8bTERERDWDqElASUkJAgMDceXKFTHDICIiiRJ0eFRHWq0O0DVDQ0OcPXtWzBCIiEjCpD4nQPThgIEDB+LHH38UOwwiIiLJEbUnAABKS0uxcuVK7Nu3Dy1atICZmZna+a+++kqkyIiIqKaT+sRA0ZOAc+fOoXnz5gCAy5cvq53j3gRERFSZmASIjI8mJiIiEofoSQAREZFYBIl3OL8SScCJEyewadMmZGZmori4WO3c1q1bRYqKiIhqOqkPB4i+OmDDhg1o06YNLl68iG3btqGkpATnz5/HgQMH+MRCIiKiSiR6EvDZZ59h4cKF2LFjB4yMjLB48WJcunQJffv2haurq9jhERFRDabU4VEdiZ4EpKenIyQkBABgZGSEgoICyGQyjB8/Ht99953I0RERUU0m9R0DRU8CbGxs8PDhQwBA7dq1ce7cOQBAXl4eHj9+LGZoRERENZroEwP9/f0RHx8PHx8f9OnTB2PHjsWBAwcQHx+PwMBAscMjIqIaTOrbBoueBHzzzTcoLCwEAHz66acwNDTEkSNHEBoaiqlTp4ocHRER1WTVdSxfV0RPAmxtbVVf6+npYcqUKSJGQ0REJB2iJAH5+fka17W0tKzESIiISMrYEyACa2vr5z4XQBAEyGQyKBSKKoqKiIikprrO6tcVUZIAPi+AiIhIfKIkAQEBAWLcloiISI3UVweIvk/AoUOHnnkQERFVFrF2DIyOjsYbb7wBCwsLODg4oFevXkhNTVWrU1hYiIiICNSqVQvm5uYIDQ3FnTt31OpkZmYiJCQEpqamcHBwwKRJk1BaWqpxHKKvDujQoUO5sn/OF+CcACIiqmkSExMRERGBN954A6WlpfjPf/6DoKAgXLhwAWZmZgCA8ePH47fffsPmzZthZWWFUaNGoXfv3vjjjz8APPn7GBISAicnJxw5cgRZWVkYPHgwDA0N8dlnn2kUh0wQBFHnRTx48EDtdUlJCU6fPo1p06Zh3rx5L7RhkI25p67CI3pl5VzbK3YIRJXO0M6jUtuPdhuos7airq974Wtzc3Ph4OCAxMRE+Pv748GDB7C3t0dsbCzeffddAMClS5fQuHFjJCUloXXr1ti1axe6d++O27dvw9HREQCwfPlyTJ48Gbm5uTAyMnrufUUfDrCyslI77Ozs0LlzZ3z++ef45JNPxA6PiIhqMCUEnR1FRUXIz89XO4qKijSKo+wDcdneOSdPnkRJSQk6deqkqtOoUSO4uroiKSkJAJCUlAQfHx9VAgAAwcHByM/Px/nz5zW6r+hJwNM4OjqWGx8hIiJ6VUVHR5f7YBsdHf3c65RKJcaNG4e2bdvC29sbAJCdnQ0jIyNYW1ur1XV0dER2draqzj8TgLLzZec0IfqcgLNnz6q9FgQBWVlZmD9/Ppo2bSpOUEREJAm63CwoKioKkZGRamVyufy510VERODcuXP4/fffdRiNZkRPApo2bQqZTIZ/T01o3bo1Vq5cKVJUREQkBbqcFCeXyzX6o/9Po0aNQlxcHA4dOoQ6deqoyp2cnFBcXIy8vDy13oA7d+7AyclJVefYsWNq7ZWtHiir8zyiJwEZGRlqr/X09GBvbw9jY2ORIiIiIqpcgiBg9OjR2LZtGxISElCvXj218y1atIChoSH279+P0NBQAEBqaioyMzPh5+cHAPDz88O8efOQk5MDBwcHAEB8fDwsLS3h5eWlURyiJwFubm5ih0BERBIl1rMDIiIiEBsbi19++QUWFhaqMXwrKyuYmJjAysoKw4YNQ2RkJGxtbWFpaYnRo0fDz88PrVu3BgAEBQXBy8sLgwYNQkxMDLKzszF16lRERERo3CMh+sTAMWPGYMmSJeXKv/nmG4wbN67qAyIiIslQynR3aGPZsmV48OABOnToAGdnZ9WxceNGVZ2FCxeie/fuCA0Nhb+/P5ycnLB161bVeX19fcTFxUFfXx9+fn4YOHAgBg8ejNmzZ2sch+j7BNSuXRu//vorWrRooVZ+6tQp9OjRAzdv3tS6Te4TQFLAfQJICip7n4Dp7mE6a2v2tfU6a6uqiD4ccPfuXVhZWZUrt7S0xF9//SVCREREJBVKiT9HUPThAE9PT+zevbtc+a5du+DhUbkZIBERSZugw6M6Er0nIDIyEqNGjUJubi7eeustAMD+/fuxYMECLFq0SNzgiIiIajDRk4D3338fRUVFmDdvHubMmQMAcHd3x7JlyzB48GCRoyMioppMrNUBrwrRkwAAGDlyJEaOHInc3FyYmJjA3Nxc7JCIiEgCpD4n4JVIAsrY29uLHQIREZFkiJIENG/eHPv374eNjQ2aNWsGmezpCyxPnTpVhZEREZGUSLsfQKQkoGfPnqrdjHr16iVGCERERJwTIMZNZ8yYAQBQKBTo2LEjfH19yz0ukYiIiCqXqPsE6OvrIygoCPfv3xczDCIikiglBJ0d1ZHomwV5e3vj6tWrYodBREQSJPXNgkRPAubOnYuJEyciLi4OWVlZyM/PVzuIiIiocoi+RLBbt24AgB49eqitEhAEATKZDAqFQqzQiIiohuPEQJEdPHhQ7BCIiEiihGrbka8boicB9erVQ926dcvtFSAIAm7cuCFSVERERDWf6HMC6tWrh9zc3HLl9+7dQ7169USIiIiIpEKpw6M6Er0noGzs/98ePXoEY2NjESIiIiKpqK5L+3RFtCQgMjISACCTyTBt2jSYmpqqzikUChw9ehRNmzYVKToiIqKaT7Qk4PTp0wCe9ASkpKTAyMhIdc7IyAhNmjTBxIkTxQqPiIgkQNr9ACImAWWrAoYOHYrFixfD0tJSrFCIiEiipD4cIPrEwFWrVjEBqAb09PTwn2njkHzuIG7nnsOpswcwcXJEuXqvNayP2I0rcP3Wady8cxb7E7eiTh1nESIm0t4PP22Cd9uumL9ouapsyKhP4N22q9oxK+ZrtetSLqZi2Jgp8At+F2269MGI8Z/i0hXuhEqvPtEnBhYUFGD+/PnYv38/cnJyoFSqz7HklsKvhnGRH+L9Dwbg4xGf4OLFK2jW3AffLJuP/PyH+G7ZWgCAez1X7Nq7AevWbkb0vMV4+PARGjdugMKiIpGjJ3q+lIup2PzLTrzmWX5V0rs9umDUB4NUr42N5aqvHz/+Gx9FTkPHdq0xdcIoKBQKLP3xJ3wYORX7tq2FoYHo/8zSM1TXWf26IvpP5wcffIDExEQMGjQIzs7OFa4UIPG92aoZdsbtx949CQCAG5m3ENqnO1q0aKKqM21GJOL3JmLGtBhV2bWMzKoOlUhrjx//jSmzvsDMyWOxYs1/y503lsthV8u2wmuvXr+BB/kPEfHBIDg72gMARr4fht6DP0ZWdg5c67hUauz0crhZkMh27dqF3377DW3bthU7FHqGY0dPI3zoe6jv6Y70tGvw9m6E1n4tMXXKZwCerPLoHNwBSxZ9jy3bV8G3iReuX7uBhQuWY2fcPpGjJ3q2uQuWwt/vDfi90azCJOC3+IOI23sQdrY2CGjbCh8N7Q+T/1/CXM+1DqytLLE1bg9GDH4PCqUSW3fsgYd7Xbg4OVb1WyHSiuhJgI2NDWxtK86wNVFUVISif3U3P23vAXpxCxcsh4WFOY6d2guFQgF9fX3MnfUVNm/6FQBgb18LFhbmGBf5IebNXoiZ02LQqbM/for9Fm93G4gjvx8T+R0QVWznvgRcvJyODT8srvB8SOcOcHFyhL2dLS6nZWDhspW4lnkTi6OnAQDMzEyx6pvPMWbKbKxY/SSBcKvjghUL58LAQL/K3ge9GA4HiGzOnDmYPn061qxZo7ZXgKaio6Mxa9YstTK5oQ1MjF48saDy3gnthj7v9cDw98fj0sUr8PHxwmeff4qsrDvYELsNenpP5pju+m0fli1dBQA4l3IRb7ZqjveH9WcSQK+krDu5mL9oBb5f9BnkcqMK6/Tp2U319Wv168HezhbDxkQh8+ZtuNZxQWFREaZHL0IzHy/EzJoMpUKJ1f/9GR9PnIENPy6GsVxeYbv0auBwgMgWLFiA9PR0ODo6wt3dHYaGhmrnT5069czro6KiVBsPlXF1bqbzOKVu9twpWPTVCmzd8hsA4ML5y6jj6oLxEz/ChthtuHv3PkpKSnDpUpradZdT09Dar6UYIRM914XUK7h3Pw993x+lKlMolDiZfA7/3boDpw7+Cn199U/zPl6NAAA3bmXBtY4LftubgFtZd7B+xVeqZDhm5mS06dIHBw4noVunDlX2foi0JXoS0KtXr5e6Xi6XQ/6vTJtDAbpnYmJcbuWGUqGEnuzJP3olJSU4fTIFDRp4qNWp36Aebty4VWVxEmmjdYum2PbTMrWyqfO+Qj23uhg2sE+5BAAALl1JBwDVRMHCwkLo6cnU/t2RyfQAmQyCUtqfMqsDDgeIbMaMGWKHQBrYvesAIid9jJs3buPixSvwbeKFj0e/j/VrN6vqLFn8PVauWYwjfxzH4UN/olNnf3Tp+hbe7homYuRET2dmZooGHu5qZSYmxrC2tEADD3dk3ryNnfEJaO/3BqytLHE5LQOfL1mBlk290fD/lxL6vdkcC779EXMXLMWAd3tAUAr4Yd0mGOjr483mTSq4K71KlIK0EzXRkwAAyMvLw5YtW5Ceno5JkybB1tYWp06dgqOjI2rXri12eARg8sTZ+M+0cfhy4SzY2ddCdlYOVq/8L2Kiv1HV+W1HPCLHTsf4CR9h/hfTkHblKgaHjcKfSSdFjJzoxRkaGuLPE6fx06bt+LuwEE4O9ujcoR0+HNJPVcfDrS6++Xwmlq1aj4EfRkImk6Hxa/WxfMEc2NtxbhK92mSCIG4adPbsWXTq1AlWVla4du0aUlNT4eHhgalTpyIzMxNr167Vuk0bc89KiJTo1ZJzba/YIRBVOkM7j+dXegkD3XrrrK1117fqrK2qIvq2wZGRkRgyZAiuXLmi9ujgbt264dChQyJGRkRENZ0Sgs6O6kj0JOD48eP48MMPy5XXrl0b2dnZIkREREQkDaLPCZDL5cjPzy9XfvnyZdjb24sQERERSYXU9wkQvSegR48emD17NkpKSgA8Wd6XmZmJyZMnIzQ0VOToiIioJlPq8KiORE8CFixYgEePHsHBwQF///03AgIC4OnpCQsLC8ybN0/s8IiIiGos0YcDrKysEB8fj99//x1nz57Fo0eP0Lx5c3Tq1Ens0IiIqIarrhP6dEX0JODGjRuoW7cu2rVrh3bt2okdDhERkWSIPhzg7u6OgIAAfP/997h//77Y4RARkYQIOvxPG4cOHcLbb78NFxcXyGQybN++Xe38kCFDIJPJ1I4uXbqo1bl37x7CwsJgaWkJa2trDBs2DI8ePdIqDtGTgBMnTuDNN9/E7Nmz4ezsjF69emHLli3lHg9MRESka2JNDCwoKECTJk2wdOnSp9bp0qULsrKyVMd///tftfNhYWE4f/484uPjERcXh0OHDmHEiBFaxSH6joFlBEFAQkICYmNj8fPPP0OpVKJ3795YuXKl1m1xx0CSAu4YSFJQ2TsG9nbrobO2tl7/9YWuk8lk2LZtm9oD9YYMGYK8vLxyPQRlLl68CC8vLxw/fhwtWz55Uuvu3bvRrVs33Lx5Ey4uLhrdW/SegDIymQwdO3bE999/j3379qFevXpYs2aN2GEREVENJgiCzo6ioiLk5+erHS/Tq52QkAAHBwc0bNgQI0eOxN27d1XnkpKSYG1trUoAAKBTp07Q09PD0aNHNb7HK5ME3Lx5EzExMWjatCnefPNNmJubP7ObhIiI6GXpctvg6OhoWFlZqR3R0dEvFFeXLl2wdu1a7N+/H59//jkSExPRtWtXKBQKAEB2djYcHBzUrjEwMICtra1Wu+2KvjpgxYoViI2NxR9//IFGjRohLCwMv/zyC9zc3MQOjYiISGNRUVGIjIxUK5PL5S/UVr9+/3tSpY+PD3x9fVG/fn0kJCQgMDDwpeL8J9GTgLlz56J///5YsmQJmjThs7eJiKjq6HKnP7lc/sJ/9J/Hw8MDdnZ2SEtLQ2BgIJycnJCTk6NWp7S0FPfu3YOTk5PG7YqeBGRmZkImk4kdBhERSVB1eXbAzZs3cffuXTg7OwMA/Pz8kJeXh5MnT6JFixYAgAMHDkCpVKJVq1Yatyt6ElCWADx+/BiZmZkoLi5WO+/r6ytGWERERJXm0aNHSEtLU73OyMhAcnIybG1tYWtri1mzZiE0NBROTk5IT0/HJ598Ak9PTwQHBwMAGjdujC5dumD48OFYvnw5SkpKMGrUKPTr10/jlQHAK5AE5ObmYsiQIdi9e3eF58smQRAREemaWNsGnzhxAh07dlS9LptLEB4ejmXLluHs2bNYs2YN8vLy4OLigqCgIMyZM0dtuGH9+vUYNWoUAgMDoaenh9DQUCxZskSrOERPAsaNG4cHDx7g6NGj6NChA7Zt24Y7d+5g7ty5WLBggdjhERFRDSbWVjkdOnR45r337Nnz3DZsbW0RGxv7UnGIngQcOHAAv/zyC1q2bAk9PT24ubmhc+fOsLS0RHR0NEJCQsQOkYiIqEYSfZ+AgoIC1VpHGxsb5ObmAniyJOLUqVNihkZERDWcWNsGvypETwIaNmyI1NRUAECTJk2wYsUK3Lp1C8uXL1fNgiQiIqoMYj1A6FUh+nDA2LFjkZWVBQCYMWMGunTpgvXr18PIyAirV68WNzgiIqIaTPQkYODAgaqvW7RogevXr+PSpUtwdXWFnZ2diJEREVFNJ9bqgFeFqMMBJSUlqF+/Pi5evKgqMzU1RfPmzZkAEBFRpdPlA4SqI1GTAENDQxQWFooZAhERkWSJPjEwIiICn3/+OUpLS8UOhYiIJEaXTxGsjkSfE3D8+HHs378fe/fuhY+PD8zMzNTOb926VaTIiIiopquus/p1RfQkwNraGqGhoWKHQUREJDmiJwGrVq0SOwQiIpIoZTWd0KcroicBZXJyclSbBjVs2FC1iyAREVFlkXYK8ApMDMzPz8egQYNQu3ZtBAQEICAgALVr18bAgQPx4MEDscMjIiKqsURPAoYPH46jR48iLi4OeXl5yMvLQ1xcHE6cOIEPP/xQ7PCIiKgG4+oAkcXFxWHPnj1o166dqiw4OBjff/89unTpImJkRERU01XXP966InpPQK1atWBlZVWu3MrKCjY2NiJEREREJA2iJwFTp05FZGQksrOzVWXZ2dmYNGkSpk2bJmJkRERU00l922DRhwOWLVuGtLQ0uLq6wtXVFQCQmZkJuVyO3NxcrFixQlX31KlTYoVJREQ1kNSHA0RPAnr16iV2CERERJIkehIwY8YMsUMgIiKJ4rbBr4iTJ0+qHin8+uuvo1mzZiJHRERENV11HcvXFdGTgJycHPTr1w8JCQmwtrYGAOTl5aFjx47YsGED7O3txQ2QiIiohhJ9dcDo0aPx8OFDnD9/Hvfu3cO9e/dw7tw55OfnY8yYMWKHR0RENRg3CxLZ7t27sW/fPjRu3FhV5uXlhaVLlyIoKEjEyIiIqKaT+nCA6D0BSqUShoaG5coNDQ2hVCpFiIiIiEgaRE8C3nrrLYwdOxa3b99Wld26dQvjx49HYGCgiJEREVFNJ/XhANGTgG+++Qb5+flwd3dH/fr1Ub9+fbi7uyM/Px9ff/212OEREVENJujwv+pI9DkBdevWxalTp7Bv3z5cunQJwJM5AewFICIiqlyi9QQkJSUhLi4OACCTydC5c2dYWlpiwYIF6N+/P0aMGIGioiKxwiMiIglQCoLOjupItCRg9uzZOH/+vOp1SkoKhg8fjs6dO2PKlCnYsWMHoqOjxQqPiIgkQOrDAaIlAcnJyWpd/hs2bMCbb76J77//HpGRkViyZAk2bdokVnhEREQ1nmhzAu7fvw9HR0fV68TERHTt2lX1+o033sCNGzfECI2IiCSiunbj64poPQGOjo7IyMgAABQXF+PUqVNo3bq16vzDhw8r3D+AiIhIVzgcIJJu3bphypQpOHz4MKKiomBqaor27durzp89exb169cXKzwiIqIaT7ThgDlz5qB3794ICAiAubk51qxZAyMjI9X5lStXcttgIiKqVFIfDhAtCbCzs8OhQ4fw4MEDmJubQ19fX+385s2bYW5uLlJ0REQkBdW1G19XRN8syMrKqsJyW1vbKo6EiIhIWkRPAoiIiMTC4QAiIiKJkvpwgOgPECIiIpKaQ4cO4e2334aLiwtkMhm2b9+udl4QBEyfPh3Ozs4wMTFBp06dcOXKFbU69+7dQ1hYGCwtLWFtbY1hw4bh0aNHWsXBJICIiCRLEJQ6O7RRUFCAJk2aYOnSpRWej4mJwZIlS7B8+XIcPXoUZmZmCA4ORmFhoapOWFgYzp8/j/j4eMTFxeHQoUMYMWKEVnHIBKHmDYjYmHuKHQJRpcu5tlfsEIgqnaGdR6W271bLV2dtXb979oWuk8lk2LZtG3r16gXgSS+Ai4sLJkyYgIkTJwIAHjx4AEdHR6xevRr9+vXDxYsX4eXlhePHj6Nly5YAgN27d6Nbt264efMmXFxcNLo3ewKIiIh0oKioCPn5+WrHizwNNyMjA9nZ2ejUqZOqzMrKCq1atUJSUhKAJ0/itba2ViUAANCpUyfo6enh6NGjGt+LSQAREUmWIAg6O6Kjo2FlZaV2vMjTcLOzswFA7fk6Za/LzmVnZ8PBwUHtvIGBAWxtbVV1NMHVAUREJFlKHa4OiIqKQmRkpFqZXC7XWfuVgUkAERGRDsjlcp380XdycgIA3LlzB87OzqryO3fuoGnTpqo6OTk5ateVlpbi3r17qus1weEAIiKSLF0OB+hKvXr14OTkhP3796vK8vPzcfToUfj5+QEA/Pz8kJeXh5MnT6rqHDhwAEqlEq1atdL4XuwJICIiyRJrx8BHjx4hLS1N9TojIwPJycmwtbWFq6srxo0bh7lz56JBgwaoV68epk2bBhcXF9UKgsaNG6NLly4YPnw4li9fjpKSEowaNQr9+vXTeGUAwCSAiIioyp04cQIdO3ZUvS6bSxAeHo7Vq1fjk08+QUFBAUaMGIG8vDy0a9cOu3fvhrGxseqa9evXY9SoUQgMDISenh5CQ0OxZMkSreLgPgFE1RT3CSApqOx9ApysG+usrey8izprq6qwJ4CIiCSrBn4O1gonBhIREUkUewKIiEiydLlPQHXEJICIiCSLwwFEREQkSewJICIiyRJrn4BXBZMAIiKSLA4HEBERkSSxJ4CIiCSLqwOIiIgkisMBREREJEnsCSAiIsni6gAiIiKJEiQ+J4DDAURERBLFngAiIpIsDgcQERFJFFcHEBERkSSxJ4CIiCRL6hMDmQQQEZFkcTiAiIiIJIk9AUREJFlS7wlgEkBERJIl7RSAwwFERESSJROk3hdCL62oqAjR0dGIioqCXC4XOxyiSsGfc6qJmATQS8vPz4eVlRUePHgAS0tLscMhqhT8OaeaiMMBREREEsUkgIiISKKYBBAREUkUkwB6aXK5HDNmzOBkKarR+HNONREnBhIREUkUewKIiIgkikkAERGRRDEJICIikigmAaQT7u7uWLRokc7aGzJkCHr16qWz9oheRTNnzkTTpk111l5CQgJkMhny8vJ01ibVbEwCqqG3334bXbp0qfDc4cOHIZPJcPbs2Ze6x7Vr1yCTyZCcnKxR/ePHj2PEiBEvdU+q/nJzczFy5Ei4urpCLpfDyckJwcHB+OOPP8QOrcp06NAB48aN06juxIkTsX///soNiOgZ+BTBamjYsGEIDQ3FzZs3UadOHbVzq1atQsuWLeHr61slsRQXF8PIyAj29vZVcj9tlJSUwNDQUOwwJCU0NBTFxcVYs2YNPDw8cOfOHezfvx93796ttHuW/QxWJ4IgQKFQwNzcHObm5mKHo6Y6fj/pJQhU7ZSUlAiOjo7CnDlz1MofPnwomJubC8uWLRMOHz4stGvXTjA2Nhbq1KkjjB49Wnj06JGqrpubmzBv3jxh6NChgrm5uVC3bl1hxYoVqvN48oRN1REQECAIgiCEh4cLPXv2FObOnSs4OzsL7u7uqvYWLlyouv7+/fvCiBEjBAcHB0Eulwuvv/66sGPHDkEQBGHGjBlCkyZN1GJfuHCh4Obmpnpddp8yu3btEtq2bStYWVkJtra2QkhIiJCWlqY6n5GRIQAQNmzYIPj7+wtyuVxYtWrVC3x36UXdv39fACAkJCQ8tc7169eFHj16CGZmZoKFhYXQp08fITs7W3W+7Gdj+fLlQp06dQQTExOhT58+Ql5enqrO034G165dK7Ro0UIwNzcXHB0dhf79+wt37twRBEEQFAqFULt2beHbb79Vi+fUqVOCTCYTrl27pnoPw4YNE+zs7AQLCwuhY8eOQnJycrn41q5dK7i5uQmWlpbCe++9J+Tn56ti+/fvTkZGhnDw4EEBgLBz506hefPmgqGhoXDw4MEKfxd+/PFHwcvLSzAyMhKcnJyEiIgIQRD+9zN++vTpct/zgwcPCoIgqO5z//59QRAE4a+//hL69esnuLi4CCYmJoK3t7cQGxurdr+AgAAhIiJCGDt2rFCrVi2hQ4cOT/3/RzUPhwOqIQMDAwwePBirV6+G8I9tHjZv3gyFQgE/Pz906dIFoaGhOHv2LDZu3Ijff/8do0aNUmtnwYIFaNmyJU6fPo2PP/4YI0eORGpqKgDg2LFjAIB9+/YhKysLW7duVV23f/9+pKamIj4+HnFxceXiUyqV6Nq1K/744w+sW7cOFy5cwPz586Gvr//C77mgoACRkZE4ceIE9u/fDz09PbzzzjtQKpVq9aZMmYKxY8fi4sWLCA4OfuH7kfbKPtVu374dRUVF5c4rlUr07NkT9+7dQ2JiIuLj43H16lW89957avXS0tKwadMm7NixA7t371b9fP5TRT+DJSUlmDNnDs6cOYPt27fj2rVrGDJkCABAT08P/fv3R2xsrFo769evR9u2beHm5gYA6NOnD3JycrBr1y6cPHkSzZs3R2BgIO7du6e6Jj09Hdu3b0dcXBzi4uKQmJiI+fPnAwAWL14MPz8/DB8+HFlZWcjKykLdunVV106ZMgXz58/HxYsXK+ytW7ZsGSIiIjBixAikpKTg119/haenp6b/C8opLCxEixYt8Ntvv+HcuXMYMWIEBg0apPr9LrNmzRoYGRnhjz/+wPLly1/4flQNiZ2F0Iu5ePGi2icAQRCE9u3bCwMHDhSGDRsmjBgxQq3+4cOHBT09PeHvv/8WBOHJJ/eBAweqziuVSsHBwUFYtmyZIAgVf+oQhCefdBwdHYWioiK18n/2BOzZs0fQ09MTUlNTK4z9RXoC/i03N1cAIKSkpKjFu2jRoqdeQ5Vvy5Ytgo2NjWBsbCy0adNGiIqKEs6cOSMIgiDs3btX0NfXFzIzM1X1z58/LwAQjh07JgjCk58NfX194ebNm6o6u3btEvT09ISsrCxBEJ7+M/hvx48fFwAIDx8+FARBEE6fPi3IZDLh+vXrgiD8r3eg7Gf+8OHDgqWlpVBYWKjWTv369VW9ZDNmzBBMTU1Vn/wFQRAmTZoktGrVSvU6ICBAGDt2rFobZZ/Qt2/frlb+798FFxcX4dNPP63w/bxIT0BFQkJChAkTJqjF26xZs6fWp5qNPQHVVKNGjdCmTRusXLkSwJNPT4cPH8awYcNw5swZrF69WvXJzNzcHMHBwVAqlcjIyFC18c9PIjKZDE5OTsjJyXnuvX18fJ45ZpicnIw6dergtddee4l3qO7KlSvo378/PDw8YGlpCXd3dwBAZmamWr2WLVvq7J6kvdDQUNy+fRu//vorunTpgoSEBDRv3hyrV6/GxYsXUbduXbVPxl5eXrC2tsbFixdVZa6urqhdu7bqtZ+fH5RKpaqXCqj4Z/DkyZN4++234erqCgsLCwQEBAD4389I06ZN0bhxY1VvQGJiInJyctCnTx8AwJkzZ/Do0SPUqlVL7XcnIyMD6enpqvu4u7vDwsJC9drZ2Vmj3xvg2T+fOTk5uH37NgIDAzVqSxMKhQJz5syBj48PbG1tYW5ujj179pT7vWnRooXO7knVCycGVmPDhg3D6NGjsXTpUqxatQr169dHQEAAHj16hA8//BBjxowpd42rq6vq639PmpPJZOW61ytiZmb2zPMmJibPPK+np6c2jAE86cp9lrfffhtubm74/vvv4eLiAqVSCW9vbxQXF2sVG1U+Y2NjdO7cGZ07d8a0adPwwQcfYMaMGZgwYYLO7vHv/88FBQUIDg5GcHAw1q9fD3t7e2RmZiI4OFjtZyQsLAyxsbGYMmUKYmNj0aVLF9SqVQsA8OjRIzg7OyMhIaHc/aytrVVfv+jvTUVx/5MmvzcA1H53nvd788UXX2Dx4sVYtGgRfHx8YGZmhnHjxvH3hlTYE1CN9e3bF3p6eoiNjcXatWvx/vvvQyaToXnz5rhw4QI8PT3LHZrO+i2rp1AotI7L19cXN2/exOXLlys8b29vj+zsbLV/zJ61FPHu3btITU3F1KlTERgYiMaNG+P+/ftax0Xi8PLyQkFBARo3bowbN27gxo0bqnMXLlxAXl4evLy8VGWZmZm4ffu26vWff/4JPT09NGzY8Kn3uHTpEu7evYv58+ejffv2aNSoUYWfzgcMGIBz587h5MmT2LJlC8LCwlTnmjdvjuzsbBgYGJT7vbGzs9P4/RoZGb3Q742FhQXc3d2fumSwbAVOVlaWqux5S3j/+OMP9OzZEwMHDkSTJk3g4eHx1N9LkiYmAdWYubk53nvvPURFRSErK0s1CWry5Mk4cuQIRo0aheTkZFy5cgW//PJLuYmBz+Lg4AATExPs3r0bd+7cwYMHDzS+NiAgAP7+/ggNDUV8fDwyMjKwa9cu7N69G8CTddS5ubmIiYlBeno6li5dil27dj21PRsbG9SqVQvfffcd0tLScODAAURGRmocD1WNu3fv4q233sK6detw9uxZZGRkYPPmzYiJiUHPnj3RqVMn+Pj4ICwsDKdOncKxY8cwePBgBAQEqHWTGxsbIzw8HGfOnMHhw4cxZswY9O3bF05OTk+9t6urK4yMjPD111/j6tWr+PXXXzFnzpxy9dzd3dGmTRsMGzYMCoUCPXr0UJ3r1KkT/Pz80KtXL+zduxfXrl3DkSNH8Omnn+LEiRMafx/c3d1x9OhRXLt2DX/99ZfGvQTAk82DFixYgCVLluDKlSs4deoUvv76awBPegpat26tmliYmJiIqVOnPrO9Bg0aID4+HkeOHMHFixfx4Ycf4s6dOxrHQzUfk4BqbtiwYbh//z6Cg4Ph4uIC4Mkn8cTERFy+fBnt27dHs2bNMH36dNV5TRgYGGDJkiVYsWIFXFxc0LNnT63i+vnnn/HGG2+gf//+8PLywieffKL6dNS4cWN8++23WLp0KZo0aYJjx45h4sSJT21LT08PGzZswMmTJ+Ht7Y3x48fjiy++0Coeqnzm5uZo1aoVFi5cCH9/f3h7e2PatGkYPnw4vvnmG8hkMvzyyy+wsbGBv78/OnXqBA8PD2zcuFGtHU9PT/Tu3RvdunVDUFAQfH198e233z7z3vb29li9ejU2b94MLy8vzJ8/H19++WWFdcPCwnDmzBm88847al3wMpkMO3fuhL+/P4YOHYrXXnsN/fr1w/Xr1+Ho6Kjx92HixInQ19eHl5eXalhCU+Hh4Vi0aBG+/fZbvP766+jevTuuXLmiOr9y5UqUlpaiRYsWGDduHObOnfvM9qZOnYrmzZsjODgYHTp0gJOTE3fiJDV8lDARvTJmzpyJ7du3a7xTJRG9HPYEEBERSRSTACIiIonicAAREZFEsSeAiIhIopgEEBERSRSTACIiIoliEkBERCRRTAKIiIgkikkAkQ4NGTJEbUe2Dh06YNy4cVUeR0JCAmQyGfLy8p5aRyaTYfv27Rq3OXPmTDRt2vSl4rp27RpkMhk3AyJ6RTAJoBpvyJAhkMlkkMlkMDIygqenJ2bPno3S0tJKv/fWrVsr3MO+Ipr84SYi0iU+SpgkoUuXLli1ahWKioqwc+dOREREwNDQEFFRUeXqFhcXa/y0xeextbXVSTtERJWBPQEkCXK5HE5OTnBzc8PIkSPRqVMn/PrrrwD+14U/b948uLi4qB5Ze+PGDfTt2xfW1tawtbVFz549ce3aNVWbCoUCkZGRsLa2Rq1atfDJJ5/g33tv/Xs4oKioCJMnT0bdunUhl8vh6emJH3/8EdeuXUPHjh0BPHlqokwmUz0VUqlUIjo6GvXq1YOJiQmaNGmCLVu2qN1n586deO2112BiYoKOHTuqxampyZMn47XXXoOpqSk8PDwwbdq0Cp9Xv2LFCtStWxempqbo27dvuSdM/vDDD2jcuDGMjY3RqFGjZz785/79+wgLC4O9vT1MTEzQoEEDrFq1SuvYiejFsCeAJMnExAR3795Vvd6/fz8sLS0RHx8PACgpKUFwcDD8/Pxw+PBhGBgYYO7cuejSpQvOnj0LIyMjLFiwAKtXr8bKlSvRuHFjLFiwANu2bcNbb7311PsOHjwYSUlJWLJkCZo0aYKMjAz89ddfqFu3Ln7++WeEhoYiNTUVlpaWqifcRUdHY926dVi+fDkaNGiAQ4cOYeDAgbC3t0dAQABu3LiB3r17IyIiAiNGjMCJEycwYcIErb8nFhYWWL16NVxcXJCSkoLhw4fDwsICn3zyiapOWloaNm3ahB07diA/Px/Dhg3Dxx9/jPXr1wMA1q9fj+nTp+Obb75Bs2bNcPr0aQwfPhxmZmYIDw8vd89p06bhwoUL2LVrF+zs7JCWloa///5b69iJ6AUJRDVceHi40LNnT0EQBEGpVArx8fGCXC4XJk6cqDrv6OgoFBUVqa756aefhIYNGwpKpVJVVlRUJJiYmAh79uwRBEEQnJ2dhZiYGNX5kpISoU6dOqp7CYIgBAQECGPHjhUEQRBSU1MFAEJ8fHyFcR48eFAAINy/f19VVlhYKJiamgpHjhxRqzts2DChf//+giAIQlRUlODl5aV2fvLkyeXa+jcAwrZt2556/osvvhBatGihej1jxgxBX19fuHnzpqps165dgp6enpCVlSUIgiDUr19fiI2NVWtnzpw5gp+fnyAIgpCRkSEAEE6fPi0IgiC8/fbbwtChQ58aAxFVLvYEkCTExcXB3NwcJSUlUCqVGDBgAGbOnKk67+PjozYP4MyZM0hLS4OFhYVaO4WFhUhPT8eDBw+QlZWFVq1aqc4ZGBigZcuW5YYEyiQnJ0NfXx8BAQEax52WlobHjx+jc+fOauXFxcVo1qwZAODixYtqcQCAn5+fxvcos3HjRixZsgTp6el49OgRSktLYWlpqVbH1dUVtWvXVruPUqlEamoqLCwskJ6ejmHDhmH48OGqOqWlpbCysqrwniNHjkRoaChOnTqFoKAg9OrVC23atNE6diJ6MUwCSBI6duyIZcuWwcjICC4uLjAwUP/RNzMzU3v96NEjtGjRQtXN/U/29vYvFENZ9742Hj16BAD47bff1P74Ak/mOehKUlISwsLCMGvWLAQHB8PKygobNmzAggULtI71+++/L5eU6OvrV3hN165dcf36dezcuRPx8fEIDAxEREQEvvzyyxd/M0SkMSYBJAlmZmbw9PTUuH7z5s2xceNGODg4lPs0XMbZ2RlHjx6Fv78/gCefeE+ePInmzZtXWN/HxwdKpRKJiYno1KlTufNlPREKhUJV5uXlBblcjszMzKf2IDRu3Fg1ybHMn3/++fw3+Q9HjhyBm5sbPv30U1XZ9evXy9XLzMzE7du34eLiorqPnp4eGjZsCEdHR7i4uODq1asICwvT+N729vYIDw9HeHg42rdvj0mTJjEJIKoiXB1AVIGwsDDY2dmhZ8+eOHz4MDIyMpCQkIAxY8bg5s2bAICxY8di/vz52L59Oy5duoSPP/74mWv83d3dER4ejvfffx/bt29Xtblp0yYAgJubG2QyGeLi4pCbm4tHjx7BwsICEydOxPjx47FmzRqkp6fj1KlT+Prrr7FmzRoAwEcffYQrV65g0qRJSE1NRWxsLFavXq3V+23QoAEyMzOxYcMGpKenY8mSJdi2bVu5esbGxggPD8eZM2dw+PBhjBkzBn379oWTkxMAYNasWYiOjsaSJUtw+fJlpKSkYNWqVfjqq68qvO/06dPxyy+/IC0tDefPn0dcXBwaN26sVexE9OKYBBBVwNTUFIcOHYKrqyt69+6Nxo0bY9iwYSgsLFT1DEyYMAGDBg1CeHg4/Pz8YGFhgXfeeeeZ7S5btgzvvvsuPv74YzRq1AjDhw9HQUEBAKB27dqYNWsWpkyZAkdHR4waNQoAMGfOHEybNg3R0dFo3LgxunTpgt9++w316tUD8GSc/ueff8b27dvRpEkTLF++HJ999plW77dHjx4YP348Ro0ahaZNm+LIkSOYNm1auXqenp7o3bs3unXrhqCgIPj6+qotAfzggw/www8/YNWqVfDx8UFAQABWr16tivXfjIyMEBUVBV9fX/j7+0NfXx8bNmzQKnYienEy4WmzmIiIiKhGY08AERGRRDEJICIikigmAURERBLFJICIiEiimAQQERFJFJMAIiIiiWISQEREJFFMAoiIiCSKSQAREZFEMQkgIiKSKCYBREREEvV/wJM1CAFUCY4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 600x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision 0.86742 Recall 0.84191 F1 0.85448\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[356,  70],\n",
       "        [ 86, 458]]),\n",
       " 0.8674242424242424,\n",
       " 0.8419117647058824,\n",
       " 0.8544776119402985)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ven_sop_model = VentricularSopraventricularModel(\n",
    "    input_shape=X.shape[1:],\n",
    "    num_classes=2,\n",
    "    num_features=8,\n",
    "    N=4,\n",
    "    filters=128,\n",
    "    verbose=True,\n",
    "    labels=[\"Ventricular\", \"Sopraventricular\"]\n",
    ")\n",
    "ven_sop_model.preprocess_data(X, features, y, test_size=.1)\n",
    "ven_sop_model.compile_train_model(\n",
    "    metrics = get_metrics(),\n",
    "    loss = BinaryCrossentropy(),\n",
    "    callbacks = get_callbacks(patience_es=30, patience_lr=25),\n",
    "    lr=1e-3,\n",
    "    batch_size=128,\n",
    "    epochs=70,\n",
    "    validation_split=.1\n",
    ")\n",
    "ven_sop_model.test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nmc3dpXsJe9r"
   },
   "source": [
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S-qVdr80njUk"
   },
   "outputs": [],
   "source": [
    "metrics = {'folds':[]}\n",
    "i = 1\n",
    "\n",
    "#kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "kf = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "normal_indices = np.where(y == 'N')[0]\n",
    "X_v_s = np.delete(X, normal_indices, axis=0)\n",
    "features_v_s = np.delete(features, normal_indices, axis=0)\n",
    "y_v_s = np.delete(y, normal_indices, axis=0)\n",
    "\n",
    "for train_ids, test_ids in kf.split(X_v_s, y_v_s):\n",
    "\n",
    "    print(\"Fold\", i)\n",
    "    i = i+1\n",
    "\n",
    "    X_train = X_v_s[train_ids]\n",
    "    X_test = X_v_s[test_ids]\n",
    "    features_train = features_v_s[train_ids]\n",
    "    features_test = features_v_s[test_ids]\n",
    "    y_train = y_v_s[train_ids]\n",
    "    y_test = y_v_s[test_ids]\n",
    "\n",
    "    train_uniques, train_counts = np.unique(y_train, return_counts=True)\n",
    "    test_uniques, test_counts = np.unique(y_test, return_counts=True)\n",
    "    fold_metrics = {'train_unique':train_uniques.tolist(), 'train_dist':train_counts.tolist(), 'test_unique':test_uniques.tolist(), 'test_dist':test_counts.tolist()}\n",
    "\n",
    "    ven_sop_model = VentricularSopraventricularModel(\n",
    "        input_shape=X.shape[1:],\n",
    "        num_classes=2,\n",
    "        num_features=8,\n",
    "        N=4,\n",
    "        filters=128,\n",
    "        verbose=True,\n",
    "        labels=[\"Ventricular\", \"Sopraventricular\"]\n",
    "    )\n",
    "    ven_sop_model.preprocess_data(X_train, features_train, y_train)\n",
    "    ven_sop_model.compile_train_model(\n",
    "        metrics = get_metrics(),\n",
    "        loss = BinaryCrossentropy(),\n",
    "        callbacks = get_callbacks(patience_es=30, patience_lr=25),\n",
    "        lr=1e-3,\n",
    "        batch_size=128,\n",
    "        epochs=70,\n",
    "        validation_split=.1\n",
    "    )\n",
    "\n",
    "    # Evaluate models on unseen data\n",
    "    pred_ven_sop = ven_sop_model.get_predictions_model(X_test, features_test)\n",
    "    map_labels = {'V': 0, 'S': 1}\n",
    "    y_real = np.array([map_labels[label[0]] for label in y_test], dtype=np.float32)\n",
    "\n",
    "    confusion_mat, precision, recall, f1 = get_score_model(pred_ven_sop, y_real)\n",
    "    print(confusion_mat, precision, recall, f1)\n",
    "    fold_metrics['conf_matrix'] = confusion_mat.tolist()\n",
    "    fold_metrics['precision'] = precision.tolist()\n",
    "    fold_metrics['recall'] = recall.tolist()\n",
    "    fold_metrics['f1'] = f1.tolist()\n",
    "    metrics['folds'].append(fold_metrics)\n",
    "\n",
    "    del ven_sop_model\n",
    "\n",
    "\n",
    "with open('./Metrics/metrics_cv_ven_sop_model.json', 'w') as jsonfile:\n",
    "    json.dump(metrics, jsonfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Complete Model"
   ],
   "metadata": {
    "id": "pwJLv0-QMQc7"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class CompleteModel(ResNetModel):\n",
    "    def __init__(self, input_shape, num_classes, num_features, N, filters, verbose, labels):\n",
    "        super().__init__(input_shape, num_classes, num_features, N, filters, verbose, labels)\n",
    "\n",
    "    def preprocess_data(self, X, features, y, test_size = None):\n",
    "\n",
    "        map_nor_abn = {'N': [1, 0, 0], 'V': [0, 1, 0], 'S': [0, 0, 1]}\n",
    "        y_ohe = np.array([map_nor_abn[label[0]] for label in y], dtype=np.float32)\n",
    "\n",
    "        if test_size is None:\n",
    "            self.X_train = X\n",
    "            self.y_ohe_train = y_ohe\n",
    "            self.features_train = features\n",
    "        else:\n",
    "            self.X_train, self.X_test, self.y_ohe_train, self.y_ohe_test, self.features_train, self.features_test = train_test_split(\n",
    "                X, y_ohe, features,\n",
    "                test_size=test_size,\n",
    "                stratify=y,\n",
    "                random_state=self.seed,\n",
    "                shuffle=True\n",
    "            )\n",
    "\n",
    "        if self.verbose:\n",
    "            print('X_train.shape {} y_ohe_train.shape {} features_test.shape {}'.format(self.X_train.shape,  self.y_ohe_train.shape, self.features_train.shape))\n",
    "            uniques, counts = np.unique(self.y_ohe_train, return_counts=True, axis=0)\n",
    "            print('Distribution training set:', uniques, counts)\n",
    "            if test_size is not None:\n",
    "                print('X_test.shape {} y_ohe_test.shape {} features_test.shape {}'.format(self.X_test.shape, self.y_ohe_test.shape, self.features_test.shape))"
   ],
   "metadata": {
    "id": "-oCBXJNHTNce"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Cross validation single model"
   ],
   "metadata": {
    "id": "aeieuNZs7uHv"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "metrics = {'folds':[]}\n",
    "i=1\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "unique_signal_ids = np.unique(maps_idx[:,0])\n",
    "\n",
    "for train_signal_ids, test_signal_ids in kf.split(unique_signal_ids):\n",
    "\n",
    "    print(\"Fold\", i)\n",
    "    i=i+1\n",
    "\n",
    "    fold_metrics = {'train_signals':train_signal_ids.tolist(), 'test_signal':test_signal_ids.tolist()}\n",
    "\n",
    "    train_mask = np.isin(maps_idx[:,0], train_signal_ids)\n",
    "    test_mask = np.isin(maps_idx[:,0], test_signal_ids)\n",
    "\n",
    "    X_train = X[train_mask, :]\n",
    "    X_test = X[test_mask, :]\n",
    "    features_train = features[train_mask, :]\n",
    "    features_test = features[test_mask, :]\n",
    "    y_train = y[train_mask,:]\n",
    "    y_test = y[test_mask,:]\n",
    "\n",
    "    train_uniques, train_counts = np.unique(y_train, return_counts=True)\n",
    "    test_uniques, test_counts = np.unique(y_test, return_counts=True)\n",
    "    print(\"Train:\", train_uniques, train_counts, \"Test:\", test_uniques, test_counts)\n",
    "    fold_metrics['train_unique'] = train_uniques.tolist()\n",
    "    fold_metrics['test_unique'] = test_uniques.tolist()\n",
    "    fold_metrics['train_dist'] = train_counts.tolist()\n",
    "    fold_metrics['test_dist'] = test_counts.tolist()\n",
    "\n",
    "    # Train 1st model\n",
    "    c_model = CompleteModel(\n",
    "        input_shape=X_train.shape[1:],\n",
    "        num_classes=3,\n",
    "        num_features=8,\n",
    "        N=6,\n",
    "        filters=128,\n",
    "        verbose=True,\n",
    "        labels=[\"N\", \"V\", \"S\"]\n",
    "    )\n",
    "    c_model.preprocess_data(X_train, features_train, y_train)\n",
    "    c_model.compile_train_model(\n",
    "        metrics = get_metrics(),\n",
    "        loss = CategoricalCrossentropy(),\n",
    "        callbacks = get_callbacks(patience_es=10, patience_lr=7),\n",
    "        lr=1e-4,\n",
    "        batch_size=256,\n",
    "        epochs=30,\n",
    "        validation_split=0.1,\n",
    "        use_sample_weight=True\n",
    "    )\n",
    "\n",
    "    # Evaluate models on unseen data\n",
    "    pred = c_model.get_predictions_model(X_test, features_test)\n",
    "\n",
    "    map_labels = {'N': 0, 'V': 1, 'S': 2}\n",
    "    y_real = np.array([map_labels[label[0]] for label in y_test], dtype=np.float32)\n",
    "\n",
    "    confusion_mat, precision, recall, f1 = get_score_model(pred, y_real)\n",
    "    print(confusion_mat, precision, recall, f1)\n",
    "    fold_metrics['conf_matrix'] = confusion_mat.tolist()\n",
    "    fold_metrics['precision'] = precision.tolist()\n",
    "    fold_metrics['recall'] = recall.tolist()\n",
    "    fold_metrics['f1'] = f1.tolist()\n",
    "    metrics['folds'].append(fold_metrics)\n",
    "\n",
    "with open('./Metrics/metrics_cv_complete.json', 'w') as jsonfile:\n",
    "    json.dump(metrics, jsonfile)"
   ],
   "metadata": {
    "id": "bBKhckilT_6J"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Bagging"
   ],
   "metadata": {
    "id": "Bi3m7P4iNWNj"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def create_bootstrap_samples(X, features, y, indexes_N, indexes_V, indexes_S):\n",
    "    # Generate the bootstraps per class\n",
    "    bootstrap_indices_N_idx = np.random.randint(low=0, high=len(indexes_N), size=int(0.25 * len(indexes_N)))\n",
    "    bootstrap_indices_N = indexes_N[bootstrap_indices_N_idx]\n",
    "    bootstrap_indices_V_idx = np.random.randint(low=0, high=len(indexes_V), size=len(indexes_V))\n",
    "    bootstrap_indices_V = indexes_V[bootstrap_indices_V_idx]\n",
    "    bootstrap_indices_S_idx = np.random.randint(low=0, high=len(indexes_S), size=len(indexes_S))\n",
    "    bootstrap_indices_S = indexes_S[bootstrap_indices_S_idx]\n",
    "\n",
    "    # Joint the indexes\n",
    "    bootstrap_indices = np.concatenate([bootstrap_indices_N, bootstrap_indices_V, bootstrap_indices_S], axis=0)\n",
    "    np.random.shuffle(bootstrap_indices)\n",
    "\n",
    "    # Create the oob samples\n",
    "    oob_N_complete = np.delete(indexes_N, bootstrap_indices_N_idx, axis=0)\n",
    "    oob_N = np.random.randint(low=0, high=len(oob_N_complete), size=int(0.25 * len(oob_N_complete)))\n",
    "    oob_N = oob_N_complete[oob_N]\n",
    "\n",
    "    oob_V = np.delete(indexes_V, bootstrap_indices_V_idx, axis=0)\n",
    "    oob_S = np.delete(indexes_S, bootstrap_indices_S_idx, axis=0)\n",
    "\n",
    "    oob_indices = np.concatenate([oob_N, oob_V, oob_S], axis=0)\n",
    "    np.random.shuffle(oob_indices)\n",
    "\n",
    "    return X[bootstrap_indices], features[bootstrap_indices], y[bootstrap_indices], X[oob_indices], features[oob_indices], y[oob_indices]"
   ],
   "metadata": {
    "id": "MC0_B9w_Uny-"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "bagging_models = []\n",
    "\n",
    "X_train, X_test, y_train, y_test, features_train, features_test = train_test_split(\n",
    "    X, y, features,\n",
    "    test_size=0.1,\n",
    "    stratify=y,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "indexes_N = np.where(y_train =='N')[0]\n",
    "indexes_V = np.where(y_train =='V')[0]\n",
    "indexes_S = np.where(y_train =='S')[0]\n",
    "\n",
    "n_bootstraps = 10\n",
    "\n",
    "for i in range(n_bootstraps):\n",
    "    print(f'Building bootstrap {i}')\n",
    "    X_boot, features_boot, y_boot, X_oob, features_oob, y_oob = create_bootstrap_samples(X, features, y, indexes_N, indexes_V, indexes_S)\n",
    "\n",
    "    c_model_bagging = CompleteModel(\n",
    "        input_shape=X_train.shape[1:],\n",
    "        num_classes=3,\n",
    "        num_features=8,\n",
    "        N=4,\n",
    "        filters=128,\n",
    "        verbose=True,\n",
    "        labels=[\"N\", \"V\", \"S\"]\n",
    "    )\n",
    "    c_model_bagging.preprocess_data(X_boot, features_boot, y_boot)\n",
    "    c_model_bagging.compile_train_model(\n",
    "        metrics = get_metrics(),\n",
    "        loss = CategoricalCrossentropy(),\n",
    "        callbacks = get_callbacks(patience_es=5, patience_lr=5),\n",
    "        lr=1e-4,\n",
    "        batch_size=256,\n",
    "        epochs=15,\n",
    "        validation_split=0.1,\n",
    "        use_sample_weight=False\n",
    "    )\n",
    "\n",
    "    pred = c_model_bagging.get_predictions_model(X_oob, features_oob)\n",
    "\n",
    "    map_labels = {'N': 0, 'V': 1, 'S': 2}\n",
    "    y_real = np.array([map_labels[label[0]] for label in y_oob], dtype=np.float32)\n",
    "\n",
    "    confusion_mat, precision, recall, f1 = get_score_model(pred, y_real)\n",
    "    print(confusion_mat, precision, recall, f1)\n",
    "\n",
    "    bagging_models.append({'Idx': i, 'Model': c_model_bagging, 'Precision': precision, 'Recall': recall, 'F1': f1})\n",
    "\n",
    "    del X_boot, features_boot, y_boot, c_model_bagging\n"
   ],
   "metadata": {
    "id": "6Atw23qPPH_x"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Cross-Validation Bagging"
   ],
   "metadata": {
    "id": "rExw2E2ZeoAK"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def majority_vote(predictions):\n",
    "    predictions_array = np.array(predictions)\n",
    "    final_predictions = np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=predictions_array)\n",
    "\n",
    "    return final_predictions"
   ],
   "metadata": {
    "id": "yStrLUW2d9-R"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def create_bootstrap_samples_CV(X_t, features_t, y_t, indexes_N, indexes_V, indexes_S):\n",
    "    # Generate the bootstraps per class\n",
    "    bootstrap_indices_N_idx = np.random.randint(low=0, high=len(indexes_N), size=int(0.30 * len(indexes_N)))\n",
    "    bootstrap_indices_N = indexes_N[bootstrap_indices_N_idx]\n",
    "    bootstrap_indices_V_idx = np.random.randint(low=0, high=len(indexes_V), size=len(indexes_V))\n",
    "    bootstrap_indices_V = indexes_V[bootstrap_indices_V_idx]\n",
    "    bootstrap_indices_S_idx = np.random.randint(low=0, high=len(indexes_S), size=len(indexes_S))\n",
    "    bootstrap_indices_S = indexes_S[bootstrap_indices_S_idx]\n",
    "\n",
    "    # Joint the indexes\n",
    "    bootstrap_indices = np.concatenate([bootstrap_indices_N, bootstrap_indices_V, bootstrap_indices_S], axis=0)\n",
    "    np.random.shuffle(bootstrap_indices)\n",
    "\n",
    "    return X_t[bootstrap_indices], features_t[bootstrap_indices], y_t[bootstrap_indices]"
   ],
   "metadata": {
    "id": "ZtGjKUZaewFB"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "metrics = {'folds':[]}\n",
    "fold=1\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "unique_signal_ids = np.unique(maps_idx[:,0])\n",
    "\n",
    "for train_signal_ids, test_signal_ids in kf.split(unique_signal_ids):\n",
    "\n",
    "    print(\"Fold\", fold)\n",
    "    fold=fold+1\n",
    "\n",
    "    fold_metrics = {'train_signals':train_signal_ids.tolist(), 'test_signal':test_signal_ids.tolist()}\n",
    "\n",
    "    train_mask = np.isin(maps_idx[:,0], train_signal_ids)\n",
    "    test_mask = np.isin(maps_idx[:,0], test_signal_ids)\n",
    "\n",
    "    X_train = X[train_mask, :]\n",
    "    X_test = X[test_mask, :]\n",
    "    features_train = features[train_mask, :]\n",
    "    features_test = features[test_mask, :]\n",
    "    y_train = y[train_mask,:]\n",
    "    y_test = y[test_mask,:]\n",
    "\n",
    "    train_uniques, train_counts = np.unique(y_train, return_counts=True)\n",
    "    test_uniques, test_counts = np.unique(y_test, return_counts=True)\n",
    "    print(\"Train:\", train_uniques, train_counts, \"Test:\", test_uniques, test_counts)\n",
    "    fold_metrics['train_unique'] = train_uniques.tolist()\n",
    "    fold_metrics['test_unique'] = test_uniques.tolist()\n",
    "    fold_metrics['train_dist'] = train_counts.tolist()\n",
    "    fold_metrics['test_dist'] = test_counts.tolist()\n",
    "\n",
    "    bagging_models_cv = []\n",
    "    indexes_N = np.where(y_train =='N')[0]\n",
    "    indexes_V = np.where(y_train =='V')[0]\n",
    "    indexes_S = np.where(y_train =='S')[0]\n",
    "\n",
    "    n_bootstraps = 10\n",
    "\n",
    "    for i in range(n_bootstraps):\n",
    "        print(f'Building bootstrap model {i+1}')\n",
    "        X_boot, features_boot, y_boot = create_bootstrap_samples_CV(X_train, features_train, y_train, indexes_N, indexes_V, indexes_S)\n",
    "\n",
    "        c_model_bagging = CompleteModel(\n",
    "            input_shape=X_train.shape[1:],\n",
    "            num_classes=3,\n",
    "            num_features=8,\n",
    "            N=4,\n",
    "            filters=128,\n",
    "            verbose=True,\n",
    "            labels=[\"N\", \"V\", \"S\"]\n",
    "        )\n",
    "        c_model_bagging.preprocess_data(X_boot, features_boot, y_boot)\n",
    "        c_model_bagging.compile_train_model(\n",
    "            metrics = get_metrics(),\n",
    "            loss = CategoricalCrossentropy(),\n",
    "            callbacks = get_callbacks(patience_es=5, patience_lr=3),\n",
    "            lr=1e-4,\n",
    "            batch_size=256,\n",
    "            epochs=10,\n",
    "            validation_split=0.1,\n",
    "            use_sample_weight=False\n",
    "        )\n",
    "\n",
    "        bagging_models_cv.append({'Idx': i, 'Model': c_model_bagging})\n",
    "\n",
    "        del X_boot, features_boot, y_boot, c_model_bagging\n",
    "\n",
    "    # Prediction with test set\n",
    "    predictions = []\n",
    "    map_labels = {'N': 0, 'V': 1, 'S': 2}\n",
    "    y_real = np.array([map_labels[label[0]] for label in y_test], dtype=np.float32)\n",
    "    for model in bagging_models_cv:\n",
    "        predictions.append(np.array(model['Model'].get_predictions_model(X_test, features_test)))\n",
    "\n",
    "    del bagging_models_cv\n",
    "\n",
    "    final_predictions = majority_vote(predictions)\n",
    "\n",
    "    confusion_mat, precision, recall, f1 = get_score_model(final_predictions, y_real)\n",
    "    print(confusion_mat, precision, recall, f1)\n",
    "    fold_metrics['conf_matrix'] = confusion_mat.tolist()\n",
    "    fold_metrics['precision'] = precision.tolist()\n",
    "    fold_metrics['recall'] = recall.tolist()\n",
    "    fold_metrics['f1'] = f1.tolist()\n",
    "    metrics['folds'].append(fold_metrics)\n",
    "\n",
    "    del predictions, final_predictions, indexes_N, indexes_V, indexes_S\n",
    "\n",
    "with open('./Metrics/metrics_cv_bagging.json', 'w') as jsonfile:\n",
    "    json.dump(metrics, jsonfile)\n"
   ],
   "metadata": {
    "id": "BPOVwwmx1xkT"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Final training"
   ],
   "metadata": {
    "id": "V_SOmEMt81Iu"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model Normal vs Abnormal"
   ],
   "metadata": {
    "id": "6aVJLheW82T9"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "final_nor_abn_model = NormalAbnormalModel(\n",
    "    input_shape=X.shape[1:],\n",
    "    num_classes=2,\n",
    "    num_features=8,\n",
    "    N=6,\n",
    "    filters=128,\n",
    "    verbose=True,\n",
    "    labels=[\"Normal\", \"Abnormal\"]\n",
    ")\n",
    "final_nor_abn_model.preprocess_data(X, features, y)\n",
    "final_nor_abn_model.compile_train_model(\n",
    "    metrics = get_metrics(),\n",
    "    loss = KLDivergence(reduction=\"sum_over_batch_size\", name=\"kl_divergence\"),\n",
    "    callbacks = get_callbacks(),\n",
    "    lr=1e-3,\n",
    "    batch_size=256,\n",
    "    epochs=25,\n",
    "    validation_split=.1\n",
    ")\n",
    "final_nor_abn_model.save_model(\"normal_abnormal\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3D9bjI5v8xk_",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1706561431930,
     "user_tz": -60,
     "elapsed": 337355,
     "user": {
      "displayName": "A Z",
      "userId": "08254881691717370367"
     }
    },
    "outputId": "b7880c97-0efb-48ed-d7bc-d8981a745253"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "X_train.shape (112848, 128, 2) y_ohe_train.shape (112848, 2) features_test.shape (112848, 8)\n",
      "Epoch 1/25\n",
      "397/397 [==============================] - 58s 103ms/step - loss: 0.0936 - precision: 0.9642 - recall: 0.9642 - fn: 3641.0000 - val_loss: 0.0098 - val_precision: 0.9962 - val_recall: 0.9962 - val_fn: 43.0000 - lr: 0.0010\n",
      "Epoch 2/25\n",
      "397/397 [==============================] - 37s 93ms/step - loss: 0.0785 - precision: 0.9717 - recall: 0.9717 - fn: 2872.0000 - val_loss: 0.0065 - val_precision: 0.9993 - val_recall: 0.9993 - val_fn: 8.0000 - lr: 0.0010\n",
      "Epoch 3/25\n",
      "397/397 [==============================] - 38s 96ms/step - loss: 0.0732 - precision: 0.9742 - recall: 0.9742 - fn: 2624.0000 - val_loss: 0.0060 - val_precision: 0.9996 - val_recall: 0.9996 - val_fn: 5.0000 - lr: 0.0010\n",
      "Epoch 4/25\n",
      "397/397 [==============================] - 39s 98ms/step - loss: 0.0703 - precision: 0.9752 - recall: 0.9752 - fn: 2515.0000 - val_loss: 0.0151 - val_precision: 0.9914 - val_recall: 0.9914 - val_fn: 97.0000 - lr: 0.0010\n",
      "Epoch 5/25\n",
      "397/397 [==============================] - 39s 98ms/step - loss: 0.0691 - precision: 0.9754 - recall: 0.9754 - fn: 2503.0000 - val_loss: 0.0179 - val_precision: 0.9906 - val_recall: 0.9906 - val_fn: 106.0000 - lr: 0.0010\n",
      "Epoch 6/25\n",
      "397/397 [==============================] - ETA: 0s - loss: 0.0678 - precision: 0.9761 - recall: 0.9761 - fn: 2424.0000\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "397/397 [==============================] - 40s 100ms/step - loss: 0.0678 - precision: 0.9761 - recall: 0.9761 - fn: 2424.0000 - val_loss: 0.0138 - val_precision: 0.9930 - val_recall: 0.9930 - val_fn: 79.0000 - lr: 0.0010\n",
      "Epoch 7/25\n",
      "397/397 [==============================] - 38s 95ms/step - loss: 0.0629 - precision: 0.9774 - recall: 0.9774 - fn: 2292.0000 - val_loss: 0.0067 - val_precision: 0.9986 - val_recall: 0.9986 - val_fn: 16.0000 - lr: 1.0000e-04\n",
      "Saving model at ./Models/normal_abnormal.keras\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Bagging classifier"
   ],
   "metadata": {
    "id": "Ao81swDe86fC"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "indexes_N = np.where(y =='N')[0]\n",
    "indexes_V = np.where(y =='V')[0]\n",
    "indexes_S = np.where(y =='S')[0]\n",
    "\n",
    "n_bootstraps = 10\n",
    "\n",
    "for i in range(n_bootstraps):\n",
    "    print(f'Building bootstrap {i}')\n",
    "\n",
    "    X_boot, features_boot, y_boot = create_bootstrap_samples_CV(X, features, y, indexes_N, indexes_V, indexes_S)\n",
    "\n",
    "    c_model_bagging = CompleteModel(\n",
    "        input_shape=X.shape[1:],\n",
    "        num_classes=3,\n",
    "        num_features=8,\n",
    "        N=4,\n",
    "        filters=128,\n",
    "        verbose=True,\n",
    "        labels=[\"N\", \"V\", \"S\"]\n",
    "    )\n",
    "    c_model_bagging.preprocess_data(X_boot, features_boot, y_boot)\n",
    "    c_model_bagging.compile_train_model(\n",
    "        metrics = get_metrics(),\n",
    "        loss = CategoricalCrossentropy(),\n",
    "        callbacks = get_callbacks(patience_es=5, patience_lr=5),\n",
    "        lr=1e-4,\n",
    "        batch_size=256,\n",
    "        epochs=20,\n",
    "        validation_split=0.1\n",
    "    )\n",
    "\n",
    "    c_model_bagging.save_model(f'bagging_model_{i}', path=\"./Models/Bagging\")\n",
    "\n",
    "    del X_boot, features_boot, y_boot, c_model_bagging\n"
   ],
   "metadata": {
    "id": "mRGXHd4Oqh68",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1706563263471,
     "user_tz": -60,
     "elapsed": 1309580,
     "user": {
      "displayName": "A Z",
      "userId": "08254881691717370367"
     }
    },
    "outputId": "7d80ebc9-db92-46d7-b6b2-4a8b6d8ba32e"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Building bootstrap 0\n",
      "X_train.shape (40653, 128, 2) y_ohe_train.shape (40653, 3) features_test.shape (40653, 8)\n",
      "Distribution training set: [[0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]] [ 5281  4416 30956]\n",
      "Epoch 1/20\n",
      "143/143 [==============================] - 45s 51ms/step - loss: 0.2799 - precision: 0.9055 - recall: 0.8566 - fn: 5245.0000 - val_loss: 0.5183 - val_precision: 0.9092 - val_recall: 0.7634 - val_fn: 962.0000 - lr: 1.0000e-04\n",
      "Epoch 2/20\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 0.2401 - precision: 0.9114 - recall: 0.8824 - fn: 4303.0000 - val_loss: 0.3809 - val_precision: 0.8878 - val_recall: 0.8060 - val_fn: 789.0000 - lr: 1.0000e-04\n",
      "Epoch 3/20\n",
      "143/143 [==============================] - 5s 34ms/step - loss: 0.2307 - precision: 0.9153 - recall: 0.8903 - fn: 4014.0000 - val_loss: 0.3083 - val_precision: 0.9016 - val_recall: 0.8180 - val_fn: 740.0000 - lr: 1.0000e-04\n",
      "Epoch 4/20\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 0.2198 - precision: 0.9217 - recall: 0.8974 - fn: 3752.0000 - val_loss: 0.2777 - val_precision: 0.8889 - val_recall: 0.8657 - val_fn: 546.0000 - lr: 1.0000e-04\n",
      "Epoch 5/20\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 0.2124 - precision: 0.9251 - recall: 0.9047 - fn: 3487.0000 - val_loss: 0.2380 - val_precision: 0.9077 - val_recall: 0.8928 - val_fn: 436.0000 - lr: 1.0000e-04\n",
      "Epoch 6/20\n",
      "143/143 [==============================] - 5s 34ms/step - loss: 0.2075 - precision: 0.9267 - recall: 0.9062 - fn: 3431.0000 - val_loss: 0.2109 - val_precision: 0.9245 - val_recall: 0.9041 - val_fn: 390.0000 - lr: 1.0000e-04\n",
      "Epoch 7/20\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 0.2017 - precision: 0.9294 - recall: 0.9104 - fn: 3279.0000 - val_loss: 0.2072 - val_precision: 0.9247 - val_recall: 0.9063 - val_fn: 381.0000 - lr: 1.0000e-04\n",
      "Epoch 8/20\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 0.1976 - precision: 0.9311 - recall: 0.9132 - fn: 3177.0000 - val_loss: 0.1963 - val_precision: 0.9291 - val_recall: 0.9147 - val_fn: 347.0000 - lr: 1.0000e-04\n",
      "Epoch 9/20\n",
      "143/143 [==============================] - 5s 35ms/step - loss: 0.1942 - precision: 0.9310 - recall: 0.9149 - fn: 3115.0000 - val_loss: 0.1922 - val_precision: 0.9324 - val_recall: 0.9132 - val_fn: 353.0000 - lr: 1.0000e-04\n",
      "Epoch 10/20\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 0.1890 - precision: 0.9343 - recall: 0.9191 - fn: 2960.0000 - val_loss: 0.1915 - val_precision: 0.9300 - val_recall: 0.9154 - val_fn: 344.0000 - lr: 1.0000e-04\n",
      "Epoch 11/20\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 0.1860 - precision: 0.9355 - recall: 0.9204 - fn: 2912.0000 - val_loss: 0.1885 - val_precision: 0.9292 - val_recall: 0.9203 - val_fn: 324.0000 - lr: 1.0000e-04\n",
      "Epoch 12/20\n",
      "143/143 [==============================] - 5s 34ms/step - loss: 0.1835 - precision: 0.9346 - recall: 0.9209 - fn: 2895.0000 - val_loss: 0.1909 - val_precision: 0.9295 - val_recall: 0.9149 - val_fn: 346.0000 - lr: 1.0000e-04\n",
      "Epoch 13/20\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 0.1787 - precision: 0.9385 - recall: 0.9262 - fn: 2701.0000 - val_loss: 0.1869 - val_precision: 0.9299 - val_recall: 0.9196 - val_fn: 327.0000 - lr: 1.0000e-04\n",
      "Epoch 14/20\n",
      "143/143 [==============================] - 5s 33ms/step - loss: 0.1755 - precision: 0.9372 - recall: 0.9248 - fn: 2750.0000 - val_loss: 0.1860 - val_precision: 0.9305 - val_recall: 0.9186 - val_fn: 331.0000 - lr: 1.0000e-04\n",
      "Epoch 15/20\n",
      "143/143 [==============================] - 5s 35ms/step - loss: 0.1736 - precision: 0.9396 - recall: 0.9275 - fn: 2651.0000 - val_loss: 0.1750 - val_precision: 0.9402 - val_recall: 0.9284 - val_fn: 291.0000 - lr: 1.0000e-04\n",
      "Epoch 16/20\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 0.1676 - precision: 0.9418 - recall: 0.9312 - fn: 2519.0000 - val_loss: 0.1798 - val_precision: 0.9314 - val_recall: 0.9220 - val_fn: 317.0000 - lr: 1.0000e-04\n",
      "Epoch 17/20\n",
      "143/143 [==============================] - 4s 31ms/step - loss: 0.1638 - precision: 0.9425 - recall: 0.9315 - fn: 2506.0000 - val_loss: 0.1782 - val_precision: 0.9327 - val_recall: 0.9238 - val_fn: 310.0000 - lr: 1.0000e-04\n",
      "Epoch 18/20\n",
      "143/143 [==============================] - 5s 33ms/step - loss: 0.1625 - precision: 0.9431 - recall: 0.9331 - fn: 2446.0000 - val_loss: 0.1852 - val_precision: 0.9330 - val_recall: 0.9247 - val_fn: 306.0000 - lr: 1.0000e-04\n",
      "Epoch 19/20\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 0.1601 - precision: 0.9442 - recall: 0.9330 - fn: 2453.0000 - val_loss: 0.1781 - val_precision: 0.9292 - val_recall: 0.9225 - val_fn: 315.0000 - lr: 1.0000e-04\n",
      "Epoch 20/20\n",
      "143/143 [==============================] - ETA: 0s - loss: 0.1559 - precision: 0.9451 - recall: 0.9350 - fn: 2379.0000\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "143/143 [==============================] - 4s 31ms/step - loss: 0.1559 - precision: 0.9451 - recall: 0.9350 - fn: 2379.0000 - val_loss: 0.1771 - val_precision: 0.9346 - val_recall: 0.9250 - val_fn: 305.0000 - lr: 1.0000e-04\n",
      "Saving model at ./Models/Bagging/bagging_model_0.keras\n",
      "Building bootstrap 1\n",
      "X_train.shape (40653, 128, 2) y_ohe_train.shape (40653, 3) features_test.shape (40653, 8)\n",
      "Distribution training set: [[0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]] [ 5281  4416 30956]\n",
      "Epoch 1/20\n",
      "143/143 [==============================] - 18s 35ms/step - loss: 0.4843 - precision: 0.9688 - recall: 0.6236 - fn: 13770.0000 - val_loss: 0.4170 - val_precision: 0.9272 - val_recall: 0.7427 - val_fn: 1046.0000 - lr: 1.0000e-04\n",
      "Epoch 2/20\n",
      "143/143 [==============================] - 5s 34ms/step - loss: 0.2677 - precision: 0.9176 - recall: 0.8399 - fn: 5856.0000 - val_loss: 0.3029 - val_precision: 0.8834 - val_recall: 0.8606 - val_fn: 567.0000 - lr: 1.0000e-04\n",
      "Epoch 3/20\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 0.2490 - precision: 0.9084 - recall: 0.8749 - fn: 4578.0000 - val_loss: 0.3325 - val_precision: 0.8693 - val_recall: 0.8559 - val_fn: 586.0000 - lr: 1.0000e-04\n",
      "Epoch 4/20\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 0.2382 - precision: 0.9157 - recall: 0.8824 - fn: 4304.0000 - val_loss: 0.2945 - val_precision: 0.8912 - val_recall: 0.8564 - val_fn: 584.0000 - lr: 1.0000e-04\n",
      "Epoch 5/20\n",
      "143/143 [==============================] - 6s 43ms/step - loss: 0.2268 - precision: 0.9213 - recall: 0.8920 - fn: 3951.0000 - val_loss: 0.2505 - val_precision: 0.9051 - val_recall: 0.8822 - val_fn: 479.0000 - lr: 1.0000e-04\n",
      "Epoch 6/20\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 0.2195 - precision: 0.9223 - recall: 0.8972 - fn: 3761.0000 - val_loss: 0.2311 - val_precision: 0.9198 - val_recall: 0.8974 - val_fn: 417.0000 - lr: 1.0000e-04\n",
      "Epoch 7/20\n",
      "143/143 [==============================] - 4s 31ms/step - loss: 0.2120 - precision: 0.9249 - recall: 0.9022 - fn: 3577.0000 - val_loss: 0.2288 - val_precision: 0.9189 - val_recall: 0.9006 - val_fn: 404.0000 - lr: 1.0000e-04\n",
      "Epoch 8/20\n",
      "143/143 [==============================] - 5s 37ms/step - loss: 0.2081 - precision: 0.9253 - recall: 0.9033 - fn: 3539.0000 - val_loss: 0.2320 - val_precision: 0.9169 - val_recall: 0.9006 - val_fn: 404.0000 - lr: 1.0000e-04\n",
      "Epoch 9/20\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 0.2037 - precision: 0.9283 - recall: 0.9076 - fn: 3379.0000 - val_loss: 0.2210 - val_precision: 0.9248 - val_recall: 0.9019 - val_fn: 399.0000 - lr: 1.0000e-04\n",
      "Epoch 10/20\n",
      "143/143 [==============================] - 5s 35ms/step - loss: 0.1997 - precision: 0.9290 - recall: 0.9097 - fn: 3305.0000 - val_loss: 0.2247 - val_precision: 0.9200 - val_recall: 0.9051 - val_fn: 386.0000 - lr: 1.0000e-04\n",
      "Epoch 11/20\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 0.1966 - precision: 0.9286 - recall: 0.9104 - fn: 3278.0000 - val_loss: 0.2128 - val_precision: 0.9292 - val_recall: 0.9075 - val_fn: 376.0000 - lr: 1.0000e-04\n",
      "Epoch 12/20\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 0.1936 - precision: 0.9317 - recall: 0.9132 - fn: 3174.0000 - val_loss: 0.2067 - val_precision: 0.9281 - val_recall: 0.9080 - val_fn: 374.0000 - lr: 1.0000e-04\n",
      "Epoch 13/20\n",
      "143/143 [==============================] - 5s 35ms/step - loss: 0.1891 - precision: 0.9313 - recall: 0.9132 - fn: 3174.0000 - val_loss: 0.2116 - val_precision: 0.9289 - val_recall: 0.9152 - val_fn: 345.0000 - lr: 1.0000e-04\n",
      "Epoch 14/20\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 0.1867 - precision: 0.9331 - recall: 0.9166 - fn: 3051.0000 - val_loss: 0.2068 - val_precision: 0.9278 - val_recall: 0.9100 - val_fn: 366.0000 - lr: 1.0000e-04\n",
      "Epoch 15/20\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 0.1834 - precision: 0.9331 - recall: 0.9176 - fn: 3016.0000 - val_loss: 0.2143 - val_precision: 0.9285 - val_recall: 0.9073 - val_fn: 377.0000 - lr: 1.0000e-04\n",
      "Epoch 16/20\n",
      "143/143 [==============================] - 6s 39ms/step - loss: 0.1801 - precision: 0.9348 - recall: 0.9197 - fn: 2939.0000 - val_loss: 0.1974 - val_precision: 0.9322 - val_recall: 0.9137 - val_fn: 351.0000 - lr: 1.0000e-04\n",
      "Epoch 17/20\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 0.1773 - precision: 0.9363 - recall: 0.9220 - fn: 2852.0000 - val_loss: 0.2018 - val_precision: 0.9303 - val_recall: 0.9164 - val_fn: 340.0000 - lr: 1.0000e-04\n",
      "Epoch 18/20\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 0.1738 - precision: 0.9362 - recall: 0.9214 - fn: 2875.0000 - val_loss: 0.2121 - val_precision: 0.9299 - val_recall: 0.9142 - val_fn: 349.0000 - lr: 1.0000e-04\n",
      "Epoch 19/20\n",
      "143/143 [==============================] - 5s 35ms/step - loss: 0.1725 - precision: 0.9381 - recall: 0.9234 - fn: 2803.0000 - val_loss: 0.1937 - val_precision: 0.9361 - val_recall: 0.9220 - val_fn: 317.0000 - lr: 1.0000e-04\n",
      "Epoch 20/20\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 0.1700 - precision: 0.9380 - recall: 0.9245 - fn: 2762.0000 - val_loss: 0.1881 - val_precision: 0.9334 - val_recall: 0.9206 - val_fn: 323.0000 - lr: 1.0000e-04\n",
      "Saving model at ./Models/Bagging/bagging_model_1.keras\n",
      "Building bootstrap 2\n",
      "X_train.shape (40653, 128, 2) y_ohe_train.shape (40653, 3) features_test.shape (40653, 8)\n",
      "Distribution training set: [[0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]] [ 5281  4416 30956]\n",
      "Epoch 1/20\n",
      "143/143 [==============================] - 18s 36ms/step - loss: 0.2861 - precision: 0.9030 - recall: 0.8508 - fn: 5458.0000 - val_loss: 0.3761 - val_precision: 0.8826 - val_recall: 0.8283 - val_fn: 698.0000 - lr: 1.0000e-04\n",
      "Epoch 2/20\n",
      "143/143 [==============================] - 5s 33ms/step - loss: 0.2421 - precision: 0.9104 - recall: 0.8811 - fn: 4352.0000 - val_loss: 0.3500 - val_precision: 0.8836 - val_recall: 0.8436 - val_fn: 636.0000 - lr: 1.0000e-04\n",
      "Epoch 3/20\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 0.2320 - precision: 0.9152 - recall: 0.8902 - fn: 4019.0000 - val_loss: 0.3628 - val_precision: 0.8746 - val_recall: 0.8251 - val_fn: 711.0000 - lr: 1.0000e-04\n",
      "Epoch 4/20\n",
      "143/143 [==============================] - 4s 31ms/step - loss: 0.2243 - precision: 0.9192 - recall: 0.8959 - fn: 3808.0000 - val_loss: 0.2756 - val_precision: 0.9170 - val_recall: 0.8640 - val_fn: 553.0000 - lr: 1.0000e-04\n",
      "Epoch 5/20\n",
      "143/143 [==============================] - 5s 33ms/step - loss: 0.2168 - precision: 0.9218 - recall: 0.9014 - fn: 3606.0000 - val_loss: 0.2369 - val_precision: 0.9112 - val_recall: 0.8883 - val_fn: 454.0000 - lr: 1.0000e-04\n",
      "Epoch 6/20\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 0.2078 - precision: 0.9257 - recall: 0.9072 - fn: 3397.0000 - val_loss: 0.2071 - val_precision: 0.9247 - val_recall: 0.9097 - val_fn: 367.0000 - lr: 1.0000e-04\n",
      "Epoch 7/20\n",
      "143/143 [==============================] - 4s 31ms/step - loss: 0.2041 - precision: 0.9271 - recall: 0.9096 - fn: 3307.0000 - val_loss: 0.1973 - val_precision: 0.9266 - val_recall: 0.9164 - val_fn: 340.0000 - lr: 1.0000e-04\n",
      "Epoch 8/20\n",
      "143/143 [==============================] - 5s 32ms/step - loss: 0.2003 - precision: 0.9285 - recall: 0.9133 - fn: 3173.0000 - val_loss: 0.1978 - val_precision: 0.9334 - val_recall: 0.9208 - val_fn: 322.0000 - lr: 1.0000e-04\n",
      "Epoch 9/20\n",
      "143/143 [==============================] - 5s 33ms/step - loss: 0.1953 - precision: 0.9300 - recall: 0.9149 - fn: 3114.0000 - val_loss: 0.2022 - val_precision: 0.9308 - val_recall: 0.9169 - val_fn: 338.0000 - lr: 1.0000e-04\n",
      "Epoch 10/20\n",
      "143/143 [==============================] - 5s 37ms/step - loss: 0.1917 - precision: 0.9327 - recall: 0.9180 - fn: 3000.0000 - val_loss: 0.1933 - val_precision: 0.9342 - val_recall: 0.9186 - val_fn: 331.0000 - lr: 1.0000e-04\n",
      "Epoch 11/20\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 0.1860 - precision: 0.9341 - recall: 0.9204 - fn: 2912.0000 - val_loss: 0.1821 - val_precision: 0.9342 - val_recall: 0.9215 - val_fn: 319.0000 - lr: 1.0000e-04\n",
      "Epoch 12/20\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 0.1814 - precision: 0.9361 - recall: 0.9237 - fn: 2791.0000 - val_loss: 0.1899 - val_precision: 0.9334 - val_recall: 0.9203 - val_fn: 324.0000 - lr: 1.0000e-04\n",
      "Epoch 13/20\n",
      "143/143 [==============================] - 5s 35ms/step - loss: 0.1806 - precision: 0.9344 - recall: 0.9227 - fn: 2828.0000 - val_loss: 0.1765 - val_precision: 0.9412 - val_recall: 0.9299 - val_fn: 285.0000 - lr: 1.0000e-04\n",
      "Epoch 14/20\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 0.1759 - precision: 0.9373 - recall: 0.9248 - fn: 2751.0000 - val_loss: 0.1891 - val_precision: 0.9322 - val_recall: 0.9230 - val_fn: 313.0000 - lr: 1.0000e-04\n",
      "Epoch 15/20\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 0.1743 - precision: 0.9380 - recall: 0.9269 - fn: 2675.0000 - val_loss: 0.1839 - val_precision: 0.9356 - val_recall: 0.9181 - val_fn: 333.0000 - lr: 1.0000e-04\n",
      "Epoch 16/20\n",
      "143/143 [==============================] - 5s 34ms/step - loss: 0.1712 - precision: 0.9395 - recall: 0.9281 - fn: 2630.0000 - val_loss: 0.1731 - val_precision: 0.9403 - val_recall: 0.9294 - val_fn: 287.0000 - lr: 1.0000e-04\n",
      "Epoch 17/20\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 0.1667 - precision: 0.9405 - recall: 0.9299 - fn: 2563.0000 - val_loss: 0.1762 - val_precision: 0.9395 - val_recall: 0.9284 - val_fn: 291.0000 - lr: 1.0000e-04\n",
      "Epoch 18/20\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 0.1631 - precision: 0.9430 - recall: 0.9324 - fn: 2472.0000 - val_loss: 0.1670 - val_precision: 0.9433 - val_recall: 0.9333 - val_fn: 271.0000 - lr: 1.0000e-04\n",
      "Epoch 19/20\n",
      "143/143 [==============================] - 5s 35ms/step - loss: 0.1597 - precision: 0.9429 - recall: 0.9331 - fn: 2446.0000 - val_loss: 0.1644 - val_precision: 0.9452 - val_recall: 0.9368 - val_fn: 257.0000 - lr: 1.0000e-04\n",
      "Epoch 20/20\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 0.1576 - precision: 0.9452 - recall: 0.9350 - fn: 2377.0000 - val_loss: 0.1747 - val_precision: 0.9406 - val_recall: 0.9304 - val_fn: 283.0000 - lr: 1.0000e-04\n",
      "Saving model at ./Models/Bagging/bagging_model_2.keras\n",
      "Building bootstrap 3\n",
      "X_train.shape (40653, 128, 2) y_ohe_train.shape (40653, 3) features_test.shape (40653, 8)\n",
      "Distribution training set: [[0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]] [ 5281  4416 30956]\n",
      "Epoch 1/20\n",
      "143/143 [==============================] - 18s 39ms/step - loss: 0.2816 - precision: 0.9044 - recall: 0.8583 - fn: 5184.0000 - val_loss: 0.5414 - val_precision: 0.9625 - val_recall: 0.7378 - val_fn: 1066.0000 - lr: 1.0000e-04\n",
      "Epoch 2/20\n",
      "143/143 [==============================] - 5s 36ms/step - loss: 0.2404 - precision: 0.9138 - recall: 0.8867 - fn: 4144.0000 - val_loss: 0.3871 - val_precision: 0.9055 - val_recall: 0.7804 - val_fn: 893.0000 - lr: 1.0000e-04\n",
      "Epoch 3/20\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 0.2291 - precision: 0.9163 - recall: 0.8914 - fn: 3975.0000 - val_loss: 0.3261 - val_precision: 0.8890 - val_recall: 0.8333 - val_fn: 678.0000 - lr: 1.0000e-04\n",
      "Epoch 4/20\n",
      "143/143 [==============================] - 5s 34ms/step - loss: 0.2218 - precision: 0.9209 - recall: 0.8982 - fn: 3723.0000 - val_loss: 0.2668 - val_precision: 0.9119 - val_recall: 0.8832 - val_fn: 475.0000 - lr: 1.0000e-04\n",
      "Epoch 5/20\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 0.2170 - precision: 0.9218 - recall: 0.9022 - fn: 3578.0000 - val_loss: 0.2192 - val_precision: 0.9276 - val_recall: 0.8952 - val_fn: 426.0000 - lr: 1.0000e-04\n",
      "Epoch 6/20\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 0.2120 - precision: 0.9238 - recall: 0.9036 - fn: 3526.0000 - val_loss: 0.2161 - val_precision: 0.9243 - val_recall: 0.8984 - val_fn: 413.0000 - lr: 1.0000e-04\n",
      "Epoch 7/20\n",
      "143/143 [==============================] - 5s 34ms/step - loss: 0.2059 - precision: 0.9270 - recall: 0.9088 - fn: 3335.0000 - val_loss: 0.2096 - val_precision: 0.9248 - val_recall: 0.9038 - val_fn: 391.0000 - lr: 1.0000e-04\n",
      "Epoch 8/20\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 0.2028 - precision: 0.9284 - recall: 0.9093 - fn: 3318.0000 - val_loss: 0.2127 - val_precision: 0.9216 - val_recall: 0.9051 - val_fn: 386.0000 - lr: 1.0000e-04\n",
      "Epoch 9/20\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 0.2011 - precision: 0.9294 - recall: 0.9112 - fn: 3249.0000 - val_loss: 0.2052 - val_precision: 0.9222 - val_recall: 0.9095 - val_fn: 368.0000 - lr: 1.0000e-04\n",
      "Epoch 10/20\n",
      "143/143 [==============================] - 5s 34ms/step - loss: 0.1966 - precision: 0.9294 - recall: 0.9135 - fn: 3166.0000 - val_loss: 0.2078 - val_precision: 0.9240 - val_recall: 0.9120 - val_fn: 358.0000 - lr: 1.0000e-04\n",
      "Epoch 11/20\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 0.1934 - precision: 0.9319 - recall: 0.9154 - fn: 3097.0000 - val_loss: 0.2080 - val_precision: 0.9230 - val_recall: 0.9078 - val_fn: 375.0000 - lr: 1.0000e-04\n",
      "Epoch 12/20\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 0.1897 - precision: 0.9325 - recall: 0.9161 - fn: 3068.0000 - val_loss: 0.2038 - val_precision: 0.9209 - val_recall: 0.9105 - val_fn: 364.0000 - lr: 1.0000e-04\n",
      "Epoch 13/20\n",
      "143/143 [==============================] - 5s 35ms/step - loss: 0.1836 - precision: 0.9344 - recall: 0.9202 - fn: 2921.0000 - val_loss: 0.1991 - val_precision: 0.9293 - val_recall: 0.9154 - val_fn: 344.0000 - lr: 1.0000e-04\n",
      "Epoch 14/20\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 0.1797 - precision: 0.9373 - recall: 0.9234 - fn: 2802.0000 - val_loss: 0.1973 - val_precision: 0.9290 - val_recall: 0.9169 - val_fn: 338.0000 - lr: 1.0000e-04\n",
      "Epoch 15/20\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 0.1781 - precision: 0.9367 - recall: 0.9243 - fn: 2768.0000 - val_loss: 0.1887 - val_precision: 0.9369 - val_recall: 0.9240 - val_fn: 309.0000 - lr: 1.0000e-04\n",
      "Epoch 16/20\n",
      "143/143 [==============================] - 5s 35ms/step - loss: 0.1744 - precision: 0.9375 - recall: 0.9252 - fn: 2736.0000 - val_loss: 0.1982 - val_precision: 0.9276 - val_recall: 0.9142 - val_fn: 349.0000 - lr: 1.0000e-04\n",
      "Epoch 17/20\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 0.1708 - precision: 0.9397 - recall: 0.9281 - fn: 2629.0000 - val_loss: 0.1942 - val_precision: 0.9296 - val_recall: 0.9120 - val_fn: 358.0000 - lr: 1.0000e-04\n",
      "Epoch 18/20\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 0.1674 - precision: 0.9412 - recall: 0.9297 - fn: 2571.0000 - val_loss: 0.1842 - val_precision: 0.9350 - val_recall: 0.9238 - val_fn: 310.0000 - lr: 1.0000e-04\n",
      "Epoch 19/20\n",
      "143/143 [==============================] - 5s 34ms/step - loss: 0.1626 - precision: 0.9418 - recall: 0.9308 - fn: 2531.0000 - val_loss: 0.1771 - val_precision: 0.9337 - val_recall: 0.9255 - val_fn: 303.0000 - lr: 1.0000e-04\n",
      "Epoch 20/20\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 0.1613 - precision: 0.9425 - recall: 0.9317 - fn: 2500.0000 - val_loss: 0.1856 - val_precision: 0.9354 - val_recall: 0.9220 - val_fn: 317.0000 - lr: 1.0000e-04\n",
      "Saving model at ./Models/Bagging/bagging_model_3.keras\n",
      "Building bootstrap 4\n",
      "X_train.shape (40653, 128, 2) y_ohe_train.shape (40653, 3) features_test.shape (40653, 8)\n",
      "Distribution training set: [[0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]] [ 5281  4416 30956]\n",
      "Epoch 1/20\n",
      "143/143 [==============================] - 19s 47ms/step - loss: 0.3384 - precision: 0.9247 - recall: 0.7863 - fn: 7820.0000 - val_loss: 0.3430 - val_precision: 0.8794 - val_recall: 0.8428 - val_fn: 639.0000 - lr: 1.0000e-04\n",
      "Epoch 2/20\n",
      "143/143 [==============================] - 4s 31ms/step - loss: 0.2407 - precision: 0.9133 - recall: 0.8799 - fn: 4393.0000 - val_loss: 0.2975 - val_precision: 0.8971 - val_recall: 0.8596 - val_fn: 571.0000 - lr: 1.0000e-04\n",
      "Epoch 3/20\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 0.2298 - precision: 0.9210 - recall: 0.8862 - fn: 4165.0000 - val_loss: 0.2754 - val_precision: 0.9104 - val_recall: 0.8768 - val_fn: 501.0000 - lr: 1.0000e-04\n",
      "Epoch 4/20\n",
      "143/143 [==============================] - 5s 35ms/step - loss: 0.2211 - precision: 0.9237 - recall: 0.8934 - fn: 3899.0000 - val_loss: 0.2275 - val_precision: 0.9163 - val_recall: 0.8881 - val_fn: 455.0000 - lr: 1.0000e-04\n",
      "Epoch 5/20\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 0.2155 - precision: 0.9250 - recall: 0.8987 - fn: 3705.0000 - val_loss: 0.2225 - val_precision: 0.9115 - val_recall: 0.8920 - val_fn: 439.0000 - lr: 1.0000e-04\n",
      "Epoch 6/20\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 0.2074 - precision: 0.9275 - recall: 0.9046 - fn: 3491.0000 - val_loss: 0.2052 - val_precision: 0.9227 - val_recall: 0.9070 - val_fn: 378.0000 - lr: 1.0000e-04\n",
      "Epoch 7/20\n",
      "143/143 [==============================] - 5s 35ms/step - loss: 0.2044 - precision: 0.9274 - recall: 0.9070 - fn: 3403.0000 - val_loss: 0.2213 - val_precision: 0.9114 - val_recall: 0.8933 - val_fn: 434.0000 - lr: 1.0000e-04\n",
      "Epoch 8/20\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 0.2019 - precision: 0.9296 - recall: 0.9094 - fn: 3316.0000 - val_loss: 0.1970 - val_precision: 0.9308 - val_recall: 0.9095 - val_fn: 368.0000 - lr: 1.0000e-04\n",
      "Epoch 9/20\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 0.1966 - precision: 0.9306 - recall: 0.9110 - fn: 3258.0000 - val_loss: 0.2008 - val_precision: 0.9279 - val_recall: 0.9088 - val_fn: 371.0000 - lr: 1.0000e-04\n",
      "Epoch 10/20\n",
      "143/143 [==============================] - 5s 35ms/step - loss: 0.1906 - precision: 0.9317 - recall: 0.9147 - fn: 3122.0000 - val_loss: 0.1874 - val_precision: 0.9334 - val_recall: 0.9174 - val_fn: 336.0000 - lr: 1.0000e-04\n",
      "Epoch 11/20\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 0.1881 - precision: 0.9329 - recall: 0.9172 - fn: 3031.0000 - val_loss: 0.1910 - val_precision: 0.9300 - val_recall: 0.9152 - val_fn: 345.0000 - lr: 1.0000e-04\n",
      "Epoch 12/20\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 0.1850 - precision: 0.9342 - recall: 0.9184 - fn: 2984.0000 - val_loss: 0.1831 - val_precision: 0.9347 - val_recall: 0.9193 - val_fn: 328.0000 - lr: 1.0000e-04\n",
      "Epoch 13/20\n",
      "143/143 [==============================] - 5s 35ms/step - loss: 0.1828 - precision: 0.9356 - recall: 0.9201 - fn: 2925.0000 - val_loss: 0.1808 - val_precision: 0.9372 - val_recall: 0.9179 - val_fn: 334.0000 - lr: 1.0000e-04\n",
      "Epoch 14/20\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 0.1761 - precision: 0.9378 - recall: 0.9230 - fn: 2819.0000 - val_loss: 0.1869 - val_precision: 0.9355 - val_recall: 0.9233 - val_fn: 312.0000 - lr: 1.0000e-04\n",
      "Epoch 15/20\n",
      "143/143 [==============================] - 4s 31ms/step - loss: 0.1734 - precision: 0.9383 - recall: 0.9247 - fn: 2755.0000 - val_loss: 0.1758 - val_precision: 0.9356 - val_recall: 0.9218 - val_fn: 318.0000 - lr: 1.0000e-04\n",
      "Epoch 16/20\n",
      "143/143 [==============================] - 5s 34ms/step - loss: 0.1705 - precision: 0.9392 - recall: 0.9269 - fn: 2676.0000 - val_loss: 0.1814 - val_precision: 0.9335 - val_recall: 0.9250 - val_fn: 305.0000 - lr: 1.0000e-04\n",
      "Epoch 17/20\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 0.1655 - precision: 0.9419 - recall: 0.9312 - fn: 2516.0000 - val_loss: 0.1886 - val_precision: 0.9269 - val_recall: 0.9169 - val_fn: 338.0000 - lr: 1.0000e-04\n",
      "Epoch 18/20\n",
      "143/143 [==============================] - 4s 31ms/step - loss: 0.1634 - precision: 0.9428 - recall: 0.9303 - fn: 2549.0000 - val_loss: 0.1848 - val_precision: 0.9347 - val_recall: 0.9223 - val_fn: 316.0000 - lr: 1.0000e-04\n",
      "Epoch 19/20\n",
      "143/143 [==============================] - 5s 33ms/step - loss: 0.1589 - precision: 0.9437 - recall: 0.9333 - fn: 2440.0000 - val_loss: 0.1755 - val_precision: 0.9375 - val_recall: 0.9267 - val_fn: 298.0000 - lr: 1.0000e-04\n",
      "Epoch 20/20\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 0.1550 - precision: 0.9455 - recall: 0.9355 - fn: 2361.0000 - val_loss: 0.1749 - val_precision: 0.9361 - val_recall: 0.9260 - val_fn: 301.0000 - lr: 1.0000e-04\n",
      "Saving model at ./Models/Bagging/bagging_model_4.keras\n",
      "Building bootstrap 5\n",
      "X_train.shape (40653, 128, 2) y_ohe_train.shape (40653, 3) features_test.shape (40653, 8)\n",
      "Distribution training set: [[0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]] [ 5281  4416 30956]\n",
      "Epoch 1/20\n",
      "143/143 [==============================] - 20s 41ms/step - loss: 0.2787 - precision: 0.9079 - recall: 0.8573 - fn: 5221.0000 - val_loss: 0.7809 - val_precision: 0.9884 - val_recall: 0.4196 - val_fn: 2360.0000 - lr: 1.0000e-04\n",
      "Epoch 2/20\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 0.2374 - precision: 0.9164 - recall: 0.8848 - fn: 4215.0000 - val_loss: 0.4792 - val_precision: 0.9347 - val_recall: 0.7818 - val_fn: 887.0000 - lr: 1.0000e-04\n",
      "Epoch 3/20\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 0.2319 - precision: 0.9172 - recall: 0.8850 - fn: 4208.0000 - val_loss: 0.3455 - val_precision: 0.9007 - val_recall: 0.8389 - val_fn: 655.0000 - lr: 1.0000e-04\n",
      "Epoch 4/20\n",
      "143/143 [==============================] - 5s 36ms/step - loss: 0.2197 - precision: 0.9217 - recall: 0.8964 - fn: 3789.0000 - val_loss: 0.2582 - val_precision: 0.9109 - val_recall: 0.8701 - val_fn: 528.0000 - lr: 1.0000e-04\n",
      "Epoch 5/20\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 0.2144 - precision: 0.9238 - recall: 0.8990 - fn: 3696.0000 - val_loss: 0.2200 - val_precision: 0.9132 - val_recall: 0.8950 - val_fn: 427.0000 - lr: 1.0000e-04\n",
      "Epoch 6/20\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 0.2102 - precision: 0.9252 - recall: 0.9009 - fn: 3626.0000 - val_loss: 0.1970 - val_precision: 0.9258 - val_recall: 0.9088 - val_fn: 371.0000 - lr: 1.0000e-04\n",
      "Epoch 7/20\n",
      "143/143 [==============================] - 5s 36ms/step - loss: 0.2043 - precision: 0.9282 - recall: 0.9069 - fn: 3407.0000 - val_loss: 0.2052 - val_precision: 0.9286 - val_recall: 0.9120 - val_fn: 358.0000 - lr: 1.0000e-04\n",
      "Epoch 8/20\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 0.2025 - precision: 0.9286 - recall: 0.9084 - fn: 3351.0000 - val_loss: 0.2022 - val_precision: 0.9267 - val_recall: 0.9112 - val_fn: 361.0000 - lr: 1.0000e-04\n",
      "Epoch 9/20\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 0.1990 - precision: 0.9301 - recall: 0.9105 - fn: 3276.0000 - val_loss: 0.1977 - val_precision: 0.9275 - val_recall: 0.9122 - val_fn: 357.0000 - lr: 1.0000e-04\n",
      "Epoch 10/20\n",
      "143/143 [==============================] - 5s 36ms/step - loss: 0.1973 - precision: 0.9306 - recall: 0.9099 - fn: 3295.0000 - val_loss: 0.1884 - val_precision: 0.9331 - val_recall: 0.9166 - val_fn: 339.0000 - lr: 1.0000e-04\n",
      "Epoch 11/20\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 0.1904 - precision: 0.9320 - recall: 0.9147 - fn: 3122.0000 - val_loss: 0.1861 - val_precision: 0.9355 - val_recall: 0.9211 - val_fn: 321.0000 - lr: 1.0000e-04\n",
      "Epoch 12/20\n",
      "143/143 [==============================] - 4s 31ms/step - loss: 0.1888 - precision: 0.9325 - recall: 0.9158 - fn: 3081.0000 - val_loss: 0.1860 - val_precision: 0.9345 - val_recall: 0.9228 - val_fn: 314.0000 - lr: 1.0000e-04\n",
      "Epoch 13/20\n",
      "143/143 [==============================] - 5s 34ms/step - loss: 0.1838 - precision: 0.9346 - recall: 0.9189 - fn: 2966.0000 - val_loss: 0.1900 - val_precision: 0.9302 - val_recall: 0.9152 - val_fn: 345.0000 - lr: 1.0000e-04\n",
      "Epoch 14/20\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 0.1821 - precision: 0.9346 - recall: 0.9195 - fn: 2947.0000 - val_loss: 0.1857 - val_precision: 0.9357 - val_recall: 0.9198 - val_fn: 326.0000 - lr: 1.0000e-04\n",
      "Epoch 15/20\n",
      "143/143 [==============================] - 5s 33ms/step - loss: 0.1776 - precision: 0.9357 - recall: 0.9208 - fn: 2897.0000 - val_loss: 0.1763 - val_precision: 0.9395 - val_recall: 0.9279 - val_fn: 293.0000 - lr: 1.0000e-04\n",
      "Epoch 16/20\n",
      "143/143 [==============================] - 5s 33ms/step - loss: 0.1735 - precision: 0.9379 - recall: 0.9233 - fn: 2808.0000 - val_loss: 0.1712 - val_precision: 0.9436 - val_recall: 0.9306 - val_fn: 282.0000 - lr: 1.0000e-04\n",
      "Epoch 17/20\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 0.1730 - precision: 0.9380 - recall: 0.9241 - fn: 2777.0000 - val_loss: 0.1760 - val_precision: 0.9411 - val_recall: 0.9282 - val_fn: 292.0000 - lr: 1.0000e-04\n",
      "Epoch 18/20\n",
      "143/143 [==============================] - 5s 33ms/step - loss: 0.1687 - precision: 0.9400 - recall: 0.9274 - fn: 2657.0000 - val_loss: 0.1732 - val_precision: 0.9417 - val_recall: 0.9294 - val_fn: 287.0000 - lr: 1.0000e-04\n",
      "Epoch 19/20\n",
      "143/143 [==============================] - 5s 32ms/step - loss: 0.1642 - precision: 0.9422 - recall: 0.9293 - fn: 2585.0000 - val_loss: 0.1679 - val_precision: 0.9458 - val_recall: 0.9316 - val_fn: 278.0000 - lr: 1.0000e-04\n",
      "Epoch 20/20\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 0.1628 - precision: 0.9417 - recall: 0.9300 - fn: 2561.0000 - val_loss: 0.1726 - val_precision: 0.9376 - val_recall: 0.9282 - val_fn: 292.0000 - lr: 1.0000e-04\n",
      "Saving model at ./Models/Bagging/bagging_model_5.keras\n",
      "Building bootstrap 6\n",
      "X_train.shape (40653, 128, 2) y_ohe_train.shape (40653, 3) features_test.shape (40653, 8)\n",
      "Distribution training set: [[0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]] [ 5281  4416 30956]\n",
      "Epoch 1/20\n",
      "143/143 [==============================] - 18s 42ms/step - loss: 0.3039 - precision: 0.9019 - recall: 0.8439 - fn: 5712.0000 - val_loss: 0.4774 - val_precision: 0.9111 - val_recall: 0.7740 - val_fn: 919.0000 - lr: 1.0000e-04\n",
      "Epoch 2/20\n",
      "143/143 [==============================] - 4s 31ms/step - loss: 0.2430 - precision: 0.9107 - recall: 0.8814 - fn: 4339.0000 - val_loss: 0.3603 - val_precision: 0.8914 - val_recall: 0.7993 - val_fn: 816.0000 - lr: 1.0000e-04\n",
      "Epoch 3/20\n",
      "143/143 [==============================] - 4s 31ms/step - loss: 0.2341 - precision: 0.9162 - recall: 0.8882 - fn: 4092.0000 - val_loss: 0.2937 - val_precision: 0.8922 - val_recall: 0.8532 - val_fn: 597.0000 - lr: 1.0000e-04\n",
      "Epoch 4/20\n",
      "143/143 [==============================] - 5s 35ms/step - loss: 0.2244 - precision: 0.9180 - recall: 0.8952 - fn: 3835.0000 - val_loss: 0.3081 - val_precision: 0.8871 - val_recall: 0.8660 - val_fn: 545.0000 - lr: 1.0000e-04\n",
      "Epoch 5/20\n",
      "143/143 [==============================] - 4s 31ms/step - loss: 0.2170 - precision: 0.9224 - recall: 0.9008 - fn: 3629.0000 - val_loss: 0.2252 - val_precision: 0.9166 - val_recall: 0.8977 - val_fn: 416.0000 - lr: 1.0000e-04\n",
      "Epoch 6/20\n",
      "143/143 [==============================] - 5s 32ms/step - loss: 0.2142 - precision: 0.9226 - recall: 0.9018 - fn: 3593.0000 - val_loss: 0.2094 - val_precision: 0.9285 - val_recall: 0.9134 - val_fn: 352.0000 - lr: 1.0000e-04\n",
      "Epoch 7/20\n",
      "143/143 [==============================] - 5s 33ms/step - loss: 0.2092 - precision: 0.9237 - recall: 0.9045 - fn: 3494.0000 - val_loss: 0.2040 - val_precision: 0.9318 - val_recall: 0.9142 - val_fn: 349.0000 - lr: 1.0000e-04\n",
      "Epoch 8/20\n",
      "143/143 [==============================] - 4s 31ms/step - loss: 0.2038 - precision: 0.9272 - recall: 0.9084 - fn: 3351.0000 - val_loss: 0.2024 - val_precision: 0.9310 - val_recall: 0.9154 - val_fn: 344.0000 - lr: 1.0000e-04\n",
      "Epoch 9/20\n",
      "143/143 [==============================] - 5s 34ms/step - loss: 0.2000 - precision: 0.9280 - recall: 0.9106 - fn: 3272.0000 - val_loss: 0.1991 - val_precision: 0.9298 - val_recall: 0.9147 - val_fn: 347.0000 - lr: 1.0000e-04\n",
      "Epoch 10/20\n",
      "143/143 [==============================] - 5s 35ms/step - loss: 0.1954 - precision: 0.9302 - recall: 0.9127 - fn: 3193.0000 - val_loss: 0.2041 - val_precision: 0.9287 - val_recall: 0.9161 - val_fn: 341.0000 - lr: 1.0000e-04\n",
      "Epoch 11/20\n",
      "143/143 [==============================] - 6s 40ms/step - loss: 0.1893 - precision: 0.9315 - recall: 0.9163 - fn: 3061.0000 - val_loss: 0.1936 - val_precision: 0.9326 - val_recall: 0.9218 - val_fn: 318.0000 - lr: 1.0000e-04\n",
      "Epoch 12/20\n",
      "143/143 [==============================] - 5s 35ms/step - loss: 0.1848 - precision: 0.9336 - recall: 0.9178 - fn: 3009.0000 - val_loss: 0.1911 - val_precision: 0.9397 - val_recall: 0.9235 - val_fn: 311.0000 - lr: 1.0000e-04\n",
      "Epoch 13/20\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 0.1824 - precision: 0.9345 - recall: 0.9204 - fn: 2914.0000 - val_loss: 0.2004 - val_precision: 0.9328 - val_recall: 0.9215 - val_fn: 319.0000 - lr: 1.0000e-04\n",
      "Epoch 14/20\n",
      "143/143 [==============================] - 5s 32ms/step - loss: 0.1771 - precision: 0.9362 - recall: 0.9235 - fn: 2800.0000 - val_loss: 0.1816 - val_precision: 0.9356 - val_recall: 0.9252 - val_fn: 304.0000 - lr: 1.0000e-04\n",
      "Epoch 15/20\n",
      "143/143 [==============================] - 5s 33ms/step - loss: 0.1731 - precision: 0.9381 - recall: 0.9261 - fn: 2704.0000 - val_loss: 0.1856 - val_precision: 0.9370 - val_recall: 0.9294 - val_fn: 287.0000 - lr: 1.0000e-04\n",
      "Epoch 16/20\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 0.1688 - precision: 0.9386 - recall: 0.9277 - fn: 2646.0000 - val_loss: 0.1798 - val_precision: 0.9387 - val_recall: 0.9304 - val_fn: 283.0000 - lr: 1.0000e-04\n",
      "Epoch 17/20\n",
      "143/143 [==============================] - 5s 33ms/step - loss: 0.1646 - precision: 0.9414 - recall: 0.9307 - fn: 2537.0000 - val_loss: 0.1735 - val_precision: 0.9423 - val_recall: 0.9326 - val_fn: 274.0000 - lr: 1.0000e-04\n",
      "Epoch 18/20\n",
      "143/143 [==============================] - 5s 33ms/step - loss: 0.1608 - precision: 0.9418 - recall: 0.9326 - fn: 2465.0000 - val_loss: 0.1758 - val_precision: 0.9428 - val_recall: 0.9361 - val_fn: 260.0000 - lr: 1.0000e-04\n",
      "Epoch 19/20\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 0.1586 - precision: 0.9431 - recall: 0.9333 - fn: 2441.0000 - val_loss: 0.1823 - val_precision: 0.9400 - val_recall: 0.9292 - val_fn: 288.0000 - lr: 1.0000e-04\n",
      "Epoch 20/20\n",
      "143/143 [==============================] - 5s 34ms/step - loss: 0.1561 - precision: 0.9438 - recall: 0.9344 - fn: 2399.0000 - val_loss: 0.1788 - val_precision: 0.9385 - val_recall: 0.9338 - val_fn: 269.0000 - lr: 1.0000e-04\n",
      "Saving model at ./Models/Bagging/bagging_model_6.keras\n",
      "Building bootstrap 7\n",
      "X_train.shape (40653, 128, 2) y_ohe_train.shape (40653, 3) features_test.shape (40653, 8)\n",
      "Distribution training set: [[0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]] [ 5281  4416 30956]\n",
      "Epoch 1/20\n",
      "143/143 [==============================] - 18s 36ms/step - loss: 0.3984 - precision: 0.9343 - recall: 0.7271 - fn: 9986.0000 - val_loss: 0.3468 - val_precision: 0.8764 - val_recall: 0.8598 - val_fn: 570.0000 - lr: 1.0000e-04\n",
      "Epoch 2/20\n",
      "143/143 [==============================] - 5s 32ms/step - loss: 0.2506 - precision: 0.9043 - recall: 0.8755 - fn: 4555.0000 - val_loss: 0.2371 - val_precision: 0.9047 - val_recall: 0.8896 - val_fn: 449.0000 - lr: 1.0000e-04\n",
      "Epoch 3/20\n",
      "143/143 [==============================] - 5s 34ms/step - loss: 0.2363 - precision: 0.9134 - recall: 0.8865 - fn: 4151.0000 - val_loss: 0.2284 - val_precision: 0.9161 - val_recall: 0.8886 - val_fn: 453.0000 - lr: 1.0000e-04\n",
      "Epoch 4/20\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 0.2267 - precision: 0.9207 - recall: 0.8911 - fn: 3984.0000 - val_loss: 0.2178 - val_precision: 0.9201 - val_recall: 0.8974 - val_fn: 417.0000 - lr: 1.0000e-04\n",
      "Epoch 5/20\n",
      "143/143 [==============================] - 5s 32ms/step - loss: 0.2189 - precision: 0.9237 - recall: 0.8964 - fn: 3790.0000 - val_loss: 0.1968 - val_precision: 0.9299 - val_recall: 0.9070 - val_fn: 378.0000 - lr: 1.0000e-04\n",
      "Epoch 6/20\n",
      "143/143 [==============================] - 5s 34ms/step - loss: 0.2113 - precision: 0.9278 - recall: 0.8999 - fn: 3661.0000 - val_loss: 0.1869 - val_precision: 0.9344 - val_recall: 0.9080 - val_fn: 374.0000 - lr: 1.0000e-04\n",
      "Epoch 7/20\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 0.2055 - precision: 0.9285 - recall: 0.9044 - fn: 3497.0000 - val_loss: 0.1902 - val_precision: 0.9313 - val_recall: 0.9129 - val_fn: 354.0000 - lr: 1.0000e-04\n",
      "Epoch 8/20\n",
      "143/143 [==============================] - 5s 35ms/step - loss: 0.2025 - precision: 0.9305 - recall: 0.9056 - fn: 3454.0000 - val_loss: 0.1815 - val_precision: 0.9380 - val_recall: 0.9152 - val_fn: 345.0000 - lr: 1.0000e-04\n",
      "Epoch 9/20\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 0.1975 - precision: 0.9315 - recall: 0.9083 - fn: 3356.0000 - val_loss: 0.1808 - val_precision: 0.9322 - val_recall: 0.9127 - val_fn: 355.0000 - lr: 1.0000e-04\n",
      "Epoch 10/20\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 0.1927 - precision: 0.9334 - recall: 0.9126 - fn: 3198.0000 - val_loss: 0.1748 - val_precision: 0.9390 - val_recall: 0.9161 - val_fn: 341.0000 - lr: 1.0000e-04\n",
      "Epoch 11/20\n",
      "143/143 [==============================] - 5s 36ms/step - loss: 0.1884 - precision: 0.9355 - recall: 0.9160 - fn: 3074.0000 - val_loss: 0.1726 - val_precision: 0.9397 - val_recall: 0.9198 - val_fn: 326.0000 - lr: 1.0000e-04\n",
      "Epoch 12/20\n",
      "143/143 [==============================] - 4s 31ms/step - loss: 0.1862 - precision: 0.9363 - recall: 0.9179 - fn: 3005.0000 - val_loss: 0.1649 - val_precision: 0.9376 - val_recall: 0.9235 - val_fn: 311.0000 - lr: 1.0000e-04\n",
      "Epoch 13/20\n",
      "143/143 [==============================] - 4s 31ms/step - loss: 0.1810 - precision: 0.9376 - recall: 0.9201 - fn: 2924.0000 - val_loss: 0.1712 - val_precision: 0.9398 - val_recall: 0.9250 - val_fn: 305.0000 - lr: 1.0000e-04\n",
      "Epoch 14/20\n",
      "143/143 [==============================] - 5s 36ms/step - loss: 0.1786 - precision: 0.9379 - recall: 0.9203 - fn: 2915.0000 - val_loss: 0.1657 - val_precision: 0.9362 - val_recall: 0.9206 - val_fn: 323.0000 - lr: 1.0000e-04\n",
      "Epoch 15/20\n",
      "143/143 [==============================] - 4s 31ms/step - loss: 0.1769 - precision: 0.9374 - recall: 0.9219 - fn: 2859.0000 - val_loss: 0.1599 - val_precision: 0.9420 - val_recall: 0.9262 - val_fn: 300.0000 - lr: 1.0000e-04\n",
      "Epoch 16/20\n",
      "143/143 [==============================] - 4s 31ms/step - loss: 0.1723 - precision: 0.9408 - recall: 0.9248 - fn: 2751.0000 - val_loss: 0.1603 - val_precision: 0.9387 - val_recall: 0.9270 - val_fn: 297.0000 - lr: 1.0000e-04\n",
      "Epoch 17/20\n",
      "143/143 [==============================] - 5s 36ms/step - loss: 0.1692 - precision: 0.9414 - recall: 0.9265 - fn: 2689.0000 - val_loss: 0.1701 - val_precision: 0.9405 - val_recall: 0.9260 - val_fn: 301.0000 - lr: 1.0000e-04\n",
      "Epoch 18/20\n",
      "143/143 [==============================] - 5s 34ms/step - loss: 0.1674 - precision: 0.9425 - recall: 0.9275 - fn: 2652.0000 - val_loss: 0.1633 - val_precision: 0.9393 - val_recall: 0.9255 - val_fn: 303.0000 - lr: 1.0000e-04\n",
      "Epoch 19/20\n",
      "143/143 [==============================] - 5s 36ms/step - loss: 0.1643 - precision: 0.9429 - recall: 0.9293 - fn: 2585.0000 - val_loss: 0.1538 - val_precision: 0.9433 - val_recall: 0.9321 - val_fn: 276.0000 - lr: 1.0000e-04\n",
      "Epoch 20/20\n",
      "143/143 [==============================] - 5s 33ms/step - loss: 0.1610 - precision: 0.9439 - recall: 0.9316 - fn: 2501.0000 - val_loss: 0.1541 - val_precision: 0.9433 - val_recall: 0.9329 - val_fn: 273.0000 - lr: 1.0000e-04\n",
      "Saving model at ./Models/Bagging/bagging_model_7.keras\n",
      "Building bootstrap 8\n",
      "X_train.shape (40653, 128, 2) y_ohe_train.shape (40653, 3) features_test.shape (40653, 8)\n",
      "Distribution training set: [[0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]] [ 5281  4416 30956]\n",
      "Epoch 1/20\n",
      "143/143 [==============================] - 19s 45ms/step - loss: 0.3370 - precision: 0.9171 - recall: 0.7910 - fn: 7648.0000 - val_loss: 0.4447 - val_precision: 0.8517 - val_recall: 0.8232 - val_fn: 719.0000 - lr: 1.0000e-04\n",
      "Epoch 2/20\n",
      "143/143 [==============================] - 5s 34ms/step - loss: 0.2414 - precision: 0.9133 - recall: 0.8808 - fn: 4362.0000 - val_loss: 0.3049 - val_precision: 0.8939 - val_recall: 0.8704 - val_fn: 527.0000 - lr: 1.0000e-04\n",
      "Epoch 3/20\n",
      "143/143 [==============================] - 4s 31ms/step - loss: 0.2262 - precision: 0.9181 - recall: 0.8902 - fn: 4018.0000 - val_loss: 0.2950 - val_precision: 0.8881 - val_recall: 0.8650 - val_fn: 549.0000 - lr: 1.0000e-04\n",
      "Epoch 4/20\n",
      "143/143 [==============================] - 5s 34ms/step - loss: 0.2169 - precision: 0.9221 - recall: 0.9002 - fn: 3651.0000 - val_loss: 0.2385 - val_precision: 0.9185 - val_recall: 0.8930 - val_fn: 435.0000 - lr: 1.0000e-04\n",
      "Epoch 5/20\n",
      "143/143 [==============================] - 5s 32ms/step - loss: 0.2095 - precision: 0.9240 - recall: 0.9059 - fn: 3442.0000 - val_loss: 0.2200 - val_precision: 0.9215 - val_recall: 0.9031 - val_fn: 394.0000 - lr: 1.0000e-04\n",
      "Epoch 6/20\n",
      "143/143 [==============================] - 4s 31ms/step - loss: 0.2038 - precision: 0.9275 - recall: 0.9097 - fn: 3303.0000 - val_loss: 0.2165 - val_precision: 0.9211 - val_recall: 0.9046 - val_fn: 388.0000 - lr: 1.0000e-04\n",
      "Epoch 7/20\n",
      "143/143 [==============================] - 5s 36ms/step - loss: 0.1988 - precision: 0.9300 - recall: 0.9130 - fn: 3183.0000 - val_loss: 0.2156 - val_precision: 0.9227 - val_recall: 0.9068 - val_fn: 379.0000 - lr: 1.0000e-04\n",
      "Epoch 8/20\n",
      "143/143 [==============================] - 4s 31ms/step - loss: 0.1932 - precision: 0.9314 - recall: 0.9149 - fn: 3114.0000 - val_loss: 0.2079 - val_precision: 0.9267 - val_recall: 0.9112 - val_fn: 361.0000 - lr: 1.0000e-04\n",
      "Epoch 9/20\n",
      "143/143 [==============================] - 4s 31ms/step - loss: 0.1886 - precision: 0.9330 - recall: 0.9173 - fn: 3024.0000 - val_loss: 0.2044 - val_precision: 0.9249 - val_recall: 0.9090 - val_fn: 370.0000 - lr: 1.0000e-04\n",
      "Epoch 10/20\n",
      "143/143 [==============================] - 5s 36ms/step - loss: 0.1853 - precision: 0.9351 - recall: 0.9205 - fn: 2907.0000 - val_loss: 0.1980 - val_precision: 0.9317 - val_recall: 0.9166 - val_fn: 339.0000 - lr: 1.0000e-04\n",
      "Epoch 11/20\n",
      "143/143 [==============================] - 4s 31ms/step - loss: 0.1795 - precision: 0.9360 - recall: 0.9229 - fn: 2821.0000 - val_loss: 0.2007 - val_precision: 0.9337 - val_recall: 0.9176 - val_fn: 335.0000 - lr: 1.0000e-04\n",
      "Epoch 12/20\n",
      "143/143 [==============================] - 5s 32ms/step - loss: 0.1755 - precision: 0.9388 - recall: 0.9254 - fn: 2731.0000 - val_loss: 0.1955 - val_precision: 0.9323 - val_recall: 0.9179 - val_fn: 334.0000 - lr: 1.0000e-04\n",
      "Epoch 13/20\n",
      "143/143 [==============================] - 5s 35ms/step - loss: 0.1713 - precision: 0.9393 - recall: 0.9260 - fn: 2708.0000 - val_loss: 0.1896 - val_precision: 0.9325 - val_recall: 0.9242 - val_fn: 308.0000 - lr: 1.0000e-04\n",
      "Epoch 14/20\n",
      "143/143 [==============================] - 4s 31ms/step - loss: 0.1667 - precision: 0.9416 - recall: 0.9295 - fn: 2578.0000 - val_loss: 0.1980 - val_precision: 0.9309 - val_recall: 0.9213 - val_fn: 320.0000 - lr: 1.0000e-04\n",
      "Epoch 15/20\n",
      "143/143 [==============================] - 5s 33ms/step - loss: 0.1659 - precision: 0.9430 - recall: 0.9318 - fn: 2494.0000 - val_loss: 0.1860 - val_precision: 0.9360 - val_recall: 0.9240 - val_fn: 309.0000 - lr: 1.0000e-04\n",
      "Epoch 16/20\n",
      "143/143 [==============================] - 5s 34ms/step - loss: 0.1603 - precision: 0.9443 - recall: 0.9334 - fn: 2436.0000 - val_loss: 0.1922 - val_precision: 0.9326 - val_recall: 0.9228 - val_fn: 314.0000 - lr: 1.0000e-04\n",
      "Epoch 17/20\n",
      "143/143 [==============================] - 4s 31ms/step - loss: 0.1550 - precision: 0.9468 - recall: 0.9366 - fn: 2319.0000 - val_loss: 0.1808 - val_precision: 0.9385 - val_recall: 0.9274 - val_fn: 295.0000 - lr: 1.0000e-04\n",
      "Epoch 18/20\n",
      "143/143 [==============================] - 5s 34ms/step - loss: 0.1503 - precision: 0.9477 - recall: 0.9383 - fn: 2257.0000 - val_loss: 0.1768 - val_precision: 0.9394 - val_recall: 0.9306 - val_fn: 282.0000 - lr: 1.0000e-04\n",
      "Epoch 19/20\n",
      "143/143 [==============================] - 5s 33ms/step - loss: 0.1502 - precision: 0.9482 - recall: 0.9392 - fn: 2226.0000 - val_loss: 0.1771 - val_precision: 0.9409 - val_recall: 0.9324 - val_fn: 275.0000 - lr: 1.0000e-04\n",
      "Epoch 20/20\n",
      "143/143 [==============================] - 5s 32ms/step - loss: 0.1460 - precision: 0.9497 - recall: 0.9417 - fn: 2134.0000 - val_loss: 0.1799 - val_precision: 0.9371 - val_recall: 0.9306 - val_fn: 282.0000 - lr: 1.0000e-04\n",
      "Saving model at ./Models/Bagging/bagging_model_8.keras\n",
      "Building bootstrap 9\n",
      "X_train.shape (40653, 128, 2) y_ohe_train.shape (40653, 3) features_test.shape (40653, 8)\n",
      "Distribution training set: [[0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]] [ 5281  4416 30956]\n",
      "Epoch 1/20\n",
      "143/143 [==============================] - 18s 37ms/step - loss: 0.3044 - precision: 0.9096 - recall: 0.8382 - fn: 5919.0000 - val_loss: 0.4135 - val_precision: 0.8954 - val_recall: 0.7688 - val_fn: 940.0000 - lr: 1.0000e-04\n",
      "Epoch 2/20\n",
      "143/143 [==============================] - 5s 36ms/step - loss: 0.2393 - precision: 0.9148 - recall: 0.8862 - fn: 4162.0000 - val_loss: 0.3435 - val_precision: 0.9105 - val_recall: 0.8082 - val_fn: 780.0000 - lr: 1.0000e-04\n",
      "Epoch 3/20\n",
      "143/143 [==============================] - 4s 31ms/step - loss: 0.2283 - precision: 0.9184 - recall: 0.8918 - fn: 3959.0000 - val_loss: 0.3126 - val_precision: 0.8754 - val_recall: 0.8379 - val_fn: 659.0000 - lr: 1.0000e-04\n",
      "Epoch 4/20\n",
      "143/143 [==============================] - 5s 32ms/step - loss: 0.2244 - precision: 0.9185 - recall: 0.8961 - fn: 3803.0000 - val_loss: 0.2932 - val_precision: 0.8881 - val_recall: 0.8571 - val_fn: 581.0000 - lr: 1.0000e-04\n",
      "Epoch 5/20\n",
      "143/143 [==============================] - 6s 43ms/step - loss: 0.2178 - precision: 0.9205 - recall: 0.9009 - fn: 3627.0000 - val_loss: 0.2323 - val_precision: 0.9139 - val_recall: 0.8957 - val_fn: 424.0000 - lr: 1.0000e-04\n",
      "Epoch 6/20\n",
      "143/143 [==============================] - 4s 31ms/step - loss: 0.2123 - precision: 0.9232 - recall: 0.9037 - fn: 3523.0000 - val_loss: 0.2285 - val_precision: 0.9124 - val_recall: 0.8967 - val_fn: 420.0000 - lr: 1.0000e-04\n",
      "Epoch 7/20\n",
      "143/143 [==============================] - 4s 31ms/step - loss: 0.2082 - precision: 0.9249 - recall: 0.9072 - fn: 3397.0000 - val_loss: 0.2134 - val_precision: 0.9207 - val_recall: 0.8989 - val_fn: 411.0000 - lr: 1.0000e-04\n",
      "Epoch 8/20\n",
      "143/143 [==============================] - 5s 36ms/step - loss: 0.2043 - precision: 0.9257 - recall: 0.9076 - fn: 3379.0000 - val_loss: 0.2013 - val_precision: 0.9257 - val_recall: 0.9129 - val_fn: 354.0000 - lr: 1.0000e-04\n",
      "Epoch 9/20\n",
      "143/143 [==============================] - 4s 31ms/step - loss: 0.1976 - precision: 0.9299 - recall: 0.9136 - fn: 3162.0000 - val_loss: 0.2029 - val_precision: 0.9263 - val_recall: 0.9085 - val_fn: 372.0000 - lr: 1.0000e-04\n",
      "Epoch 10/20\n",
      "143/143 [==============================] - 4s 31ms/step - loss: 0.1957 - precision: 0.9311 - recall: 0.9149 - fn: 3115.0000 - val_loss: 0.2127 - val_precision: 0.9202 - val_recall: 0.9110 - val_fn: 362.0000 - lr: 1.0000e-04\n",
      "Epoch 11/20\n",
      "143/143 [==============================] - 5s 36ms/step - loss: 0.1905 - precision: 0.9320 - recall: 0.9158 - fn: 3082.0000 - val_loss: 0.1996 - val_precision: 0.9271 - val_recall: 0.9132 - val_fn: 353.0000 - lr: 1.0000e-04\n",
      "Epoch 12/20\n",
      "143/143 [==============================] - 4s 31ms/step - loss: 0.1856 - precision: 0.9333 - recall: 0.9191 - fn: 2960.0000 - val_loss: 0.1937 - val_precision: 0.9296 - val_recall: 0.9154 - val_fn: 344.0000 - lr: 1.0000e-04\n",
      "Epoch 13/20\n",
      "143/143 [==============================] - 5s 33ms/step - loss: 0.1823 - precision: 0.9351 - recall: 0.9221 - fn: 2849.0000 - val_loss: 0.1884 - val_precision: 0.9272 - val_recall: 0.9183 - val_fn: 332.0000 - lr: 1.0000e-04\n",
      "Epoch 14/20\n",
      "143/143 [==============================] - 5s 35ms/step - loss: 0.1785 - precision: 0.9364 - recall: 0.9243 - fn: 2771.0000 - val_loss: 0.1801 - val_precision: 0.9298 - val_recall: 0.9218 - val_fn: 318.0000 - lr: 1.0000e-04\n",
      "Epoch 15/20\n",
      "143/143 [==============================] - 5s 31ms/step - loss: 0.1739 - precision: 0.9388 - recall: 0.9255 - fn: 2727.0000 - val_loss: 0.1878 - val_precision: 0.9333 - val_recall: 0.9230 - val_fn: 313.0000 - lr: 1.0000e-04\n",
      "Epoch 16/20\n",
      "143/143 [==============================] - 5s 36ms/step - loss: 0.1703 - precision: 0.9395 - recall: 0.9284 - fn: 2619.0000 - val_loss: 0.1792 - val_precision: 0.9332 - val_recall: 0.9206 - val_fn: 323.0000 - lr: 1.0000e-04\n",
      "Epoch 17/20\n",
      "143/143 [==============================] - 5s 31ms/step - loss: 0.1673 - precision: 0.9405 - recall: 0.9292 - fn: 2590.0000 - val_loss: 0.1892 - val_precision: 0.9320 - val_recall: 0.9235 - val_fn: 311.0000 - lr: 1.0000e-04\n",
      "Epoch 18/20\n",
      "143/143 [==============================] - 4s 31ms/step - loss: 0.1635 - precision: 0.9427 - recall: 0.9321 - fn: 2484.0000 - val_loss: 0.1741 - val_precision: 0.9331 - val_recall: 0.9267 - val_fn: 298.0000 - lr: 1.0000e-04\n",
      "Epoch 19/20\n",
      "143/143 [==============================] - 5s 36ms/step - loss: 0.1592 - precision: 0.9434 - recall: 0.9339 - fn: 2417.0000 - val_loss: 0.1770 - val_precision: 0.9343 - val_recall: 0.9240 - val_fn: 309.0000 - lr: 1.0000e-04\n",
      "Epoch 20/20\n",
      "143/143 [==============================] - 4s 31ms/step - loss: 0.1562 - precision: 0.9449 - recall: 0.9352 - fn: 2370.0000 - val_loss: 0.1627 - val_precision: 0.9398 - val_recall: 0.9299 - val_fn: 285.0000 - lr: 1.0000e-04\n",
      "Saving model at ./Models/Bagging/bagging_model_9.keras\n"
     ]
    }
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "collapsed_sections": [
    "AhJX3GOFPC-t",
    "lcKtEN5APNij",
    "YdO4r1NlRyer",
    "929xR2rvPmrK",
    "caLbubaVT5CU",
    "gyEKLr4iWqDv",
    "6LN5uqufXGJu",
    "hpwW82QnXKVA"
   ],
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
